{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Natural Language Processing** (NLP) (SOLUTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:  [https://github.com/d-insight/code-bank.git](https://github.com/d-insight/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we are revisiting the EPFL course book data. We would like to understand how similar courses are based on their textual description. Think about this case: maybe you liked a course very much and would now like to take the most similar one to that.\n",
    "\n",
    "### Business Objective:\n",
    "\n",
    "* To discover similarity relationship between EPFL courses based on their textual description \n",
    "\n",
    "### Learning Objectives:\n",
    "\n",
    "* Getting familiar with text preprocessing facilities in the `nltk` library\n",
    "* Understanding intuition behind different vector space models to work with text data, e.g. TFIDF\n",
    "* Learning how to transform a raw corpus into the vector space model of choice\n",
    "* Learning how to query similar documents to a focal document in a given space\n",
    "* Learning how to visualize text data from high-dimensional space into low dimansions for visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports \n",
    "import pandas as pd\n",
    "\n",
    "# Natural Language Toolkit (NLTK) and spaCy\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import spacy\n",
    "\n",
    "# Sklearn TFIDF function and PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Plotting packages\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Python math package\n",
    "import math\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constant(s)\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Load .csv data\n",
    "\n",
    "In this part, simply load the EPFL course file from `data/epfl_description.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1**: Load EPFL course data. Look at the shape and the first 5 rows. What shape does the data have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the csv file\n",
    "df = pd.read_csv('data/epfl_description.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2**: Concatenate the course title and the course description column. Why is this useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding course titles to descriptions\n",
    "df['description'] = df['course'] + ' ' + df['description']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3**: Lowercase all the words. Otherwise, the computer thinks that \"Finance\" and \"finance\" are two different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'] = df['description'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1**: Draw a random sample of 5 course descriptions and look at the entire description. Are there any issues with the text data? If so, what are they?\n",
    "\n",
    "Hint: Look at the element in row 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a random sample of descriptions (execute repeatedly to check many examples)\n",
    "df['description'].sample(5).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a single description\n",
    "df.iloc[8, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the issues that require cleaning include:\n",
    "\n",
    "- `\\r` character is the carriage return\n",
    "- punctuation marks like `.`, `,`, `?`, `!`, etc. \n",
    "- quotation marks and other symbols like `$`, `(`, `)`, etc.\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2**: Remove the parts of the text identified above. Also remove multiple white spaces. How did the element in row 8 change?\n",
    "\n",
    "Hint: use a \"regular expressions\" (regex), which defines a search pattern for strings - a very handy tool for pre-processing text. You can visit https://regex101.com/ to test your regex expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the current dataframe, this is the raw data, now we are cleaning it\n",
    "df_dropna = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace characters\n",
    "df_dropna['description'] = df_dropna['description'].str.replace('\\r',' ')\n",
    "df_dropna['description'] = df_dropna['description'].str.replace('.',' ')\n",
    "df_dropna['description'] = df_dropna['description'].str.replace(',',' ')\n",
    "df_dropna['description'] = df_dropna['description'].str.replace(';',' ')\n",
    "df_dropna['description'] = df_dropna['description'].str.replace('$',' ')\n",
    "df_dropna['description'] = df_dropna['description'].str.replace('(',' ')\n",
    "df_dropna['description'] = df_dropna['description'].str.replace(')',' ')\n",
    "df_dropna['description'] = df_dropna['description'].str.replace('?',' ')\n",
    "df_dropna['description'] = df_dropna['description'].str.replace('!',' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove multiple white spaces\n",
    "df_dropna['description'] = df_dropna['description'].str.replace('\\s+', ' ', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the same description as above\n",
    "df_dropna.iloc[8, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3**: Clean data by removing rows with missing data in any column. How many clean rows are left?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values and reset index\n",
    "df_clean = df_dropna.dropna(axis=0, how='any')\n",
    "df_clean.reset_index(drop=True, inplace=True)\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['description'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Tokenize and lemmatize course descriptions \n",
    "\n",
    "For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For example:\n",
    "\n",
    "- am, are, is $\\Rightarrow$ be\n",
    "- car, cars, car's, cars' $\\Rightarrow$ car\n",
    "\n",
    "For details about lemmatization and stemming visit: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1**: Define a simple function that takes a course description as input and outputs the tokenized and lemmatized text as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy \n",
    "# (if spacy cannot load data run this in terminal: python -m spacy download en_core_web_sm)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Tokenization and lemmatization function\n",
    "def lemmatize_text(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Tokenize and lemmatize text\n",
    "    \n",
    "    Parameter: \n",
    "        text (str): input text\n",
    "    \n",
    "    Returns: \n",
    "        list: list of tokenized and lemmatized text \n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    # Set up text-processing pipeline\n",
    "    text = nlp(text)\n",
    "    text_tokenizedLemmatized = []\n",
    "\n",
    "    # Tokenize and Lemmatize each word\n",
    "    for word in text:\n",
    "        text_tokenizedLemmatized.append(word.lemma_)\n",
    "        \n",
    "    return text_tokenizedLemmatized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of lemmatizing a verb with and without POS\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "print('Without POS information:'.ljust(30) + str(lemmatizer.lemmatize('are')))\n",
    "print('With POS information:'.ljust(30) + str(lemmatizer.lemmatize('are', pos='v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2**: Apply the function to the course description in your Pandas dataframe. \n",
    "\n",
    "Hint: Use the Pandas `apply` function. Look at the second row - what changed? Did lemmatization work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['description'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the lemmatization function\n",
    "df_clean['description_lemmatized'] = df_clean['description'].apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(df_clean['description_lemmatized'].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Create a term frequency inverse document frequency (TFIDF) matrix\n",
    "\n",
    "We now have to ensure that the text description is stored as a string in our dataframe, not as a list. In the code below, replace the variable names with the ones you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform list data to text data\n",
    "df_clean['description_lemmatized_text'] = df_clean['description_lemmatized'].str.join(' ')\n",
    "df_clean['description_lemmatized_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the text data\n",
    "data = df_clean['description_lemmatized_text']\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1**: Fit and transform your text data using TFIDF. \n",
    "\n",
    "Hint: use the `TfidfVectorizer()` function in sklearn with the parameter `max_features = 400`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vectorizer\n",
    "vectorizer = TfidfVectorizer(min_df = 1, max_features = 400, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the course description data\n",
    "X = vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2**: What shape does the TFIDF matrix have? What's the meaning of the number of columns? Use the `toarray()` function to show some of the TFIDF entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from sparse matrix to regular matrix\n",
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each column is a unique term/word\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some TFIDF values\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Apply a principal component analysis (PCA)\n",
    "\n",
    "We now project the high-dimensional TFIDF matrix into its 2 principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1**: Run a PCA on the TFIDF matrix. Hint: use the `PCA.fit_transform()` function in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components = 2, random_state = SEED)\n",
    "pca_out = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2**: What's the shape of the PCA output? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3**: This is done for you: add the PCA values to the cleaned dataframe. We want a dataframe with the course name and the PCA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PCA output to dataframe\n",
    "pca_df = pd.DataFrame(data = pca_out, columns = ['PCA1', 'PCA2'])\n",
    "pca_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes\n",
    "df_out = pd.concat([df_clean, pca_df], axis = 1)\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Visualize how similar EPFL courses to each other\n",
    "\n",
    "Now we returning to the initial business objective: to discover similarity relationship between EPFL courses based on their textual description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1**: Visualize the PCA values using a simple scatter plot. Each dot represents a course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the PCA values\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.scatter(df_out['PCA1'], df_out['PCA2'], s = 10)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2**: What's the most similar course to COM-421? Compute the Euclidean distance between COM-421 and every other course. To do so, you can use this function below:\n",
    "\n",
    "```\n",
    "def euclideanDistance(p1, p2):\n",
    "    \"\"\"\n",
    "    Compute euclidean distance\n",
    "    \n",
    "    Parameter: \n",
    "        p1 (list): input point defined as a [x,y] list\n",
    "        p2 (list): input point defined as a [x,y] list\n",
    "    \n",
    "    Returns: \n",
    "        float: euclidean distance between p1 and p2\n",
    "        \n",
    "    \"\"\"   \n",
    "    \n",
    "    return math.sqrt( ((p1[0]-p2[0])**2)+((p1[1]-p2[1])**2) )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDistance(p1, p2):\n",
    "    \"\"\"\n",
    "    Compute euclidean distance\n",
    "    \n",
    "    Parameter: \n",
    "        p1 (list): input point defined as a [x,y] list\n",
    "        p2 (list): input point defined as a [x,y] list\n",
    "    \n",
    "    Returns: \n",
    "        float: euclidean distance between p1 and p2\n",
    "        \n",
    "    \"\"\"   \n",
    "    \n",
    "    return math.sqrt( ((p1[0]-p2[0])**2)+((p1[1]-p2[1])**2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter course code & the # of most similar courses to display\n",
    "CODE = 'COM-421'\n",
    "TOP  = 30\n",
    "\n",
    "WIDTH = 80\n",
    "\n",
    "print('-'*WIDTH)\n",
    "print('FOCAL COURSE')\n",
    "print('-'*WIDTH)\n",
    "print(df_out[df_out['code'] == CODE]['course'].values[0], '\\n')\n",
    "\n",
    "# Find the most similar courses\n",
    "p1 = df_out[df_out['code'] == CODE][['PCA1', 'PCA2']].values[0]\n",
    "\n",
    "# Collect all distance metrics \n",
    "all_distances = {}\n",
    "for i, row in df_out.iterrows():\n",
    "    p2 = row[['PCA1', 'PCA2']].values\n",
    "    distance = euclideanDistance(p1, p2)\n",
    "    all_distances[row['course']] = distance\n",
    "\n",
    "# Sort distances in increasing order of distance \n",
    "all_distances = {k: v for k, v in sorted(all_distances.items(), key=lambda item: item[1])}\n",
    "\n",
    "# Print most similar courses \n",
    "i = 0\n",
    "print('-'*WIDTH)\n",
    "print('TOP {} COURSES'.format(TOP).ljust(70) + 'DISTANCE')\n",
    "print('-'*WIDTH)\n",
    "for k, v in all_distances.items():\n",
    "    print('{}'.format(k).ljust(70) + '{}'.format(round(v, 4)))\n",
    "    i+= 1\n",
    "    if i == TOP: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focal course description (lemmatized)\n",
    "df_out[df_out['code'] == CODE]['description_lemmatized_text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar course description (lemmatized)\n",
    "course_name = 'Neurosciences III : behavioral and cognitive neuroscience'\n",
    "df_out[df_out['course'] == course_name]['description_lemmatized_text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
