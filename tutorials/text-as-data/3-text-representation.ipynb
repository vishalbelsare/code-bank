{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tutorial on Text as Data**: Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:  [https://github.com/d-insight/code-bank.git](https://github.com/d-insight/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "There are different ways to represent text data. In this notebook, we will dig deeper into the subject and look at different approaches to transform the text into its respective numerical representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Text representation with TF-IDF\n",
    "\n",
    "In the previous notebook, we used a very simple solution to represent text data: by counting the most common words. One of the drawbacks of this approach is that all terms receive the same weights.\n",
    "\n",
    "A better strategy is to weight words according to their occurrence in the document. The underline idea is that words that appear in every document probably do not bring any particular meaning to the topic.\n",
    "\n",
    "To do that we normalize the term frequency of the world by the inverse of the document frequency. This might sound complicated at first. In this section, we will break down the TF-IDF algorithm step by step on a simple example.\n",
    "\n",
    "\n",
    "> ☝️scikit-learn provides a function to easily compute TF-IDF weights. At the end of our custom solution, we will compare the two solutions to make sure they return the same value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images/tf-idf-image](./images/tf-idf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we are given the following reviews and we want to compute the TF-IDF weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_0 = \"the pizza was very very good quality\"\n",
    "review_1 = \"the pasta was very superior quality\"\n",
    "review_2 = \"the gnocchi weren't that good\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: First, using `scikit-learn`, compute the count term-vector matrix and save it into a DataFrame `df_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gnocchi</th>\n",
       "      <th>good</th>\n",
       "      <th>pasta</th>\n",
       "      <th>pizza</th>\n",
       "      <th>quality</th>\n",
       "      <th>superior</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>very</th>\n",
       "      <th>was</th>\n",
       "      <th>weren't</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gnocchi  good  pasta  pizza  quality  superior  that  the  very  was  \\\n",
       "0        0     1      0      1        1         0     0    1     2    1   \n",
       "1        0     0      1      0        1         1     0    1     1    1   \n",
       "2        1     1      0      0        0         0     1    1     0    0   \n",
       "\n",
       "   weren't  \n",
       "0        0  \n",
       "1        0  \n",
       "2        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(token_pattern=\"\\S+\")\n",
    "\n",
    "reviews = [review_0, review_1, review_2]\n",
    "\n",
    "count_cv = cv.fit_transform(reviews).toarray()\n",
    "\n",
    "df_count = pd.DataFrame(count_cv, columns=cv.get_feature_names())\n",
    "df_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Now, we need to compute the `idf` part. For each term, count the total number of documents that contain such a term (`document_freq`). In the literature, this is known as \"document frequency\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gnocchi     1\n",
       "good        2\n",
       "pasta       1\n",
       "pizza       1\n",
       "quality     2\n",
       "superior    1\n",
       "that        1\n",
       "the         3\n",
       "very        2\n",
       "was         2\n",
       "weren't     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_freq = (df_count != 0).sum()\n",
    "document_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Compute the `idf` part:\n",
    "\n",
    "\\begin{align}\n",
    "idf(word_i) = ln(\\frac{n+1}{document\\_freq(word_i) + 1}) + 1\n",
    "\\end{align}\n",
    "\n",
    "Example\n",
    "\n",
    "\\begin{align}\n",
    "idf(very) = ln(\\frac{3+1}{2 + 1}) + 1 = ln(4/3) + 1 \\approx 1.29\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gnocchi     1.693147\n",
       "good        1.287682\n",
       "pasta       1.693147\n",
       "pizza       1.693147\n",
       "quality     1.287682\n",
       "superior    1.693147\n",
       "that        1.693147\n",
       "the         1.000000\n",
       "very        1.287682\n",
       "was         1.287682\n",
       "weren't     1.693147\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = df_count.shape[0]\n",
    "idf = np.log((n + 1) / (document_freq + 1)) + 1\n",
    "idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Finnally, compute the TF-IDF (`tfidf`) weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gnocchi</th>\n",
       "      <th>good</th>\n",
       "      <th>pasta</th>\n",
       "      <th>pizza</th>\n",
       "      <th>quality</th>\n",
       "      <th>superior</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>very</th>\n",
       "      <th>was</th>\n",
       "      <th>weren't</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.575364</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gnocchi      good     pasta     pizza   quality  superior      that  the  \\\n",
       "0  0.000000  1.287682  0.000000  1.693147  1.287682  0.000000  0.000000  1.0   \n",
       "1  0.000000  0.000000  1.693147  0.000000  1.287682  1.693147  0.000000  1.0   \n",
       "2  1.693147  1.287682  0.000000  0.000000  0.000000  0.000000  1.693147  1.0   \n",
       "\n",
       "       very       was   weren't  \n",
       "0  2.575364  1.287682  0.000000  \n",
       "1  1.287682  1.287682  0.000000  \n",
       "2  0.000000  0.000000  1.693147  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = df_count * idf\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Now we can compare our solution with scikit-learn and see if they are the same.\n",
    "\n",
    "> ☝️Note that by default scikit-learn normalize the output. To avoid that, we need to pass `norm=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gnocchi</th>\n",
       "      <th>good</th>\n",
       "      <th>pasta</th>\n",
       "      <th>pizza</th>\n",
       "      <th>quality</th>\n",
       "      <th>superior</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>very</th>\n",
       "      <th>was</th>\n",
       "      <th>weren't</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.575364</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gnocchi      good     pasta     pizza   quality  superior      that  the  \\\n",
       "0  0.000000  1.287682  0.000000  1.693147  1.287682  0.000000  0.000000  1.0   \n",
       "1  0.000000  0.000000  1.693147  0.000000  1.287682  1.693147  0.000000  1.0   \n",
       "2  1.693147  1.287682  0.000000  0.000000  0.000000  0.000000  1.693147  1.0   \n",
       "\n",
       "       very       was   weren't  \n",
       "0  2.575364  1.287682  0.000000  \n",
       "1  1.287682  1.287682  0.000000  \n",
       "2  0.000000  0.000000  1.693147  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "reviews = [review_0, review_1, review_2]\n",
    "\n",
    "vectorizer = TfidfVectorizer(token_pattern=\"\\S+\", min_df=1, norm=None)\n",
    "\n",
    "m = vectorizer.fit_transform(reviews).toarray()\n",
    "\n",
    "tfidf_sklearn = pd.DataFrame(m, columns=vectorizer.get_feature_names())\n",
    "tfidf_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gnocchi</th>\n",
       "      <th>good</th>\n",
       "      <th>pasta</th>\n",
       "      <th>pizza</th>\n",
       "      <th>quality</th>\n",
       "      <th>superior</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>very</th>\n",
       "      <th>was</th>\n",
       "      <th>weren't</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gnocchi  good  pasta  pizza  quality  superior  that   the  very   was  \\\n",
       "0     True  True   True   True     True      True  True  True  True  True   \n",
       "1     True  True   True   True     True      True  True  True  True  True   \n",
       "2     True  True   True   True     True      True  True  True  True  True   \n",
       "\n",
       "   weren't  \n",
       "0     True  \n",
       "1     True  \n",
       "2     True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_sklearn == tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: Explain why the outputs of `CountVectorizer` and `TfidfVectorizer` are sparse matrix. What's the main advantage of using sparse representation? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "Recall that the rows of the matrix represents the document (in this case reviews) whereas the columns are terms (i.e tokens/words). Clearly, not all documents contain all words of the vocabulary, and therefore most of the values are zero. Instead of returning the full matrix composed of many zeros, we return a special data structure (i.e a sparse matrix) containing only non-zero values plus the information regarding the position in the matrix. This way we can save a lot of memory.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7 (**homework**): Go back to Notebook 2, part 3 and compute again the topic, this time using `TfidfVectorizer` instead of `CountVectorizer`. Does the topics are better defined? (spoiler: **yes**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Word2Vec\n",
    "\n",
    "Word2vec is a two-layer neural networks to vectorize words. Its input is a text corpus and its output is a set of vectors, feature vectors that represent words in that corpus.\n",
    "\n",
    "\n",
    "**Learning objective**: intuitively understand Word2Vec and be able to implement it\n",
    "\n",
    "**Useful resources**:\n",
    " - [Word2vec paper: Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    " - [The Illustrated Word2vec](http://jalammar.github.io/illustrated-word2vec/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: By looking at the following image, can you explain the difference between the continuous-bag-of-words (CBOW) and skip-gram model architectures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images/cbow vs skip-gram](./images/cbow-vs-skip-gram.pbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "**CBOW**: given a context, predict the word\n",
    "\n",
    "**Skip-gram**: given as input a single word, predict the context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: We will now train a Word2Vec model using [Gensim](https://radimrehurek.com/gensim/). Gensim's Word2Vec expect as input a list of sentences where each sentence is a list of tokens as in the example below. Load `review_clean.csv` into a Pandas Dataframe `df`, transform it as a list of list of tokens and store it into a variable called `corpus`.\n",
    "\n",
    "> ☝️ For tokenization, you can simply use `string.split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Resturant', 'was', 'ok', '.'], ['Restaurant', 'was', 'average']]\n"
     ]
    }
   ],
   "source": [
    "sentence_one = [\"Resturant\", \"was\", \"ok\", \".\"]\n",
    "sentence_two = [\"Restaurant\", \"was\", \"average\"]\n",
    "\n",
    "valid_gensim_input = [sentence_one, sentence_two]\n",
    "\n",
    "print(valid_gensim_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>categories</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Omar great !  diagnose correct cell phone issu...</td>\n",
       "      <td>Omar is great!  He can diagnose and correct an...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Mobile Phones, Shopping, Telecommunications, M...</td>\n",
       "      <td>T-Mobile</td>\n",
       "      <td>16635 N Tatum Blvd, Ste 110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TERRIBLE ! ! ! went salon close proximity waxi...</td>\n",
       "      <td>TERRIBLE!!! I went to this salon due to its cl...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Day Spas, Beauty &amp; Spas, Nail Salons</td>\n",
       "      <td>Pretty Nails</td>\n",
       "      <td>1660 E Camelback Rd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_clean  \\\n",
       "0  Omar great !  diagnose correct cell phone issu...   \n",
       "1  TERRIBLE ! ! ! went salon close proximity waxi...   \n",
       "\n",
       "                                                text  stars  \\\n",
       "0  Omar is great!  He can diagnose and correct an...    5.0   \n",
       "1  TERRIBLE!!! I went to this salon due to its cl...    1.0   \n",
       "\n",
       "                                          categories          name  \\\n",
       "0  Mobile Phones, Shopping, Telecommunications, M...      T-Mobile   \n",
       "1               Day Spas, Beauty & Spas, Nail Salons  Pretty Nails   \n",
       "\n",
       "                       address  \n",
       "0  16635 N Tatum Blvd, Ste 110  \n",
       "1          1660 E Camelback Rd  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/review_clean.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Omar',\n",
       "  'great',\n",
       "  '!',\n",
       "  'diagnose',\n",
       "  'correct',\n",
       "  'cell',\n",
       "  'phone',\n",
       "  'issue',\n",
       "  '-',\n",
       "  'professional',\n",
       "  'patient',\n",
       "  '-',\n",
       "  'ideal',\n",
       "  'buyers',\n",
       "  '/',\n",
       "  'customers',\n",
       "  'limited',\n",
       "  'technology',\n",
       "  'knowledge'],\n",
       " ['Thank', 'Omar', 'help']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into sentences\n",
    "sentences_s = df['text_clean'].str.split(\".\").explode()\n",
    "\n",
    "# Split each sentence into tokens\n",
    "tokens_s = sentences_s.str.split()\n",
    "\n",
    "# Concatenate all list togethers\n",
    "corpus = list(tokens_s)\n",
    "corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Define and train the `word2vec` model. Set the embedding `size` to 300, train using the skip-gram model and set `min_count` to 5.\n",
    "\n",
    "> ☝️ Have a look at the [Gensim Word2Vec tutorial](https://radimrehurek.com/gensim/models/word2vec.html) for help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9089505, 11248120)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec(size=300, sg=1, min_count=5)\n",
    "\n",
    "\n",
    "w2v_model.build_vocab(corpus, progress_per=10000)\n",
    "\n",
    "\n",
    "w2v_model.train(corpus, total_examples=w2v_model.corpus_count, epochs=20, report_delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Once the model is trained we can extract the word embeddings and compare it. Execute the cell below to create a DataFrame that map selected words to their Word2Vec vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pasta</td>\n",
       "      <td>[0.16100411, -0.44462204, 0.09203769, 0.131131...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pizza</td>\n",
       "      <td>[0.5975063, -0.54391915, 0.5388808, 0.19324102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spaghetti</td>\n",
       "      <td>[0.34720272, -0.29686236, 0.26158643, 0.001025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>car</td>\n",
       "      <td>[0.3174404, -0.41663548, 0.5467024, 0.7372491,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>insurance</td>\n",
       "      <td>[-0.38761958, -0.23813048, -0.011089393, 0.336...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phone</td>\n",
       "      <td>[0.088869356, -0.2655629, 0.335964, 0.91946524...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>technology</td>\n",
       "      <td>[-0.09345005, -0.18767874, 0.06817056, 0.36209...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        vocab                                             vector\n",
       "0       pasta  [0.16100411, -0.44462204, 0.09203769, 0.131131...\n",
       "1       pizza  [0.5975063, -0.54391915, 0.5388808, 0.19324102...\n",
       "2   spaghetti  [0.34720272, -0.29686236, 0.26158643, 0.001025...\n",
       "3         car  [0.3174404, -0.41663548, 0.5467024, 0.7372491,...\n",
       "4   insurance  [-0.38761958, -0.23813048, -0.011089393, 0.336...\n",
       "5       phone  [0.088869356, -0.2655629, 0.335964, 0.91946524...\n",
       "6  technology  [-0.09345005, -0.18767874, 0.06817056, 0.36209..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_words = [\"pasta\", \"pizza\", \"spaghetti\", \"car\", \"insurance\", \"phone\", \"technology\"]\n",
    "\n",
    "dict_ = {}\n",
    "\n",
    "for word in list_of_words:\n",
    "    dict_[word] = [w2v_model.wv[word]]\n",
    "\n",
    "df_vocab = pd.DataFrame(dict_).T.reset_index().rename(columns={0: 'vector', 'index': 'vocab'})\n",
    "df_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: using Texthero `pca` and `scatterplot` functions, apply principal component analysis and visualize the vector space. Can you spot anything interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmAAAAHiCAYAAABP+3CeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de5yPdf7/8efnMAcMM4ZhQpHDEiXnU9aUHEcOoU1tInYToiQ5pCQmJbtSfirWWZKtFC1StvSdJcdMcqoIOc6YwcyYMTOfz+f6/SGzO5Uy5vN5X2bmcb/d3HbmPdfner8+r/aP5+19va/rcliWZQkAAADGOO0uAAAAoLghgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhbrsLyK8zZ87L5wv8kzPKlQtTcnJ6wOcpyuihf9BH/6CP/kEf/YM+Fty13kOn06GyZUtd9u+FLoD5fJaRAHZpLhQMPfQP+ugf9NE/6KN/0MeCK8w95BIkAACAYQQwAAAAwwhgAAAAhhW6PWAAAOC3eb0enTmTJI8n2+5SAiYx0Smfz2d3GXI6XSpRIkxhYeFyOBxX/DkCGAAARcyZM0kKDS2pUqWi8xUKChO32ymPx94AZlmWvF6P0tLO6syZJEVGVrjiz3IJEgCAIsbjyVapUmWKbPi6VjgcDrndQYqIKKfs7Av5+iwBDACAIojwZY7D4ZSUv0diEMAAAAAMI4ABAIBCqXXrJnaXcNUIYAAAAIZxFyQAAAiYceNGqX37jrrjjnaSpIED+2rkyNGaNetVpaWlKjS0hB5//EnddFM9nTx5Qi+8MFFnzqQoNDRUo0c/o5o1a+nNN/+ftm/fqtTUVEVERCgubqoqVrx4x+FLL8Vp797dCg+P0Nixzyo6OtrOr3vFWAEDAAAB07FjrNavXydJ+vHHI8rKytL06VN1zz19tHDhMg0b9oTGjx+t7Oxs/e1vLyompq0WL16uAQMe1sKFc3X06I86cuSQ3nhjnpYte1+VK1fRunVrc8/fsGEjLViwVDExd2jGjGl2fc18I4ABAICAadWqtXbv/kYZGef16acf68472+vo0aOKiWkrSbr55ltUpkwZHTlyWDt37lCnTrGSpJYtW2vSpBdVpcr1evTREVq16gO99tp07d69S5mZGZKkkJAQdejQWZLUsWNnffXVdnu+5FUggP2My+UUd+4CAOAfQUFBatWqteLjv9C///2JOne+S5aV95ENliV5vV65XO7/GbP0ww8HtW/fXo0Y8agsy6c77rhTbdrcnvt5p9OV5xxud+HZWUUA+0mIvCqVky7P7p0KPX9GOWnpdpcEAECR0LFjrJYtW6IyZcIVHX2dKleuog0b/i1J+uabXUpJSVb16jXUoEFDffrpxcuV27Zt1tSpcdq5c7saNmysHj16q1q16tqyZXPuK4gyMzMUH79BkvSvf32oJk2a2fMFr0LhiYoBFOSSLuzape/+/kru2PX391FEuw7K8pFRAQAoiPr1Gyg9PV3du/eSJD377CS9/PILmjv3TQUFBSsubqqCgoI0YsRTeumlyVqx4t2fNuGPV1hYmMaNG6V+/frI5XKrRo2aOnHiuCQpLKy0vvjic82Z84aioqI0btwEO79mvjisn68DXuOSk9Pl8/m35FJWlnY98aQ8aWm5Yw63Ww1f/39Kd4T4da7iJCqqtJKS0n7/QPwm+ugf9NE/6KN/BLqPJ08eVnR01YCd/1pwLbwL8n/9vOdOp0PlyoVd9niWdyTJIXnOn88zZHk8srxemwoCAABFGQFMktfhVrnbWuUZC6tVU75CtJkPAAAUHiQMSVlyqepD/VWiciWd3bZdYbVr6/rePZVmBUl+vtwJAABAANPFW1fTLbciOnVR5J3tZLmDFRxZWj72OQAAgAAggP2PbI8lOYIltn4BAIAAYg8YAACAYQQwAAAAwwhgAADgmhcf/4WWLVtidxl+wx4wAACgTbtP6v0NB5ScmqVyZULUM6aGWtaLtrusXPv377W7BL8igAEAUMxt2n1SC9fsU/ZPT5ZPTs3SwjX7JMkvIWzHjm2aN2+2XC63EhNPqm7deho9+hnNnz9H27dvVWpqqiIiIhQXN1Xh4RGaMmWiDh48IEm6++57dMstt+rDD9+XJEVHX6dmzVropZcmKTU1TcnJp9WuXUcNHjyswHWaxCVIAACKufc3HMgNX5dke3x6f8MBv82xZ89ujRz5lJYufU9ZWdn65z/f1pEjh/TGG/O0bNn7qly5itatW6tduxKUmpqq+fOX6pVXZmnXrgTdeGN1de/eU92791SXLt30yScfq337Tpo9e4EWLlymFSve1dmzZ/1WqwmsgAEAUMwlp2bla/xqNGjQUDfcUE2S1KlTrFauXKERI57SqlUf6MiRw9q9e5cqV66i6tVr6MiRw3riiUfVosVtv7qydf/9fZWQsF1Lly7WDz8ckMeTowsXMiVF+K3eQGMFDACAYq5cmZB8jV8Nl8uV+7PPZ0lyaMSIR2VZPt1xx51q0+Z2WZal8PAILV68XL163asjRw5rwIAHlJaW98Hor702XcuXL1N09HXq12+gwsMjZFmF6801BDAAAIq5njE1FOzOGwmC3U71jKnhtzm+/nqnkpIS5fP5tHbtv1S//q1q2LCxevTorWrVqmvLls3y+XyKj9+g559/Rq1atdbjjz+pEiVKKDHxlFwul7zei09K37Zts/785wfVtm07JSaeyj1vYcIlSAAAirlLG+0DeRdk+fJRmjx5gpKSEtW0aXN17BirceNGqV+/PnK53KpRo6ZOnDiuhx76qz77bL369v2TgoODFRPTVjVq1FRaWqri4p5TZGSkHnigvyZOfEalSoUpMjJSderU1fHjx1S5chW/1RtoDquQrdklJ6f/tHQZWFFRpZXEuyALhB76B330D/roH/TRPwLdx5MnDys6umrAzp9fl+6CnDlztt/O6XY75fFcO6teP++50+lQuXJhlz2eS5AAAACGcQkSAAAEVKNGTdSoURO7y7imsAIGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAADAuNOnk/Tkk8PtLsM23AUJAACU/d1GZW99T1Z6shxh5RTctJeCa7UK2Hzly0dp2rRXA3b+ax0BDACAYi77u43K+r8FkidbkmSlJ1/8XfJLCLv0IFaXy63ExJOqW7ee+vUbqCeeGKZ3312lxx4bonPnzkqSTp9OVN26N6tz57u0cOE8SZLP59XBgwc0Z85ChYSEaPr0l3XhQqZSUlLUp88DuueePgWu0TQCGAAAxVz21vdyw1cuT7ayt77nt1WwPXt2a8GCt3T99VX1zDNjtHFjfO7fZsyYJUk6duyoRowYqmHDntD119+gO+5oJ0l65ZVpuvXWRrrppnqaMeNv6tdvoFq0aKHDh4+of//7C2UAs2UP2IwZMxQbG6suXbpo/vz5dpQAAAB+YqUn52v8ajRo0FA33FBNDodDnTrFaseObXn+npFxXuPGPakRI0bp+utvyB3/6KMP9e23+/TYYyMlSY8++riys7O1cOE8zZ49S5mZGX6r0STjK2BbtmzRl19+qZUrV8rj8Sg2NlYxMTGqXr266VIAAIAkR1i5Xw1bjrByfpvD5XLl/uzzWXl+tyxLEyeOV9u27dWyZevc8V27ErRo0Ty98cY8ud0XI8uzz45R6dJl1KZNjO64o73Wr1/ntxpNMr4C1qxZMy1atEhut1vJycnyer0qWbKk6TIAAMBPgpv2ktzBeQfdwRfH/eTrr3cqKSlRPp9Pa9f+S82b//fS5uzZsxQUFKQHHxyQO3bq1Ek9//wzmjjxBUVG/jcIbt26RX/5yyNq0+Z27dy5Q5Lk9Xr9VqcptuwBCwoK0quvvqp58+apU6dOqlix4hV/9rfeLO5vUVGljc1VVNFD/6CP/kEf/YM++kcg+5iY6JTbfeVrLO6bWsvlcurCl/+ULz1ZzrByCm1xj0L+4J/9Xy6XU+XLRykuboKSkpLUtGlztWjRQkuWLNCZM8lavHi+ataspYEDH5BlWSpduoyuv/56ZWRk6G9/ezE3YD344EP6y18GaciQvygsrLSqVq2q666rpMTEE3kuW9rB6XTm67+pw7IsK4D1/KbMzEw98sgjio2N1b333ntFn0lOTpfPF/iSo6JKKykpLeDzFGX00D/oo3/QR/+gj/4R6D6ePHlY0dFVA3b+/Lp0F+TMmbP9dk632ymPx+e38xXUz3vudDp+c9HI+CXIAwcOaO/evZKkEiVKqEOHDtq/f7/pMgAAAGxjPIAdPXpU48ePV3Z2trKzs7V+/Xo1btzYdBkAAMCQRo2a+HX1qygwvgcsJiZGCQkJ6tGjh1wulzp06KAuXbqYLgMAAMA2tmzCHz58uIYPL77vfwIAAMUbL+MGAAAwjAAGAABgGAEMAADAMAIYAAAolObOfVNz5755xcenp6dr7NiRub8PGzYo9+f+/e/3a22/x5ZN+AAA4Nqy5eQOrTywVmeyzqpsSIS61eikZtGN7C7Lr9LSUvXdd9/m/v7VV9tzf16wYKnRWghgAAAUc1tO7tDSfe8px5cjSTqTdVZL970nSQUOYYmJp/T8888oMzNTTqdDjz02Ss89N06tW8coIeHiuxzHjn1Wf/hDHX311XbNnj1LWVkXlJaWpsGDh6tt23a550hLS1X16jW1c+cOrVq1VpK0d+9uPfLIACUlJSo2tqsGDhwkr9erWbNm6Kuvtsvr9Sk29i7de++f9corL+v06SSNHftk7msQ//rXfpozZ6Fat26i+PhtBfqu+UEAAwCgmFt5YG1u+Lokx5ejlQfWFjiAffTRh2rVqrXuv/9B7dixTV9/vVOSVKZMGc2fv1Tx8V8oLu45LVy4TO+9947GjHlGVatW0/btWzVjxjS1bdvup/9tr54979GGDZ/pk0/W5p4/JSVFb7wxTxkZGerd+y7dd98DWrdujSRp3ry3lJ2drSeeeFR16tTV44+P0rBhgzRlyjRJ0rvvvqM5cxYW6PtdLfaAAQBQzJ3JOpuv8fxo0qSZ3n57iZ577mmdPp2kXr3+JEnq1q2nJKl16zZKTEzU2bNn9cwzk3Tw4PdasOAfWrZsiTIzMyVJW7duUadOsZKkmJg7FBb235det2jRSsHBwYqIiFB4eIRSU1O1bdsWxcd/of7979fDD/dXUlKiDhz4vsDfxZ9YAQMAoJgrGxLxq2GrbEhEgc9dv34DLVmyXBs3xmv9+nVavXqVJMnlcuUeY1k+OZ1ODR36VzVq1FgNGzZW48ZNNXHieEmS0+mUz2f96vn/9zwOh0OWZcnr9WnIkOGKiWkrSTp79qxKlAhVSkpKgb+Pv7ACBgBAMdetRicFOYPyjAU5g9StRqcCn3vWrBn6+OPV6tz5Lo0YMVrffrtfkrR+/ceSpA0bPlPVqjdKsvTjj4c1cOAjatmytbZs+VI+n0+S1LRp89zLjps2/Ufp6Wm/OWfjxk20cuUH8ng8ysjI0JAhA7V79zdyuVzyer25x7lcLnk8ngJ/x6vBChgAAMXcpX1egbgLslevezVx4nitXv2RnE6nRo4co9dff1W7diXoo49WqkSJUD399HMqUyZcd93VQ337/kmlSpVSvXr1deHCBWVmZuqxx0Zq0qQJWrnyfdWs+Yc8lyB/TY8evXX06I966KH75fV6FRvbVY0aNZHH41HFitEaNmyQXnvtTbVu3Ub9+9+vuXMXF/h75pfDsqxfX9O7RiUnp192GdKfoqJKKynptxM2fhs99A/66B/00T/oo38Euo8nTx5WdHTVgJ2/oHr37qrXXntT111X6YqO/+c/l6lJk2a68cbq2r9/n156abIWLVoqj8cX4Eqv3M977nQ6VK5c2GWPZwUMAABc06pUuV7PPfe0nE6HgoNDNHr0eLtLKjACGAAAMOrdd1fl6/iWLW9Ty5a3Bagae7AJHwAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABjGYygAAIBSv9yo0++/J09KstyR5VS+Zy+VadGqwOe1LEuvv/6avvjic7ndLnXr1lO1av1Bs2fPUlbWBaWlpWnw4OFq27ad4uKe07lz53Ts2I8aPHi4Wrdu44dvdm0igAEAUMylfrlRpxYtkJWdLUnypCTr1KIFklTgEPbZZ+u1a1eCFi1aJo/HoyFD/qLw8AiNGfOMqlatpu3bt2rGjGlq27adJCk8PFxTp04v0JyFAQEMAIBi7vT77+WGr0us7Gydfv+9AgewnTu3q23b9goODlZwcLAWLFiqrKwsbdz4f/rss0+1e/cuZWZm5h5ft+7NBZqvsGAPGAAAxZwnJTlf4/nhdudd6zlx4riGDv2r9u7drdq16+jBBwfof19LHRISUuA5CwMCGAAAxZw7sly+xvPj1lsbacOGf8vj8ejChQsaMeJRHTx4QAMHPqKWLVtry5Yv5fNdOy/VNoVLkAAAFHPle/bKswdMkhzBwSrfs1eBzx0Tc4f27dujAQP+LJ/P0r333q+jR39U375/UqlSpVSvXn1duHAhz2XI4sBh/e+6XyGQnJwuny/wJUdFlVZSUlrA5ynK6KF/0Ef/oI/+QR/9I9B9PHnysKKjq+brM4G6CzJQ3G6nPJ5rZ+Xs5z13Oh0qVy7sssezAgYAAFSmRatrOnAVNewBAwAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAQEDt27dHL744ye4yrik8BwwAAOjb3ae0ecMPSk/NUliZEDWPuVF/qFfRL+euU6euxoyp65dzFRUEMAAAirlvd5/ShjXf5j5ZPj01SxvWfCtJfglhO3Zs07x5syVJdevWU0LCTp09e0aPPz5KLVvepnXr1mrp0kVyOp2qVKmSnnlmknbv3qV582Zr5syLn4uLe04NGzZWw4aNNXLkMEVElFVQULBeeGGqpkyZpKSkRJ0+naQGDRpq/Pjn9dVX27V48XyFhobq0KEfVKNGTU2YEKegoCC9885b+uCD9+RyudSq1R81ZMhwpaQk6+WXX9CpU6fkdDo1aNBQNW3avMDf/XIIYAAAFHObN/zwi9f6eDw+bd7wg99WwS7JyfHozTfnKz7+C82Z87patrxNc+a8rtmz56ts2UjNnj1LR44c+s1zHDlyWK+88v9UoUK0PvlkrWrV+oMmT35JOTk5euCBe7R//z5J0jfffK233npX5ctHadCg/tq8eZMiIyO1YsW7+sc/Fis0NFQjRw7Xvn179fbbi9SlSze1bh2j06dPa8iQgVqwYKlKlizl1+9/CQEMAIBiLj01K1/jBdG8eUtJUvXqNZSWlipJuu22P2rw4IH64x9vV0xMW9WqVVs7dmy77DnKlo1UpUqV5PH41L59J+3Z842WL1+qQ4d+0Llz55SZmSFJuvHGGqpQ4WKArFr1RqWlperIkcO67bY/Kizs4nsaZ8yYJUnatm2LDh8+rH/8401Jksfj0bFjR1WrVm2/90AigAEAUOyFlQn51bAVVibE73MFBwdLkhwOhyzLkiQ9/viT+v777tq0KV6TJj2jAQMezg1Ol3g8ntyfQ0L+W9e77y7T55//W9263a3evZvphx8O5J730lz/O5/bnTf6nD6dpJCQUHm9Pr366usqUyY8d7xs2Ug/fvO8uAsSAIBirnnMjXK780YCt9up5jE3Bnxuj8ejPn3uVkREhPr2fUidOnXRt9/uV3h4hI4fP6asrCylpp5TQsJXv/r5rVs3q1u3nurQobMkh7777lv5fL5fPVaSbr21ob78cqMyMjLk8Xj03HNPa9++PWrcuInef/+fkqQffjiofv36KCvrQiC+siRWwAAAKPYu7fMK1F2Qv8XtdmvgwEF6/PEhCgkJVVhYaY0f/5yioiqoZcvb1Lfvn3TddZV0660Nf/Xzf/rT/Zo2bYqWLVuskiVL6eab6+vEieOqXLnKrx5fu3Yd9ez5Jz3yyEPy+SzFxNyhpk2b68Ybq2vq1Dj169dHlmVp/PjnA7b/S5Ic1qV1ukIiOTldPl/gS46KKq2kpLSAz1OU0UP/oI/+QR/9gz76R6D7ePLkYUVHVw3Y+a8FbrfzFzcO2OnnPXc6HSpXLuyyx3MJEgAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAAMadOHFcvXt3tbsM2xDAAAAADONJ+AAAQIf3btXX8R8pI+2MSpYuq/qt71LVm5r65dw7dmzTvHmz5XK5lZh4UnXr1lO/fgOVlZWlCRPG6uDBAypduoymTJmm8PAI/ec//6c5c16XZflUqVJljRo1TpGR5dS7d1d17BirLVs26cKFC3r66YmqU+cmHT36o6ZNm6LU1HMKCQnViBGj9Ic/1PFL7YHCChgAAMXc4b1btfWTZcpIOyNJykg7o62fLNPhvVv9NseePbs1cuRTWrr0PWVlZWvjxnidPXtG9977Zy1evFyRkZH69NN1OnMmRS+//IKmTJmmhQuX6ZZbbtXf/z419zzh4eGaM2eR7r67txYvnidJiouboCFDhmvevLf01FNPa8KEcX6rO1BYAQMAoJj7Ov4jeT05eca8nhx9Hf+R31bBGjRoqBtuqCZJ6tQpVitXrlD58lGqW/dmSdKNN9bQuXNntWfPbt10Uz1dd10lSVK3bj21ePGC3PM0b95KklSjRg199tl6ZWRkaO/ePXrhhedzj8nMzNS5c2cVHh7hl9oDgQAGAEAxd2nl60rHr4bL5cr92eez5HK58oxJkmVZsizfL8a8Xm/u78HBwT/95JBlWfL5fAoODtGCBUtzj0lMPKUyZcL9VnsgcAkSAIBirmTpsvkavxpff71TSUmJ8vl8Wrv2X7krWT9Xt+7N2rNnl06cOC5JWrnyfTVq1Piy5w0LC1OVKtfr449XS5K2bv1SQ4c+7Le6A4UVMAAAirn6re/S1k+W5bkM6XIHqX7ru/w2R/nyUZo8eYKSkhLVtGlzNW3aXEuWLPjFcZGR5TRq1NMaN+5J5eR4FB0drTFjnv3Nc0+YMFkvv/yCli5dJLc7SM8//4IcDoffag8Eh2VZlt1F5Edycrp8vsCXHBVVWklJaQGfpyijh/5BH/2DPvoHffSPQPfx5MnDio6umq/PmLgLcubM2X45nyS53U55PL7fP9CQn/fc6XSoXLmwyx7PChgAAFDVm5r6LXDh99kSwGbOnKk1a9ZIkmJiYvTUU0/ZUQYAADCgUaMmatSoid1lXFOMb8LfuHGj4uPjtWLFCn3wwQfavXu3PvnkE9NlAAAA2Mb4ClhUVJTGjBmTextpjRo1dPz4cdNlAAAA2MbWTfiHDh1Snz59tGzZMlWrVs2uMgAAKFJ2796jSpXytwkfBXP8+GHVq1f3io+3bRP+d999p0GDBmn06NH5Cl/cBVl40EP/oI/+QR/9gz76R6D76PP5rqk7BAPhWrsL0ufz5flv+nt3QdryINbt27erf//+GjlypO6++247SgAAALCN8QB24sQJDR06VNOmTVOXLl1MTw8AAAxKT0/X2LEj8/25uXPf1Ny5bxZ4/hMnjqt3764FPo+/Gb8EOXfuXGVlZenFF1/MHevTp4/uu+8+06UAAICfZB1M0YUdJ2Wdz5GjVJBCG0UrpHpkgc+blpaq77771g8VFi3GA9j48eM1fvx409MCAIDLyDqYosyNRyXvxT3W1vmci79LBQ5hr7zysk6fTtLYsU+qTZvb9c9/vi2fz1Lt2nX0xBOjFRISonXr1mrRormSHLrpproaPfpiTti7d7ceeWSAkpISFRvbVQMHDtLq1au0efNGpaWl6dixo2ratIWefHKMJGnRonlat26NnE6nmjZtoSFDhuepJSUlWS++OEmnTp2Uy+XSww8PVYsWrZSenq7Jk5/V0aNHValSZSUlndILL0zTpEnPqn//v6hZsxayLEv33ddTM2fOVvnyUQXqicTLuAEAKPYu7DiZG75yea2L4wX0+OOjVL58lP7618FateoDvf76PC1YsFRly0bq7bcXKykpUa+99nf9/e8ztWTJcvl8Xm3cGC9JSklJ0auvvqG5c5fo7bcXKyPjvCRp166vNWXKy1q4cJk2bvw/HTjwvTZtild8/BeaO3ex5s17S8eO/agPPngvTy3Tp7+sRo2aaOHCZZo06SVNmfK8UlKSNX/+HN1wQ1UtWbJcAwb8VQcOfC9J6tKlW+5LvhMSvlLlytf7JXxJBDAAAIo963xOvsavxldfbdPRoz9q0KCH1L///YqP36AjRw7rm2++1i233KoKFSpKkp55ZpLatLldktSiRSsFBwcrIiJC4eERSk1NlSTdckt9lSpVSqGhoapUqbJSU89p+/Ztateuo0JCQuV2u9WlSzdt3741Tw07dmzVXXf1kCRVrlxFdeverD17vtG2bZvVsePFfel16tRVjRo1JUlt27bXtm2bdeHCBa1Z85FiY/33cnLeBQkAQDHnKBX0q2HLUSrIb3N4vT61bdtOjz8+SpKUkZEhr9ernTu35znuzJkzuT+7XK7/1uJw6NKjS4ODQ/J8xrIsWZbvZ2OS1+vJM/bLx1hZ8nq9cjqd8vl++UiLEiVKqEWL2/TZZ59q+/atGjlyzJV92SvAChgAAMVcaKNoyeXIO+hyXBwvIJfLJa/Xq4YNG+uLLz7XmTMpsixLf/vbFC1fvlQ33VRPe/Z8o+Tk05Kk1177u+LjN+R7nkaNmurTTz9WVtYFeTwerV698hfvn2zcuIk++ugDSdKxY0e1a1eC6tWrr6ZNm+uTT9ZKkg4c+F4HDx6Qw3GxH126dNPs2bNyV+P8hRUwAACKuUsb7QNxF2RkZDlVrBitV1/9mx566K8aPvwRWZalWrVq64EH+iskJESPPTZSTzwxTD6fVzffXF+xsV21YME/8jXPbbf9Ud99t18DBz4or9ej5s1bqleve5WUlJh7zOOPj9LUqXFavXqVHA6HRo8er/Lly6tfv4F64YWJ6tevjypVqqJy5corJOTiKlv9+g3kcDgUG+vfR1nY+iqiq8GT8AsPeugf9NE/6KN/0Ef/CHQfT548rOjoov0qIn8+Cf/jj1fruusqqX79Bjp58qSGDXtY77zzgRwOhw4ePKDJk6wf7G8AABdhSURBVJ/V/PlLf/McP+/57z0JnxUwAABQrFWtWk0vvzxFPp9XTqdLo0aNk9Pp1DvvvKWlSxdr0qQXf/8k+UQAAwAAxVqdOnU1d+7iX4zfe++fde+9fw7InGzCBwCgCCpkO4wKtYt3YDp+97j/RQADAKCIcbuDdf58KiHsVzjlu/jP4ZNDBeuPZVnyeHJ09uxpBQeH5uuzXIIEAKCIKVs2SmfOJCk9/azdpQTM5Z7d9VscsuTLTJPlyZEckjOklBxBoQWKYU6nSyVKhCksLDxfnyOAAQBQxLhcbpUvf53dZQRUfu8kDQ6SPDtX6dx/8r6eqNJfpyvdWVamFwu5BAkAAIo8ty9HFw5/84vxrBMH5XKZj0MEMAAAUOR5nMEqUa3+L8ZDrqshr9c/zxPLDwIYAAAo8rJzLJVq2FElajaWJDmCQhXZ7iF5g8KMX36U2AMGAACKibTsIJXpMFiRHT2SHMqygpXhyd/jI/yFAAYAAIoFy7KU4XHrWog/XIIEAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhhHAAAAADCOAAQAAGEYAAwAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhhHAAAAADCOAAQAAGEYAAwAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADCMAAYAAGCYbQEsPT1dd911l44ePWpXCQAAALawJYAlJCTovvvu06FDh+yYHgAAwFa2BLDly5drwoQJqlChgh3TAwAA2MphWZZl1+Rt27bVokWLVKVKFbtKAAAAMM5tdwH5lZycLp8v8JkxKqq0kpLSAj5PUUYP/YM++gd99A/66B/0seCu9R46nQ6VKxd2+b8brAUAAAAigAEAABhHAAMAADDM1j1g//73v+2cHgAAwBasgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhhHAAAAADCOAAQAAGEYAAwAAMIwABgAAYBgBDAAAwDD3lRyUnZ2tDRs26Pz585Ikr9erI0eOaMSIEQEtDgAAoCi6ogA2YsQI/fjjj0pKSlLdunWVkJCgZs2aBbo2AACAIumKLkHu3btX77//vu68806NGzdOy5YtU1paWqBrAwAAKJKuKIBVqFBBbrdb1apV07fffquaNWsqMzMz0LUBAAAUSVcUwEqWLKlVq1apTp06WrNmjfbv36+zZ88GujYAAIAi6YoC2LPPPqt9+/bptttuk9PpVN++fTVw4MBA1wYAAFAkXdEm/GrVqun222+Xw+HQc889p61bt6p9+/aBrg0AAKBIuqIVsOnTp+vVV1+VJF24cEFz5szRrFmzAloYAABAUXVFAWz9+vWaN2+eJCk6OlpLlizR6tWrA1oYAABAUXVFASwnJ0dBQUG5vwcFBcnhcASsKAAAgKLsivaANWrUSCNHjlTv3r3lcDi0YsUK3XrrrYGuDQAAoEi6ohWwZ555RlFRUZoyZUruz08//XSgawMAACiSrmgF7OTJk9q6dau+//57WZalnTt36syZMypRokSg6wMAAChyrmgFbOzYsbrnnnuUkJCghIQEdezYkRUwAACAq3RFASwzM1N9+vRRUFCQgoOD1bdvX50+fTrQtQEAABRJVxTArr/+eu3YsSP392+//VZVqlQJWFEAAABF2RXtATt16pT69u2r2rVry+12a8+ePYqKilLXrl0lSatWrQpokQAAAEXJFQWwp556KtB1AAAAFBtXFMCaNWsW6DoAAACKjSvaAwYAAAD/IYABAAAYRgADAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhhHAAAAADCOAAQAAGEYAAwAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADCMAAYAAGCYLQFs1apVio2NVfv27fXWW2/ZUQIAAIBt3KYnPHXqlKZPn673339fwcHB6tOnj5o3b66aNWuaLgUAAMAWxlfANm7cqBYtWigiIkIlS5ZUx44dtXbtWtNlAAAA2MZ4AEtMTFRUVFTu7xUqVNCpU6dMlwEAAGAb45cgLcv6xZjD4bjiz5crF+bPcn5TVFRpY3MVVfTQP+ijf9BH/6CP/kEfC64w99B4AKtYsaK2bduW+3tiYqIqVKhwxZ9PTk6Xz/fLEOdvUVGllZSUFvB5ijJ66B/00T/oo3/QR/+gjwV3rffQ6XT85qKR8UuQrVq10qZNm5SSkqLMzEytW7dObdq0MV0GAACAbWxZARsxYoQefPBB5eTkqHfv3qpfv77pMgAAAGxjPIBJUteuXdW1a1c7pgYAALAdT8IHAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhhHAAAAADCOAAQAAGEYAAwAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAOAnDodDTqcjYOd3Oh2Sy6XjSenKkUMOl0uOwE2Ha5jb7gIAALCbwyEpxKuzWed07kKqqoZXkTMnSF6P5d+JXE7FzduiA8fOSZIa16mgR3vfKnm8/p0H1zwCGAAAIV69uWOxEk7ukSSVcIcq7s6nVMJZWj6ff0JYUJBLn+44lhu+JGn7vkQdOHZOtSqVkdfr88s8KBy4BAkAKNYcDofOZJ3NDV+SlOm5oLd3fSi5/ReKLEkHjp79xfiBY+fkcnEdsrghgAEAijWnUzp3IfUX4ykXzsonP14atCzd3qjKL4ab14tWTg6rX8UNAQwAUKx5vZaqRVRRqDskz3i7G1vLbQX7bR6Px6calcM1oGs9lS0dogplS2jk/Y1UpmSQLMvPe81wzWMPGACg2HPmBCnuzqe09OsPdObCObWr/kc1qHiLPBf8uzJleby6vUEl3d6oinJyvApyOuRhA36xRAADABR7Xo9U0llGf7n1z/LJJ7cV7PfwdYknx6uoqJJKSkqThyuPxRYBDAAA6eLdjtlOSU55RDJCYLEHDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDDgGuB2O+V0OuwuAwBgCAEMsJHL6ZTL4dTh71OUfjZLQW6XHOQwACjyeBk3YBO326nUlAta/MaXF18CLKlW3QrqdPfN8ni9NlcHAAgkVsAAm1g+6ZNVe3LDlyR9tydROdmELwAo6ghggI0yM3J+MZad5eEyJAAUcQQwwCZOl0NNWlXNM1Y6PFRhZUJkWZf5EACgSGAPGGCTnByv6ja4TqElg7Vr21FFRpXSbW1ryBLpCwCKOgIYYKPsHK+q1YpU1RqRcjod8ni9sjx2VwUACDQCGGAzj8cnSfL6bC4EAGAMe8AAAAAMI4ABAAAYRgADAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhtkWwGbMmKHXXnvNrukBAABsYzyApaWlady4cZo3b57pqQH8BrfbKZfLYXcZAFAsGA9g69evV7Vq1fTQQw+ZnhrAr3C7nSoTHCTnyfMKOpet0iFBcjoJYgAQSA7Lsiw7Jr50+XHYsGF2TA/gJ9lnM/Xjop3yZXslScHlS6ryPTfLXSrY5soAoOgK2Mu416xZoylTpuQZq169uhYsWFCg8yYnp8vnC3xmjIoqraSktIDPU5TRQ/8IZB9Dg1w6v+lobviSpOzTGco8maacsqHyFqE3hPP/R/+gj/5BHwvuWu+h0+lQuXJhl/17wAJY586d1blz50CdHoAfOCzJm5Hzi3FvhkeOSBsKAoBigsdQAMVYjiyFN7ouz5gjyKkSN4TL4yk6q18AcK0J2AoYgGufx+NTUNlQXdezrs5tPy5nqEtlW96gTJ/39z8MALhqtgUwNt8D14bMHK/cZUMU3q66LId03uuTz2PLvTkAUGywAgZAHo9PHruLAIBihD1gAAAAhhHAAAAADCOAAQAAGEYAAwAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhhHAAAAADCOAAQAAGEYAAwAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhhHAAAAADCOAAQAAGEYAAwAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwzHsC2b9+uXr16qXv37urXr5+OHTtmugQAAABbGQ9go0aNUlxcnD788EN17dpVkydPNl0CAACArYwGsOzsbD322GOqU6eOJKl27do6ceKEyRIAAABs57Asy7JjYp/Pp8GDB+uWW27Ro48+akcJAAAAtnAH6sRr1qzRlClT8oxVr15dCxYsUHZ2tsaMGSOPx6NBgwbl67zJyeny+QKfGaOiSispKS3g8xRl9NA/6KN/0Ef/oI/+QR8L7lrvodPpULlyYZf9e8ACWOfOndW5c+dfjJ8/f16DBw9WRESEXn/9dQUFBQWqBAAAgGuSLZvwq1atqhkzZig4ONj09AAAALYL2ArYr9mzZ4/Wr1+vmjVrqkePHpKkChUqaM6cOSbLAAAAsJXRAFa3bl3t37/f5JQAAADXHJ6EDwAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwzG13ASgcXC6nnMqSQz5ZcsqnEHm9PrvLAgCgUCKA4Xe5XA7lZCTp47dnKTUlSZEVK6vT/UPlcpchhAEAcBW4BInf5XJka/WiGUpNSZIkpZw6po/fniWnsmyuDACAwokAht/l8+TofNrZPGPJJ4/KIcumigAAKNwIYPhdTneQQkuVzjMWERUtSw6bKgIAoHAjgOF3+axgdf7zMJUIKyNJCguPVMc+Q2Q5QmyuDACAwolN+PhdHq+lkuHXqffgCfL5PHI63fIqWB4PG/ABALgaBDBcEY/XkhR08Z9XEvu/AAC4alyCBAAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGue0uIL+cTkeRnKuooof+QR/9gz76B330D/pYcNdyD3+vNodlWZahWgAAACAuQQIAABhHAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwjAB2Gdu2bVPPnj3VtWtXPfLIIzp37pzdJRVK27dvV69evdS9e3f169dPx44ds7ukQm3GjBl67bXX7C6j0Fm1apViY2PVvn17vfXWW3aXU2ilp6frrrvu0tGjR+0updCaOXOmunTpoi5dumjq1Kl2l1NozZgxQ7GxserSpYvmz59vdzlXhQB2GWPHjtXUqVO1atUq1axZU3PnzrW7pEJp1KhRiouL04cffqiuXbtq8uTJdpdUKKWlpWncuHGaN2+e3aUUOqdOndL06dO1dOlSffjhh3rnnXf0/fff211WoZOQkKD77rtPhw4dsruUQmvjxo2Kj4/XihUr9MEHH2j37t365JNP7C6r0NmyZYu+/PJLrVy5Uu+9954WL16sgwcP2l1WvhHALmP16tWqWbOmcnJydOrUKZUpU8bukgqd7OxsPfbYY6pTp44kqXbt2jpx4oTNVRVO69evV7Vq1fTQQw/ZXUqhs3HjRrVo0UIREREqWbKkOnbsqLVr19pdVqGzfPlyTZgwQRUqVLC7lEIrKipKY8aMUXBwsIKCglSjRg0dP37c7rIKnWbNmmnRokVyu91KTk6W1+tVyZIl7S4r3whglxEUFKT9+/crJiZGmzdvVpcuXewuqdAJDg5W9+7dJUk+n08zZ85Uu3btbK6qcOrRo4cefvhhuVwuu0spdBITExUVFZX7e4UKFXTq1CkbKyqc4uLi1KRJE7vLKNRq1aqlBg0aSJIOHTqk1atXKyYmxuaqCqegoCC9+uqr6tKli1q2bKmKFSvaXVK+FfsAtmbNGrVp0ybPv/79+0u6uGKzceNGDRkyRCNGjLC30Gvcb/UxOztbTz75pDwejwYNGmRvode43+ojro5lWb8YczgcNlQCXPTdd99pwIABGj16tKpVq2Z3OYXW8OHDtWnTJp04cULLly+3u5x8c9tdgN06d+6szp075xnLysrSp59+mrta061bN7300kt2lFdo/FofJen8+fMaPHiwIiIi9PrrrysoKMiG6gqPy/URV69ixYratm1b7u+JiYlcRoNttm/fruHDh2vcuHFcWblKBw4cUHZ2tm666SaVKFFCHTp00P79++0uK9+K/QrYr3G73Zo4caK++eYbSRdXJRo1amRzVYXTqFGjVLVqVc2YMUPBwcF2l4NiqFWrVtq0aZNSUlKUmZmpdevWqU2bNnaXhWLoxIkTGjp0qKZNm0b4KoCjR49q/Pjxys7OVnZ2ttavX6/GjRvbXVa+FfsVsF/jcrk0ffp0Pfvss/J6vapYsaLi4uLsLqvQ2bNnj9avX6+aNWuqR48eki7uv5kzZ47NlaE4qVixokaMGKEHH3xQOTk56t27t+rXr293WSiG5s6dq6ysLL344ou5Y3369NF9991nY1WFT0xMjBISEtSjRw+5XC516NChUAZah/VrGyQAAAAQMFyCBAAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAzjQawAirTNmzdr6tSpqlixon788UeFhobqxRdfVHR0tCZPnqwdO3bI5XKpXbt2GjFihA4dOqTnn39eGRkZSkxMVJ06dfTKK68oJCTE7q8CoAghgAEo8vbs2aOxY8eqSZMmevvttzVq1Cg1bdpUWVlZWr16tbxerwYMGKAtW7bo888/V48ePdS9e3fl5OSoZ8+e+vzzz9WxY0e7vwaAIoQABqDIq1Onjpo0aSJJ6tWrl55//nnl5ORo7NixcrlccrlcWrJkiSSpadOm+s9//qM5c+bo0KFDSkxMVEZGhp3lAyiCCGAAijyXy5Xnd8uylJGRIYfDkTt24sQJhYaGauLEifJ6vercubNuv/12nThxQryxDYC/sQkfQJG3b98+7du3T5L0zjvvqFGjRurYsaNWrFghn8+n7OxsDR8+XFu3blV8fLyGDh2q2NhYORwOJSQkyOv12vwNABQ1rIABKPLKly+vV155RceOHVNkZKSmTp2qyMhIxcXFqXv37vJ6vYqNjVWHDh2UlJSkoUOHKjw8XCVKlFDTpk115MgRu78CgCLGYbG2DqAI27x5syZNmqSPPvrI7lIAIBeXIAEAAAxjBQwAAMAwVsAAAAAMI4ABAAAYRgADAAAwjAAGAABgGAEMAADAsP8Pd7EVaRv7ngwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import texthero as hero\n",
    "\n",
    "df_vocab['pca'] = hero.pca(df_vocab['vector'])\n",
    "#hero.scatterplot(df_vocab, 'pca', color='vocab')\n",
    "\n",
    "from matplotlib.pyplot import rcParams\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "import seaborn\n",
    "seaborn.scatterplot(x=df_vocab['pca'].str[0], y=df_vocab['pca'].str[1], hue=df_vocab['vocab']);\n",
    "\n",
    "#hero.scatterplot(df, 'pca', color='stars');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Subwords and contextualized word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec has been around for about a decade now. Word2Vec has some limitations, for instance it cannot handle words not present in the vocabulary used during training.\n",
    "\n",
    "\n",
    "**Papers**: \n",
    "\n",
    "- [Elmo: deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)\n",
    "- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "\n",
    "**Resources and extra lectures**: \n",
    "\n",
    "- [How is GloVe different from word2vec?](https://www.quora.com/How-is-GloVe-different-from-word2vec)\n",
    "[ELMo vs. BERT vs. Word2vec vs. GloVe](https://www.quora.com/What-are-the-main-differences-between-the-word-embeddings-of-ELMo-BERT-Word2vec-and-GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: What are the main differences between Word2Vec and FastText ?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "- Word2vec treat words as the smallest unit to train on\n",
    "- FastText treats each word as **composed of character ngrams**\n",
    "- FastText generates better word embeddings for rare words\n",
    "- FastText work with out–of–vocabulary (oov) words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: What's the main difference between Word2vec/Glove and Elmo?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "- Word2Vec (and Glove) word embeddings are context independent\n",
    "- With Elmo, the same word in two different context will have different vectors\n",
    "- Word2vec and Glove do not take into account word order in their training\n",
    "- Once trained, we can discard model with Word2Vec and Glove but we need to keep it with Elmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now represent all review using Glove. To do so, we will make use of [Flair](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md), a recently published python package that allow to easily represent any text using different embeddings.\n",
    "\n",
    "Q3: Using `WordEmbeddings('glove')` represent the word `analytics`, `business` and `pizza`. Then compute the cosine distance between vectors. Which ones are closer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance analytics-business:  0.6769274473190308\n",
      "distance analytics-pizza:  0.9892289051786065\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "from flair.embeddings import WordEmbeddings\n",
    "from flair.data import Sentence\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "\n",
    "analytics = Sentence('analytics')\n",
    "glove_embedding.embed(analytics)\n",
    "analytics_np = analytics[0].embedding.numpy()\n",
    "\n",
    "business = Sentence('business')\n",
    "glove_embedding.embed(business)\n",
    "business_np = business[0].embedding.numpy()\n",
    "\n",
    "pizza = Sentence('pizza')\n",
    "glove_embedding.embed(pizza)\n",
    "pizza_np = pizza[0].embedding.numpy()\n",
    "\n",
    "print(\"distance analytics-business: \", distance.cosine(analytics_np, business_np))\n",
    "print(\"distance analytics-pizza: \", distance.cosine(analytics_np, pizza_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now we represented only single words, but what if we want to represent documents? Here you are asked to represent 10'000 different reviews, and sort them with respect to their cosine distance to the first review.\n",
    "\n",
    "Q4: First, define a function `embed_review` that given a review it returns the respective embed. For this part, you can use [Document Pool Embeddings](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_5_DOCUMENT_EMBEDDINGS.md) from Flair: `DocumentPoolEmbeddings([glove_embedding])`.\n",
    "Test your function with the following review: \"Restaurant was fabolous\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.66933241e-02, -5.56396663e-01,  1.20346665e-01, -1.11973338e-01,\n",
       "        1.04716666e-01,  3.58339958e-02,  4.94163364e-01,  4.51049991e-02,\n",
       "        3.79222989e-01,  4.73299980e-01,  8.33370015e-02,  4.71833386e-02,\n",
       "        3.73263329e-01, -2.21253335e-01,  2.71760017e-01,  3.98333259e-02,\n",
       "        2.60616988e-01, -3.92656654e-01, -6.09496653e-01,  2.15656683e-01,\n",
       "        3.42953324e-01,  1.41179338e-01,  9.83295068e-02, -5.61051071e-01,\n",
       "        1.99090004e-01,  1.69324994e-01, -3.80899996e-01, -2.18599990e-01,\n",
       "       -1.28083333e-01, -2.39233430e-02, -1.43140003e-01,  6.18003309e-01,\n",
       "        2.43299320e-01,  5.81566632e-01,  6.24503382e-02,  2.65113324e-01,\n",
       "       -5.40033318e-02,  1.77274346e-01,  2.21469685e-01, -4.31127310e-01,\n",
       "        4.97676700e-01,  2.52333283e-02,  2.41698742e-01,  1.04423344e-01,\n",
       "        5.22879660e-01,  2.40914330e-01, -2.45366693e-01, -1.21610999e-01,\n",
       "        4.70633358e-01, -4.67902035e-01, -4.40006644e-01, -2.33179986e-01,\n",
       "        1.93666723e-02,  5.97223341e-01, -7.86930025e-01, -1.58939993e+00,\n",
       "       -4.39949661e-01, -1.84886649e-01,  4.01833326e-01,  3.98486704e-01,\n",
       "       -1.67640015e-01,  5.07163346e-01,  8.07933286e-02,  7.87566602e-02,\n",
       "        4.65440005e-01, -4.43812013e-01,  3.90460014e-01,  1.04683340e-01,\n",
       "        9.49833319e-02,  1.43583313e-01, -5.01310349e-01,  2.47083351e-01,\n",
       "       -2.21939925e-02,  2.21629992e-01,  8.97966623e-02,  3.05683643e-01,\n",
       "        2.95543343e-01,  1.73456669e-01, -4.36293334e-01, -2.63754994e-01,\n",
       "        1.26366302e-01,  6.09899946e-02, -2.18100667e-01,  3.79136652e-02,\n",
       "       -4.67563361e-01, -4.22161013e-01, -3.01696688e-01, -3.03850025e-01,\n",
       "       -5.28700054e-02,  5.39978326e-04,  3.06976676e-01, -3.80608678e-01,\n",
       "        1.14586331e-01,  8.03600028e-02, -3.26580644e-01,  7.86633417e-02,\n",
       "        1.99976657e-02, -3.71927023e-01,  4.55203295e-01,  3.51299942e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.embeddings import DocumentPoolEmbeddings\n",
    "\n",
    "docpoolembed = DocumentPoolEmbeddings([glove_embedding])\n",
    "\n",
    "def embed_review(review):\n",
    "    s = Sentence(review)\n",
    "    docpoolembed.embed(s)\n",
    "    return s.embedding.numpy()\n",
    "\n",
    "embed_review(\"Restaurant was fabolous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Apply to the `text` column your previously created method and store the result in a new column `embed`. \n",
    "\n",
    "> ☝️This operation might take a while, we use `tqdm` to show a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████| 10000/10000 [00:24<00:00, 405.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       [-0.14973384, 0.105061956, 0.35441896, -0.0065...\n",
       "1       [0.0014385143, 0.14520112, 0.2159496, -0.34776...\n",
       "2       [-0.19049452, 0.20583253, 0.2609278, -0.240894...\n",
       "3       [-0.18839265, 0.33528236, 0.24974221, -0.26905...\n",
       "4       [-0.12573816, 0.31910342, 0.14125381, -0.14221...\n",
       "                              ...                        \n",
       "9995    [-0.22443968, 0.33964384, 0.3370281, -0.230378...\n",
       "9996    [-0.21270736, 0.21685344, 0.30420268, -0.26460...\n",
       "9997    [-0.108463414, 0.16915984, 0.37964147, -0.3303...\n",
       "9998    [-0.12619269, 0.06759783, 0.20094448, -0.12893...\n",
       "9999    [-0.03584042, 0.09587897, 0.33155766, -0.27944...\n",
       "Name: embed, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df['embed'] = df['text_clean'].progress_apply(embed_review)\n",
    "df['embed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: Sort the review by cosine distance w.r.t the first review and visualize on screen the first three reviews. Do they look similar?\n",
    "\n",
    "> ☝️ The function below compute the cosine similarity between a vector and every row of the given numpy matrix. You will need to use `np.array(list(df['embed']))` to transform the Pandas Series into a NumPy matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def cosine_similarity(vector, matrix):\n",
    "    \"\"\"\n",
    "    Compute the cosine distances between the vector and each row of the matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(matrix.shape)\n",
    "    \n",
    "    v = vector.reshape(1, -1)\n",
    "    return scipy.spatial.distance.cdist(matrix, v, 'cosine').reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100)\n",
      "Omar is great!  He can diagnose and correct any cell phone issue - professional and patient - ideal for buyers/customers with limited technology knowledge.  Thank you Omar for all your help.  Jana and Lorraine\n",
      "\n",
      "\n",
      "Just wanna quick thanks to the person repair my laptop, its working great and faster than ever before. Also, the information provided to me is the greatest thing and i know  how to keep up the machine up-to-date. Thanks again\n",
      "\n",
      "\n",
      "They person/company they sent out to provide a quote didn't even show up at the scheduled time.  No call or e-mail in advance.  Will never use this company if they can't even get that part right.  Embarrassed for them!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_review_vec = df.iloc[0]['embed']\n",
    "df['cosine_similarity'] = cosine_similarity(first_review_vec, np.array(list(df['embed'])))\n",
    "\n",
    "most_similar = df.sort_values('cosine_similarity')[:3]\n",
    "for i, review in most_similar.iterrows():\n",
    "    print(review['text'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: Look now at the farthest review. What's that about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'挺不错的 味道都挺好 食物卖相也不错 唯一遗憾蒸蛋没啦 跑来中国城吃饭 周围也蛮好停车的 点了一个面一个红油抄手 一个小菜 又送了一个木耳 酸酸辣辣的 很多地方的红油抄手都会偏甜 就感觉很不正宗 但是他们家的完全不会 所以觉得很好哦 然后面里的炒黄豆很好吃'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values('cosine_similarity')['text'].iloc[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
