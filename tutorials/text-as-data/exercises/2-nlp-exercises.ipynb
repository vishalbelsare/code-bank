{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSFM text-as-data workshop\n",
    "\n",
    "## 2. Basic of Natural Language Processing \n",
    "\n",
    "Creator: [Data Science for Managers - EPFL Program](https://www.dsfm.ch)\n",
    "\n",
    "Source: [https://github.com/dsfm-org/code-bank.git](https://github.com/dsfm-org/code-bank.git)\n",
    "\n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "When dealing with text data, we can look at it under different perspectives. We can, for instance, look at a single sentence to study and capture the linguistic level. This include for instance finding named entities (named entity recognition) or finding part of speeech tags (verb, adverb, ...). \n",
    "\n",
    "Another approach is to consider the whole document as a single entity and look for similar documents, one simple solution being count the words that documents share in commons. This action is equivalent to represent a document as a vector and compute distances between vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: NLP with spaCy\n",
    "\n",
    "We already used spaCy ([spacy.io](https://spacy.io/)) in the previous notebook to tokenize the text and find the stopwords. spaCy's catchphrase is \"Industrial strength NLP in Python\". spaCy is known to be fast and simple to use.\n",
    "\n",
    "Spacy can help to answer these questions:\n",
    " - What this text is talking about?\n",
    " - What do the words mean in this context?\n",
    " - What companies and products are mentioned?\n",
    "    \n",
    "The main features of spaCy are:\n",
    " - Tokenization\n",
    " - Part-of-speech (POS) Tagging. The action to assign word types to tokens, like verb or noun.\n",
    " - Dependency Parsing. A tool to describe relations between individual tokens (see next).\n",
    " - Named Entity Recognition. Find entities such as person name or firm name in a text.\n",
    "\n",
    "Resources: \n",
    " - [spaCy 101](https://spacy.io/usage/spacy-101), the official getting-started tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Load the `review_clean.csv` CSV file into a Pandas DataFrame `df` and display the first 5 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "df = # YOUR CODE HERE #\n",
    "df.head(# YOUR CODE HERE #)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Store in a `first_review` variable the first review and display it on screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_review = # YOUR CODE HERE #\n",
    "first_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech (POS) tagging is the process of assigning grammatical properties (noun, verb, adjective, adverb,  etc.) to words. spaCy models use both the definition of the words and its context to determine the right tag.  \n",
    "\n",
    "Q3: Using spaCy, apply POS tagging to the first and look at the results. What is the part of speech tag for the `-` token? What about `phone` and `Jana`?\n",
    "\n",
    "> ☝️At line 11 we are overwriting the default printing function with another one from the [rich](https://github.com/willmcgugan/rich) library. This allows to pretty print the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(# YOUR CODE HERE #)\n",
    "\n",
    "pos = []\n",
    "\n",
    "for token in doc:\n",
    "    pos.append(# YOUR CODE HERE #)\n",
    "\n",
    "from rich import print\n",
    "print(pos[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Using [spaCy visualizer](https://spacy.io/usage/visualizers), display the dependency parse tree of the first sentence of the first review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = # YOUR CODE HERE #\n",
    "\n",
    "print(f\"First sentence is: {first_sentence}\")\n",
    "\n",
    "doc = nlp(# YOUR CODE HERE #)\n",
    "\n",
    "from spacy import displacy\n",
    "displacy.render(# YOUR CODE HERE #, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Using the spaCy recognizer, look at the named entities of the first and second reviews. Which information can you get it out of it? Can you use a similar function in your daily job?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(# YOUR CODE HERE #)\n",
    "displacy.render(# YOUR CODE HERE #, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_review = # YOUR CODE HERE #\n",
    "doc = nlp(# YOUR CODE HERE #)\n",
    "displacy.render(# YOUR CODE HERE #, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vector Space\n",
    "\n",
    "As machines cannot understand human languages as we do, we are required to somehow transform text data into a numeric format. The idea is to _map_ every review to a numeric vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 (**theory**): You just received 10 thousands new contracts. You need to categorize them in 10 different sub-categories in an efficient way. What do you do? You don't have access to any information a priori, neither your data have some metadata nor labels. Describe in layman terms how you would proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**\n",
    "\n",
    "1. Count the word occurrence in each document and create a document-term matrix count.\n",
    "1. Apply a clustering algorithm such as k-means (with k = 10 in this case) and find the different clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: [Texthero](https://texthero.org/) is a simple toolkit to preprocess and analyze text-based dataset. Texthero is still in beta and therefore some parts might change in future releases.\n",
    "\n",
    "With the aid of Texthero, represent each reviews by counting words. Select only the first 500 most common words.\n",
    "\n",
    "If you need help, you can have a look at the [getting-started](https://texthero.org/docs/getting-started) tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import texthero as hero\n",
    "\n",
    "df['count'] = # YOUR CODE HERE #\n",
    "df['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: By applying principal component analysis, reduce the dimension of the vector space to two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pca'] = # YOUR CODE HERE #\n",
    "df['pca']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Visualize the obtained vector space, can you identify any pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import rcParams\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "import seaborn\n",
    "seaborn.# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Find the most similar reviews to the second review. For this, you will need to compute the distance between every review and pick the closet one. You can use the [cosine_similarity](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) function from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "first_review_vector = # YOUR CODE HERE #\n",
    "first_review_vector\n",
    "\n",
    "cosine_similarity(\n",
    "            np.asarray(list(df['count'])), np.array(# YOUR CODE HERE #).reshape(1, -1)\n",
    "        ).reshape(1, -1)[0].argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[# YOUR CODE HERE #]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[# YOUR CODE HERE #]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[# YOUR CODE HERE #]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Topic modelling\n",
    "\n",
    "\n",
    "Topic modeling is a unsupervised learning method. The goal is to find group of different document of the same \"topic\". Topic Models are useful for uncovering hidden structure in a collection of texts. There are two common algorithms: Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).\n",
    "\n",
    "There are different python libraries that can be used to compute topic modeling, Gensim and Scikit-learn are very common. Gensim documentation is not always crystal clear and can be complex to use in some scenario. For this part, we will use scikit-learn, in particular [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [LatentDirichletAllocation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Store into a variable `reviews` all reviews and compute the \"review-term\" matrix (`review_term_matrix`) using CountVectorizer. Then, display the shape of the obtained matrix. Does it look like what you expected?\n",
    "\n",
    "> ☝️ For a faster computation, you can limit the number of terms to 500 (`max_features=500`).\n",
    "\n",
    "> ☝️ Make sure you use the \"text_clean\" column with stopwords removed (otherwise stopwords will pollute the topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "reviews = list(df['text_clean'])\n",
    "\n",
    "vectorizer = # YOUR CODE HERE #\n",
    "review_term_matrix = # YOUR CODE HERE #\n",
    "\n",
    "review_term_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Apply the LDA algorithm to the obtained `review_term_matrix`. You will need to specify the number of topics you want to compute as well as the number of iterations for the LDA algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(# YOUR CODE HERE #)\n",
    "lda.fit(review_term_matrix);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: The below function `print_top_words` display on screen the top words in each \"cluster\". Display the most common 15 words for each cluster. What do you notice?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(scikit_learn_model, feature_names, num_top_words):\n",
    "    for topic_num, topic in enumerate(scikit_learn_model.components_):\n",
    "        print(f\"Topic #{topic_num}\")\n",
    "        print(\" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "    print()\n",
    "    \n",
    "print_top_words(# YOUR CODE HERE #)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: If you wish, you can play around with the obtained topic modelling by executing this lines of code:\n",
    "\n",
    "> ☝️[PyLDAvis](https://github.com/bmabey/pyLDAvis) is a beautiful and simple library to visualize topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "# topic_vis_data = pyLDAvis.sklearn.prepare(lda, review_term_matrix, vectorizer)\n",
    "# pyLDAvis.display(topic_vis_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
