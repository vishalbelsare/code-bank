{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSFM text-as-data workshop\n",
    "\n",
    "## 1. Text preprocessing and Exploratory Data Analysis\n",
    "\n",
    "Creator: [Data Science for Managers - EPFL Program](https://www.dsfm.ch)\n",
    "\n",
    "Source: [https://github.com/dsfm-org/code-bank.git](https://github.com/dsfm-org/code-bank.git)\n",
    "\n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data are ubiquitous and most of us have to deal with it on a daily job. In this workshop, we will learn how to work with text data under different aspects.\n",
    "\n",
    "Natural Language Processing, the interactions between computers and human languages, is widely used in many different fields including speech recognition, natural language understanding, and natural language generation.\n",
    "\n",
    "Imagine you want to consolidate the brand identity of your firm and you want to understand what your clients are saying about your brand and your products. You are given a long list of reviews of both your company as well as other firms, what do you do? In this workshop we will see how we can extract insights from such data as well as understand which reviews are positive and which are negative.\n",
    "\n",
    "This workshop is composed of four different Jupyter Notebooks. All four make use of the same dataset, [Yelp Dataset](https://www.kaggle.com/yelp-dataset/yelp-dataset). This dataset is a subset of Yelp's businesses, reviews, and user data. It was originally put together for the Yelp Dataset Challenge which is a chance for students to conduct research or analysis on Yelp's data and share their discoveries. In the dataset, you'll find information about businesses across 11 metropolitan areas in four countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook you will learn how to:\n",
    "    1. Extract text data from PDF\n",
    "    2. Understand how to analyze both structured and unstructured data such as text data\n",
    "    3. Preprocess and clean text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: PDF Data Extraction\n",
    "\n",
    "Document with text data might have different extensions, PDF is one of the most common. Here, we will extract text data from PDF files and store them into a Pandas DataFrame. If you work with other file types such as power point presentation the procedure is about the same.\n",
    "\n",
    "**Learning objective**: being able to extract text data from pdf and store it into Pandas.\n",
    "\n",
    "**Useful resources**:\n",
    " - [glob](https://docs.python.org/3.7/library/glob.html) a python module from the standard library to find all the pathnames that match a specified pattern\n",
    " - [pdfminer.six](https://github.com/pdfminer/pdfminer.six) a third party python library to work with pdf files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Under `data/pdf_review` you can find a list of reviews in pdf format. Employing the `glob` module, save the name of such files into a list called `pdf_review`. Display on-screen the number of pdf files as well as the first three filenames.\n",
    "\n",
    "> ☝️ The glob module is part of the python standard library. It often helps to find and extract filenames and file paths of multiple sources.\n",
    "\n",
    "> ☝️ Try to open a pdf review with your favorite PDF viewer to get a glance of the raw data itself  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "pdf_reviews = glob.glob(# YOUR CODE HERE #)\n",
    "\n",
    "print(f\"There are {len(pdf_reviews)} pdf files.\")\n",
    "pdf_reviews[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: With the aid of [pdfminer.six](https://github.com/pdfminer/pdfminer.six), iterate over all pdf reviews found under `data/pdf_reviews/`, extract the text and store it into a `text_review` list. Look at the first two reviews. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "text_review = # YOUR CODE HERE #\n",
    "text_review[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** The extracted text data contains some special symbols that will need to be removed and preprocessed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Save the obtained `text_review` into a `pdf_review_df` Pandas DataFrame and look at the first 5 rows.\n",
    "\n",
    "> ☝️When working with Pandas Dataframe is a good practice to prepend the variable name with `_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pdf_review_df = # YOUR CODE HERE #\n",
    "pdf_review_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploratory Data Analysis\n",
    "\n",
    "Before executing any fancy machine learning model, it's useful to have a good understanding of the data we are about to deal with.\n",
    "\n",
    "The previous created DataFrame is composed of only 100 reviews.\n",
    "For this part we will start with a larger dataset composed of 500'000 reviews.\n",
    "\n",
    "**Learning objective**: be able to visualize and analyze both structured (categorical, numerical, etc.) and text data.\n",
    "\n",
    "### Structured data\n",
    "\n",
    "Q1: Load all reviews into a Pandas DataFrame named `df` and display the first five rows.\n",
    "\n",
    "> ☝️ Executing operations on 500k of data might take a while. As a suggestion, you might want to sample about 10k of data, write the code and look at the results and only at the end execute again the code on the whole dataset (Kernel > Restart and Run All). Once the data are loaded, you can use the following snippet of code to sample 10 thousands reviews: `df = df.sample(10000).reset_index(drop=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for high-quality graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = # YOUR CODE HERE #\n",
    "df = # YOUR CODE HERE # # sample 10k\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Even if we are principally dealing with text data, we are still interested in any structured data as this might help with our task. Information such as the kind of business and the city of the business might be useful for instance. You are given a second dataset (`yelp_business.csv`) containing some extra information regarding the business.\n",
    "\n",
    "Load into a Pandas DataFrame named `df_business` the `yelp_business.csv` dataset and look at the first rows. Then, merge it with the initial `df` DataFrame. Which column you need to use to _merge_ the two datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business = # YOUR CODE HERE #\n",
    "df_business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = # YOUR CODE HERE #\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: The `categories` column of the new Dataframe represents the type of business of the review. Identify and count the occurrences for each category and plot it using a bar chart. How many categories are there and what are the most common categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = # YOUR CODE HERE #\n",
    "\n",
    "category_counts[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(category_counts)} unique categories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Most common categories\"\n",
    "category_counts[:40].sort_values().plot.barh(figsize=(8,8), title=title);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: With a bar plot, display the the review's stars. What are average and median values?\n",
    "\n",
    "> ☝️Keep in mind these results, they'll be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Review's ratings\"\n",
    "\n",
    "# YOUR CODE HERE #\n",
    "\n",
    "\n",
    "print(f\"Average rating: {# YOUR CODE HERE #}\")\n",
    "print(f\"Median rating: {# YOUR CODE HERE #}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text data\n",
    "\n",
    "Q5: Add a new column `words_len` corresponding to the number of words of the `text` column. You can use the `str.split()` function to split a string. Then, display the histogram of the new column. What is the average number of words per review?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words_len'] = # YOUR CODE HERE #\n",
    "\n",
    "title = \"Number of words for each review\"\n",
    "df['words_len'].plot.hist(title=title, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average number of tokens is {df['words_len'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Print the most common 20 words. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = # YOUR CODE HERE #\n",
    "\n",
    "NUM_TOP_WORDS = 20\n",
    "top_words[:NUM_TOP_WORDS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** most of the common words are stopwords, i.e words without a special meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Cleaning\n",
    "\n",
    "When dealing with text data, data cleaning is an essential step. In general, data cleaning is both domain and task-dependent. In this section, we will see a universal pipeline valid in most scenarios. Data cleaning is also an iterative process.\n",
    "\n",
    "**Learning objective**: learn the main steps for text preprocessing, as well as experiment with regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions\n",
    "\n",
    "**Useful resources**:\n",
    " - https://www.regular-expressions.info - The Premier website about Regular Expressions\n",
    "\n",
    "Q1: Store the the first review of the the `pdf_review_df` DataFrame in a variable `r`, display it and by using regular expressions extract the `date`, the `categories` as well as the rating `stars`.\n",
    "\n",
    "> ☝️You can use this snippet of code to visualize the first review: `pdf_review_df.iloc[0]['text']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = # YOUR CODE HERE #\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "date = re.findall(# YOUR CODE HERE #, r)\n",
    "print(\"date: \", date)\n",
    "\n",
    "categories = re.findall(# YOUR CODE HERE #, r)\n",
    "print(\"categories: \", categories)\n",
    "\n",
    "stars = re.findall(# YOUR CODE HERE #, r)\n",
    "print(\"stars: \", stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ☝️ Regular expressions are a powerful tool. In most cases, there are many ways of solving the same problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization refers to the act of splitting a sentence into a list of tokens (words). This is an essential step as it will allows us to later map every word to a number that the computer and the machine learning algorithms (regression, classification, deep learning, whatever, ...) can understand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Before tokenizing the whole dataset, let's compare different algorithms on the `review_example`. Start by splitting the first review `r` by the empty space (` `) and look at the result. What do you notice? What's the main issue with this approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(# YOUR CODE HERE #)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers** Some of the tokens such as `first.` should be splitted further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Execute again the tokenization, this time using [spaCy](https://spacy.io/) (\"Industrial-Strength Natural Language Processing\"), a common python package for NLP tasks. How does it look like compared to the previous solution? What are the advantages and disadvantages of the two approaches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "review_example_spacy = nlp(# YOUR CODE HERE #)\n",
    "\n",
    "tokens_spacy = []\n",
    "for token in review_example_spacy:\n",
    "    token = token.text.strip() # remove starting and ending space\n",
    "    tokens_spacy.append(# YOUR CODE HERE #)\n",
    "\n",
    "print(tokens_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers** tokenization with spaCy is slower but it does a better job (in most cases) in separating some tokens (for instance `first.`). Note also that with the second approach the date `2019-07-01` has been splitter further, this might or might not be the best solution, depending on the task one is trying to achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Tokenize the whole DataFrame `df` using spaCy by adding a new colum `tokenized` to the dataset. Display the first five rows. This operation might take a while (3-5 minutes), you can monitor the progress using [tqdm](https://pypi.org/project/tqdm/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    # for a faster operation, we can limit the number of character to 500\n",
    "    # doing that we might split a word that is sub-optimal.\n",
    "    #text = text[:500]\n",
    "    \n",
    "    nlp_text = nlp(# YOUR CODE HERE #)\n",
    "    tokens_spacy = []\n",
    "    for token in nlp_text:\n",
    "        token = token.text.strip() # remove starting and ending space\n",
    "        tokens_spacy.append(# YOUR CODE HERE #)\n",
    "    return tokens_spacy\n",
    "    \n",
    "df['tokenized'] = df['text'].progress_apply(tokenize)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of stopwords\n",
    "\n",
    "As seen previously, some of the most common words in the dataset does not brings any valuable meaning to the reviews. This words are known as _stopwords_.\n",
    "\n",
    "There are two main approaches to remove stopwords:\n",
    " 1. Remove all top words. In this case it's important to carefully choose the threshold.\n",
    " 2. Remove all words that appears in a pre-defined stopwords list.\n",
    " \n",
    "#### Remove all _top_words_\n",
    "\n",
    "Q1: First, create a list of stopwords you want to get rid of. Look at the `top_words` and create a python `set` of _top_words_. Be careful in selecting the right threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_THRESHOLD = # YOUR CODE HERE #\n",
    "stopwords = top_words[:STOPWORDS_THRESHOLD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Define a function `remove_stopwords(tokenized_text, list_stopwords)` that given a tokenized text, removes all `list_stopwords`. Apply this function to the `tokenized` Series and store the results into another column, `without_stopwords`, of the DataFrame.\n",
    "\n",
    "> ☝️In some cases, the tokens in the stopwords list are lowercased. Make sure to take this fact into consideration when developing your own solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_text, list_stopwords):\n",
    "    return # YOUR CODE HERE #\n",
    "\n",
    "remove_stopwords(tokenized_text=[\"is\", \"Beautiful\", \"!\"], list_stopwords=[\"is\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a pre-defined stopwords list\n",
    "\n",
    "A more common approach is to remove all stopwords already pre-defined in a stopwords list. In most cases, this is a more stable solution as this permits to remove all stopwords without the risk of removing important but common words.\n",
    "\n",
    "> ☝️ Historically, one of the most common stopwords list is the one provided by [NLTK](). A more modern and valid alternative is to use the list of stopwords provided by spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Load the spaCy stopwords list and `apply` again the function on the new set as you did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import stop_words as spacy_en_stopword\n",
    "spacy_stopwords = spacy_en_stopword.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['without_stopwords'] = df['tokenized'].apply(lambda row: # YOUR CODE HERE #)\n",
    "df['without_stopwords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Join back the splitted tokens to generate a single string for each cell of the Pandas Series. Look at the head of the DataFrame and finnaly store it as a csv file: `review_clean.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = # YOUR CODE HERE #\n",
    "\n",
    "(\n",
    "    df[[\"text_clean\", \"text\", \"stars\", \"categories\", \"name\", \"address\"]]\n",
    "    .to_csv(\"./data/review_clean.csv\", index=False)\n",
    ")\n",
    "\n",
    "df.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
