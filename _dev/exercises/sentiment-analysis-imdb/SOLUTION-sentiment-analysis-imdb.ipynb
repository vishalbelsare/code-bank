{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DSFM Exercise**: Sentiment Analysis - NLP, Text Embedding (SOLUTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creator: [Data Science for Managers - EPFL Program](https://www.dsfm.ch)  \n",
    "Source:  [https://github.com/dsfm-org/code-bank.git](https://github.com/dsfm-org/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is a challenging subject in machine learning. People express their emotions in a way that can be very ambiguous for both humans and computers. In this demo, we analyze sentiments of a set of IMDB movie reviews. The dataset consists of review texts as well as a binary sentiment label (1: positive, 0: negative). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://barnraisersllc.com/wp-content/uploads/2017/01/Sentiment-Analysis.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "Image source: http://barnraisersllc.com/wp-content/uploads/2017/01/Sentiment-Analysis.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset source: *Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). \"Learning Word Vectors for Sentiment Analysis.\" The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 0**: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all import statements at the top of your notebook\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "\n",
    "# Data science packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection         import KFold, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble                import RandomForestClassifier\n",
    "from sklearn.pipeline                import Pipeline\n",
    "from sklearn.model_selection         import train_test_split\n",
    "\n",
    "# Neural networks\n",
    "from tensorflow.keras.models                 import Sequential, load_model\n",
    "from tensorflow.keras.layers                 import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text     import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# Text processing packages\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem   import SnowballStemmer\n",
    "from gensim.models import doc2vec, word2vec\n",
    "\n",
    "# Visualization packages\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants \n",
    "\n",
    "# Set a seed for replication\n",
    "SEED = 10\n",
    "\n",
    "# Set performance metric\n",
    "SCORE = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested cross validation helper function\n",
    "def nested_cv(X, y, est_pipe, p_grid, p_score, n_splits_inner = 3, n_splits_outer = 3, n_cores = 1, seed = 0):\n",
    "\n",
    "    # Cross-validation schema for inner and outer loops (stratified if it is a classification)\n",
    "    inner_cv = KFold(n_splits = n_splits_inner, shuffle = True, random_state = seed)\n",
    "    outer_cv = KFold(n_splits = n_splits_outer, shuffle = True, random_state = seed)\n",
    "    \n",
    "    # Grid search to tune hyper parameters\n",
    "    est = GridSearchCV(estimator = est_pipe, param_grid = p_grid, cv = inner_cv, scoring = p_score, n_jobs = n_cores)\n",
    "\n",
    "    # Nested CV with parameter optimization\n",
    "    nested_scores = cross_val_score(estimator = est, X = X, y = y, cv = outer_cv, scoring = p_score, n_jobs = n_cores)\n",
    "    \n",
    "    print('Average score: %0.4f (+/- %0.4f)' % (nested_scores.mean(), nested_scores.std() * 1.96))\n",
    "    \n",
    "    return nested_scores.mean(), nested_scores.std() * 1.96\n",
    "\n",
    "# Define a function to split a review into clean list of words\n",
    "def review_to_wordlist(review):\n",
    "    \n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "   \n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    \n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in words if not w in stops]\n",
    "    \n",
    "    stemmer = SnowballStemmer('english')\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    \n",
    "    return(words)\n",
    "\n",
    "# Define a function to split a review into parsed sentences, where each sentence is a word list\n",
    "def review_to_sentences(review, tokenizer):\n",
    "    \n",
    "    raw_sentences = tokenizer.tokenize(review.strip())  \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:      \n",
    "        if len(raw_sentence) > 0:           \n",
    "            sentences.append( review_to_wordlist( raw_sentence ))\n",
    "   \n",
    "    return sentences\n",
    "\n",
    "# Function to average all of the word vectors in a given paragraph\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,), dtype='float32')\n",
    "    nwords = 0.\n",
    "     \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "# Given a set of reviews (each one a list of words), calculate \n",
    "# the average feature vector for each one and return a 2D numpy array\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype='float32')\n",
    "     \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       \n",
    "       # Print a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print ('Review %d of %d' % (counter, len(reviews)))\n",
    "       \n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "       \n",
    "        # Increment the counter\n",
    "        counter = counter + 1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1**: EDA\n",
    "\n",
    "In the first part, we load the `sentiment_data.tsv` dataset. The dataset consists of `id`, `sentiment`, and `review` columns.\n",
    "\n",
    "Hint: This is a *tab*-separated file, but we can still use pandas' `read_csv` function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1**: Load the data. What shape does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('sentiment_data.tsv', header=0, delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2**: Extract the observation at index 0. How does the review text look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample observation\n",
    "print ('id: \\t\\t', data['id'][0])\n",
    "print ('sentiment: \\t', data['sentiment'][0], '\\n')\n",
    "print (data['review'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3**: Plot the distribution of the target and plot a word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target\n",
    "pd.DataFrame.hist(data,column='sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word distributions using the word cloud library\n",
    "\n",
    "# Concatenate all rows of review column\n",
    "text = data['review'].str.cat(sep=' ')\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=1600, height=800).generate(text)\n",
    "\n",
    "# Display the generated image\n",
    "wordcloud = WordCloud(max_font_size=60).generate(text)\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 4**: Check for missing values and drop rows containing missing values (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and drop corresponding rows, if any\n",
    "data.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2**: Text pre-processing\n",
    "\n",
    "In the second part, we pre-process the raw review text in the following steps:\n",
    "\n",
    "1. Remove HTML tags\n",
    "2. Keep only alphabetical terms\n",
    "3. Lowercase all words\n",
    "4. Remove stop words\n",
    "5. Stem words\n",
    "\n",
    "IMPORTANT: Use the review at index 0 for each step to see how it changes. We will pre-process all reviews in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1**: Remove HTML tags with the `BeautifulSoup` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML tags \n",
    "data_no_html = BeautifulSoup(data['review'][0])  \n",
    "\n",
    "print('Original:\\n{}\\n'.format(data['review'][0]))\n",
    "print('Without HTML tags:\\n{}\\n'.format(data_no_html.get_text()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2**: Keep only alphabetical terms/words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only alphabetical terms\n",
    "data_no_digits = re.sub('[^a-zA-Z]', ' ', data_no_html.get_text())  \n",
    "print('No digits:\\n{}'.format(data_no_digits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3**: Lowercase all words and separate text by blank spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case and separate into tokens\n",
    "lower_case = data_no_digits.lower()        \n",
    "words = lower_case.split(' ')\n",
    "print('Lower-cased and token-separated:\\n{}'.format(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 4**: Remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the stop words dataset of NLTK library\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "print('Stop words in NLTK:\\n{}\\n'.format(stopwords.words('english')))\n",
    "words_no_stop = [w for w in words if not w in stopwords.words('english')]\n",
    "print('Without stop words:\\n{}'.format(words_no_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 5**: Stem all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem words\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "print('grows  --> {}'.format(stemmer.stem('grows')))\n",
    "print('leaves --> {}'.format(stemmer.stem('leaves')))\n",
    "print('fairly --> {}\\n'.format(stemmer.stem('fairly')))\n",
    "\n",
    "words_stemmed = [stemmer.stem(w) for w in words_no_stop]\n",
    "print('Stemmed:\\n{}'.format(words_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 6**: Consolidate all steps into a single function called `review_to_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a raw review to a string of words: \n",
    "# the input is a single string (a raw movie review) and\n",
    "# the output is a single string (a preprocessed movie review)\n",
    "\n",
    "def review_to_words(raw_review):\n",
    "\n",
    "    # Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    \n",
    "    # Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    \n",
    "    # In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "     \n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # Stem words\n",
    "    stem_words = [stemmer.stem(w) for w in meaningful_words]\n",
    "    \n",
    "    # Join the words back into one string separated by space and return the result\n",
    "    return( \" \".join( stem_words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of text preprocessing\n",
    "\n",
    "clean_review = review_to_words(data['review'][0])\n",
    "\n",
    "print('Original:\\n{}\\n'.format(data['review'][0]))\n",
    "print('Cleaned:\\n{}'.format(clean_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 7**: Pre-process all reviews with the new function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and parse all movie reviews\n",
    "num_reviews = data['review'].size\n",
    "clean_data_reviews = []\n",
    "\n",
    "for i in range(0, num_reviews):\n",
    "    clean_data_reviews.append(review_to_words(data['review'][i]))\n",
    "    \n",
    "    # If the index is evenly divisible by 1000, print a message to show progress\n",
    "    if (i+1) % 1000 == 0:\n",
    "        print('Review {} of {}\\t{}%'.format(i+1, num_reviews, round((i+1)/num_reviews*100, 0)))\n",
    "    \n",
    "assert len(clean_data_reviews) == len(data), 'Error: the number of cleaned reviews does not match.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3**: Feature Extraction using TFIDF\n",
    "\n",
    "In this short part, we transform the cleaned text into TFIDF representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1**: Transform cleaned reviews to TFIDF using the 5000 most frequent words in the corpus. What shape will the new representation have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming pre-processed reviews to bag of words (BOW) feature representation\n",
    "vectorizer = TfidfVectorizer(max_features = 5000)\n",
    "features = vectorizer.fit_transform(clean_data_reviews)\n",
    "features = features.toarray()\n",
    "print (features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names (each term is a feature)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print (vocab[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2**: Print the TFIDF values of the first 10 words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 10 vocabulary words and their corresponding TFIDF values\n",
    "\n",
    "dist = np.sum(features, axis=0)\n",
    "i = 0\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print(tag.ljust(15), count)\n",
    "    i = i + 1\n",
    "    if i > 10 : \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(features, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the values of feature 0\n",
    "features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 4**: Binary classification using Random Forest with TFIDF Features\n",
    "\n",
    "In this part, we will run a binary classification using Random Forest and the TFIDF features to predict the binary sentiment label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1**: Extract the target vector from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target\n",
    "\n",
    "target = data['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2**: Fit a `RandomForestClassifier` to our data and test different parameter values for `n_estimators` (e.g. from 10 to 50). \n",
    "\n",
    "What nested cross validated accuracy do you get? How long does it take to test the different parameter values?\n",
    "\n",
    "Hint 1: Use the `%%time` magic command at the beginning of the cell for timing.\n",
    "\n",
    "Hint 2: Use the `nested_cv` helper function defined at the very top of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define pipeline\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_clf__random_state = SEED)\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"rf_clf__n_estimators\": [int(i) for i in np.linspace(10.0, 50.0, 5)]}\n",
    "\n",
    "acc_rf, std_rf = nested_cv(X = features, y = target, est_pipe = rf_pipe, p_grid = p_grid, p_score = SCORE, n_cores = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 5**: Word Vectors - Word2Vec\n",
    "\n",
    "A popular vectorization method for words is a technique known as Word2Vec, which is implemented in the `gensim` library. \n",
    "\n",
    "In Word2Vec each word is assigned a low-dimensional vector which is learnt under the assumption that words that are close to each other in a document are semantically related. Word2Vec can be used as a base for vectorize documents in low dimensions. You can read more about it here: https://cs224d.stanford.edu/lectures/CS224d-Lecture2.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1**: Extract all sentences using the `punkt` tokenizer in the `nltk` package. How many sentences are in the corpus?\n",
    "\n",
    "Hint: Use the `review_to_sentences` helper function defined at the very top of this notebook to convert reviews to sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the punkt tokenizer for sentence splitting\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all sentences from all reviews\n",
    "sentences = []\n",
    "\n",
    "for review in data['review']:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of sentences\n",
    "print (len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first sentence\n",
    "print (sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2**: Train the word vectors using the `Word2Vec` function in the `gensim` package. Generate word vectors with 300 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train word vectors\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features   = 300  # word vector dimensionality                      \n",
    "min_word_count = 40   # minimum word count                        \n",
    "num_workers    = 16   # number of threads to run in parallel\n",
    "context        = 10   # context window size                                                                                    \n",
    "\n",
    "# Initialize and train the model \n",
    "print ('Training model...')\n",
    "w2v_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "print('Done !')\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient\n",
    "w2v_model.init_sims(replace=True)\n",
    " \n",
    "# Save the model for later use. you can load it later using Word2Vec.load()\n",
    "# model_name = 'w2v_imdb'\n",
    "# model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3**: Investigate how words relate to each other using the `doesnt_match` and `most_similar` functions on the trained Word2Vec model. What's the most similar word to \"man\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the word which is less similar to other words in a set\n",
    "w2v_model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the words which are similar to a focal word\n",
    "\n",
    "w2v_model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.most_similar(\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corresponding word vector of an example word\n",
    "\n",
    "w2v_model['flower']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph vectors - Average Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the word vectors show good semantical properties, using them to get the same sort of properties from sentences is not straight forward. The simplest solution is to average word vectors of a document to come up with the same dimensional paragraph vector.\n",
    "\n",
    "Hint 1: Use the `review_to_wordlist` helper function defined at the very top of this notebook. \n",
    "\n",
    "Hint 2: Use the `getAvgFeatureVecs` helper function defined at the very top of this notebook to average word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average feature vectors for review data, using the functions we defined above.\n",
    "clean_data_reviews = []\n",
    "for review in data['review']:\n",
    "    clean_data_reviews.append( review_to_wordlist( review ))\n",
    "\n",
    "w2v_features = getAvgFeatureVecs(clean_data_reviews, w2v_model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 4**: Fit a `RandomForestClassifier` to our data and test different parameter values for `n_estimators` (e.g. from 10 to 50). \n",
    "\n",
    "What nested cross validated accuracy do you get? How long does it take to test the different parameter values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define pipeline\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_clf__random_state = SEED)\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"rf_clf__n_estimators\": [int(i) for i in np.linspace(10.0, 50.0, 5)]}\n",
    "\n",
    "acc_rfW2V, std_rfW2V = nested_cv(X = w2v_features, y = target, est_pipe = rf_pipe, p_grid = p_grid, p_score = SCORE, n_cores = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 6**: Document Vectors - Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution to obtain review vectors is to obtain them directly. Paragraph Vectors are treating each document as a word itself and obtains their vectors directly. You can read more about it here: https://cs.stanford.edu/~quocle/paragraph_vector.pdf . Again, the `gensim` library implements this method in the `doc2vec` class.\n",
    "\n",
    "The first two cells below get rid of some \"housekeeping\" to prepare the data. Make sure the variable names match with your variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec needs each review to be tagged with some sort of ids\n",
    "# Here we tag each review with the 'id' field\n",
    "\n",
    "tagged_clean_data_reviews = []\n",
    "for uid, review in zip(data['id'], clean_data_reviews):\n",
    "    tagged_clean_data_reviews.append(doc2vec.TaggedDocument(words=review, tags=['%s' % uid[1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of tagging\n",
    "\n",
    "tagged_clean_data_reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1**: Train the document vectors using the `Doc2Vec` function in the `gensim` package. Generate document vectors with 300 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train paragraph vectors\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features   = 300  # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers    = 4    # Number of threads to run in parallel\n",
    "context        = 10   # Context window size                                                                                    \n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "print('Training model...')\n",
    "d2v_model = doc2vec.Doc2Vec(tagged_clean_data_reviews, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "print('Done !')\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "d2v_model.init_sims(replace=True)\n",
    " \n",
    "# Save the model for later use. You can load it later using Doc2Vec.load().\n",
    "# d2v_model_name = 'w2v_imdb'\n",
    "# d2v_model.save(d2v_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2**: Investigate how documents relate to each other using the `docvecs.similarity` function on the trained Doc2Vec model. How similar are documents d1 = '7759_3' and d2 = '5814_8'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corresponding document vector of a review given its id\n",
    "d2v_model.docvecs['5814_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of D2V features of all reviews to be used in down-stream predictions \n",
    "d2v_features = d2v_model.docvecs\n",
    "d2v_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the similarity of two reviews (based on cosine similarity) \n",
    "d2v_model.docvecs.similarity(d1='7759_3', d2='5814_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3**: Fit a `RandomForestClassifier` to our data and test different parameter values for `n_estimators` (e.g. from 10 to 50). Hint: use the `linspace` function in the `numpy` package to generate evenly spaced numbers in a given interval.\n",
    "\n",
    "What nested cross validated accuracy do you get? How long does it take to test the different parameter values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define pipeline\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_clf__random_state = SEED)\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"rf_clf__n_estimators\": [int(i) for i in np.linspace(10.0, 50.0, 5)]}\n",
    "\n",
    "acc_rfD2V, std_rfD2V = nested_cv(X = d2v_features, y = target, est_pipe = rf_pipe, p_grid = p_grid, p_score = SCORE, n_cores = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SUMMARY OF ACCURACY SCORES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width       = 50\n",
    "models      = ['Baseline', 'Random Forest', 'Random Forest + word2vec', 'Random Forest + doc2vec']\n",
    "result_acc  = [0.5, acc_rf, acc_rfW2V, acc_rfD2V]\n",
    "result_std  = [0,   std_rf, std_rfW2V, std_rfD2V]\n",
    "\n",
    "print('', '=' * width, '\\n', 'Summary of Accuracy Scores'.center(width), '\\n', '=' * width)  \n",
    "for i in range(len(models)):\n",
    "    print(models[i].center(width-18), '{0:.4f}'.format(result_acc[i]), '+/-{0:.4f}'.format(result_std[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
