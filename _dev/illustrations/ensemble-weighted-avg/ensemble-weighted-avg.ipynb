{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DSFM Illustration**: Weighted-average ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creator: [Data Science for Managers - EPFL Program](https://www.dsfm.ch)  \n",
    "Source:  [https://github.com/dsfm-org/code-bank.git](https://github.com/dsfm-org/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. \n",
    "\n",
    "Example adapted from: https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a simple, weighted-average ensemble of multiple random forests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 0**: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# scikit-learn and keras (with tensorflow)\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.metrics                    import accuracy_score\n",
    "from sklearn.linear_model               import LogisticRegression\n",
    "from sklearn.tree                       import DecisionTreeClassifier\n",
    "from sklearn.ensemble                   import RandomForestRegressor, RandomForestClassifier, GradientBoostingClassifier\n",
    "from scipy.optimize                     import differential_evolution\n",
    "from tensorflow.keras.utils             import to_categorical\n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting constants\n",
    "FIGSIZE   = (12, 8)\n",
    "\n",
    "# modeling constants\n",
    "MAXDEPTH  = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1**: Generate toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=7, n_features=2, cluster_std=2, random_state=7)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print('TrainX: {}, TestX: {}'.format(trainX.shape, testX.shape))\n",
    "\n",
    "# plot data\n",
    "df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue', 2:'green', 3:'lightgreen', 4:'orange', 5:'purple', 6:'pink'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key], figsize=FIGSIZE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2**: Fit a single random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on dataset\n",
    "def fit_model_rf(trainX, trainy):\n",
    "    \n",
    "    # convert list of targets to one-hot-encoded matrix\n",
    "    trainy_enc = to_categorical(trainy)\n",
    "    \n",
    "    # fit model\n",
    "    m = RandomForestRegressor(max_depth = MAXDEPTH)\n",
    "    m.fit(trainX, trainy_enc)\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions and evaluate \n",
    "m = fit_model_rf(trainX, trainy)\n",
    "yhat = np.argmax(m.predict(testX), axis = 1)\n",
    "\n",
    "print('Single model accuracy: {}'.format(accuracy_score(testy, yhat)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3**: Fit many models and aggregate with a simple average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit all models\n",
    "n_members = 20\n",
    "members = [fit_model_rf(trainX, trainy) for _ in range(n_members)]\n",
    "\n",
    "# make an ensemble prediction for multi-class classification\n",
    "def ensemble_predictions(members, testX, weights = None):\n",
    "        \n",
    "    # make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = np.array(yhats)\n",
    "    \n",
    "    # weighted sum across ensemble members\n",
    "    if type(weights) ==np.ndarray: \n",
    "        summed = np.tensordot(yhats, weights, axes=((0),(0)))\n",
    "    else:\n",
    "        summed = np.sum(yhats, axis=0)\n",
    "    \n",
    "    # argmax across classes\n",
    "    result = np.argmax(summed, axis=1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def evaluate_n_members(members, n_members, testX, testy):\n",
    "    \n",
    "    # select a subset of members\n",
    "    subset = members[:n_members]\n",
    "    \n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(subset, testX)\n",
    "    \n",
    "    return accuracy_score(testy, yhat)\n",
    "\n",
    "# evaluate different numbers of ensembles on hold out set\n",
    "single_scores, ensemble_scores = list(), list()\n",
    "for i in range(1, len(members)+1):\n",
    "    \n",
    "    # evaluate model with up to i members\n",
    "    ensemble_score = evaluate_n_members(members, i, testX, testy)\n",
    "    \n",
    "    # evaluate the i'th model standalone\n",
    "    yhat = np.argmax(members[i-1].predict(testX), axis=1)\n",
    "    single_score = accuracy_score(testy, yhat)\n",
    "\n",
    "    # summarize this step\n",
    "#     print('Models <= {}: \\tsingle={}, \\tensemble={}'.format(i, round(single_score, 3), ensemble_score))\n",
    "    ensemble_scores.append(ensemble_score)\n",
    "    single_scores.append(single_score)\n",
    "    \n",
    "# View results \n",
    "print(' Models'.ljust(5), '\\t', 'Single'.center(8), '  ', 'Ensemble'.center(11), '\\n', '=' * 40)\n",
    "\n",
    "for i, j in enumerate(zip(ensemble_scores, single_scores)):\n",
    "    ensemble, single = j\n",
    "    print(' 1 to {0}'.format(i+1).ljust(5), '\\t',  '{0:.4f}'.format(single).center(8), '  ', '{0:.4f}'.format(ensemble).center(11))   \n",
    "    \n",
    "# summarize average accuracy of a single final model\n",
    "print('\\nAvg. accuracy single model: {} (std. {})'.format(round(np.mean(single_scores), 3), round(np.std(single_scores), 3)))\n",
    "print('Avg. accuracy ensemble model: {} (std. {})'.format(round(np.mean(ensemble_scores), 3), round(np.std(ensemble_scores), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 4**: Fit many models and aggregate with a weighted average\n",
    "\n",
    "One alternative to simple averaging is weighted averaging, where each model is assigned a different weight. Each weight represents the \"confidence\" we have in those model's predictions. However, it's difficult a priori to evaluate confidence in individual models - high accuracy might be achieved just by being lucky. \n",
    "\n",
    "A more principled approach is to learn weights. Instead of exhaustively searching a space of possible weight combinations, we use the available information to make the next step in the search towards weights with lower error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_ensemble(members, weights, testX, testy):\n",
    "    \n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(members, testX, weights)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(testy, yhat)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "    \n",
    "    # calculate l1 vector norm\n",
    "    result = np.linalg.norm(weights, 1)\n",
    "\n",
    "    # check for a vector of all zeros\n",
    "    if result == 0.0:\n",
    "        return weights\n",
    "    \n",
    "    # return normalized vector (unit norm)\n",
    "    return weights / result\n",
    "\n",
    "# loss function for optimization process, designed to be minimized\n",
    "def loss_function(weights, members, testX, testy):\n",
    "\n",
    "    # normalize weights\n",
    "    normalized = normalize(weights)\n",
    "    \n",
    "    # calculate error rate\n",
    "    error = 1.0 - evaluate_ensemble(members, normalized, testX, testy)\n",
    "    \n",
    "    return error\n",
    "\n",
    "# fit all models\n",
    "n_members = 5\n",
    "members = [fit_model_rf(trainX, trainy) for _ in range(n_members)]\n",
    "\n",
    "# define bounds on each weight\n",
    "bound_w = [(0.0, 1.0) for _ in range(n_members)]\n",
    "\n",
    "# arguments to the loss function\n",
    "search_arg = (members, testX, testy)\n",
    "\n",
    "# global optimization of ensemble weights\n",
    "result = differential_evolution(loss_function, bound_w, search_arg, maxiter=1000, tol=1e-7, workers=-1)\n",
    "\n",
    "# get the chosen weights\n",
    "weights = normalize(result['x'])\n",
    "\n",
    "# View results \n",
    "print(' Model'.rjust(5), '   ', 'Weight'.center(8), '\\n', '=' * 20)\n",
    "for model, weight in enumerate(weights):\n",
    "    print( '{0}'.format(model).center(5), '   ',  '{0:.4f}'.format(weight).center(8))   \n",
    "\n",
    "# evaluate chosen weights\n",
    "score = evaluate_ensemble(members, weights, testX, testy)\n",
    "print('\\nOptimized Weights Score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the initializations, optimizing the weights can increase the accuracy for 1-5%. The final ensemble model assigns the most weight to _______ (find the highest-weighted model above). \n",
    "\n",
    "Note that for simplicity, we have treated the test set as though it were a validation set. This makes the illustration simpler. In practice, however, we would need to choose and tune the weights on a validation set and then compare models on a separate test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 5**: Fit different models and aggregate with weighted average \n",
    "\n",
    "Instead of re-fitting the same random forest model, we can aggregate the predictions of different types of models. To do so, we also fit a decision tree classifier. The final prediction will then be a weighted average aggregation across two decision tree and two random forest models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit decision tree model on dataset\n",
    "def fit_model_dt(trainX, trainy):\n",
    "    \n",
    "    # convert list of targets to one-hot-encoded matrix\n",
    "    trainy_enc = to_categorical(trainy)\n",
    "    \n",
    "    # fit model\n",
    "    m = DecisionTreeClassifier()\n",
    "    m.fit(trainX, trainy_enc)\n",
    "    \n",
    "    return m\n",
    "\n",
    "# fit all models: 2 decision tree models and 2 random forest models\n",
    "n_members = 4\n",
    "members = [fit_model_dt(trainX, trainy), fit_model_dt(trainX, trainy), fit_model_rf(trainX, trainy), fit_model_rf(trainX, trainy)]\n",
    "\n",
    "# define bounds on each weight\n",
    "bound_w = [(0.0, 1.0) for _ in range(n_members)]\n",
    "\n",
    "# arguments to the loss function\n",
    "search_arg = (members, testX, testy)\n",
    "\n",
    "# global optimization of ensemble weights\n",
    "result = differential_evolution(loss_function, bound_w, search_arg, maxiter=1000, tol=1e-7, workers=-1)\n",
    "\n",
    "# get the chosen weights\n",
    "weights = normalize(result['x'])\n",
    "\n",
    "# View results \n",
    "print(' Model'.rjust(5), '   ', 'Weight'.center(8), '\\n', '=' * 20)\n",
    "for model, weight in enumerate(weights):\n",
    "    print( '{0}'.format(model).center(5), '   ',  '{0:.4f}'.format(weight).center(8))   \n",
    "\n",
    "# evaluate chosen weights\n",
    "score = evaluate_ensemble(members, weights, testX, testy)\n",
    "print('\\nOptimized Weights Score: {}'.format(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the initializations, using a different type of model can increase the accuracy for another 1-3%. The final ensemble model assigns the most weight to _______ (find the highest-weighted model above). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bonus**: Further Reading\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ensemble tutorial in sklearn: https://sebastianraschka.com/Articles/2014_ensemble_classifier.html\n",
    "- Finding ensemble weights for aggregating different models: https://www.kaggle.com/hsperr/finding-ensamble-weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
