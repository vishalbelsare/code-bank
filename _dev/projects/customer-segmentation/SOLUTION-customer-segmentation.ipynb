{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DSFM Project**: Customer Segmentation - K-means Clustering (SOLUTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creator: [Data Science for Managers - EPFL Program](https://www.dsfm.ch)  \n",
    "Source:  [https://github.com/dsfm-org/code-bank.git](https://github.com/dsfm-org/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://conversionxl.com/wp-content/uploads/2016/09/segmentation-illustration.png\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "Image source: https://conversionxl.com/wp-content/uploads/2016/09/segmentation-illustration.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset source: *Abreu, N. (2011). Analise do perfil do cliente Recheio e desenvolvimento de um sistema promocional. Mestrado em Marketing, ISCTE-IUL, Lisbon* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this demo is to cluster customers of a wholesale Portugese company into different segments. Note that clustering is an unsupervised learning task where no labels are given. Description of the fields are as follows (m.u. stands for monetary unit): \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature name     | Variable Type | Description \n",
    "|------------------|---------------|--------------------------------------------------------\n",
    "| FRESH            | Continuous    | annual spending (m.u.) on fresh products   \n",
    "| MILK             | Continuous    | annual spending (m.u.) on milk products  \n",
    "| GROCERY          | Continuous    | annual spending (m.u.) on grocery products  \n",
    "| FROZEN           | Continuous    | annual spending (m.u.) on frozen products   \n",
    "| DETERGENTS_PAPER | Continuous    | annual spending (m.u.) on detergents and paper products  \n",
    "| DELICATESSEN     | Continuous    | annual spending (m.u.) on delicatessen products    \n",
    "| CHANNEL          | Categorical   | customers channel where 1 = HoReCa (Hotel/Restaurant/Cafe); 2 = Retail channel\n",
    "| REGION           | Categorical   | customers region where 1 = Lisbon; 2 = Porto; 3 = Other  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 0**: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all import statements at the top of your notebook\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from time import time\n",
    "\n",
    "# Clustering imports\n",
    "from sklearn               import preprocessing\n",
    "from sklearn               import datasets\n",
    "from sklearn.cluster       import KMeans\n",
    "from sklearn.metrics       import silhouette_samples, silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold      import TSNE\n",
    "\n",
    "# Visualization packages\n",
    "from bokeh.models         import HoverTool\n",
    "from bokeh.plotting       import output_notebook, figure, show, ColumnDataSource\n",
    "import bokeh.plotting     as bp\n",
    "import matplotlib.pyplot  as plt\n",
    "import matplotlib.cm      as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker    import NullFormatter\n",
    "from faerun               import Faerun\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for replication\n",
    "SEED = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1**: Data Preprocessing and EDA\n",
    "\n",
    "First, we would like to understand the main characteristics of the dataset. We might need to transform and clean some features before we can specify a statistical model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Load the `customer_data.csv` file, check the data shape, and investigate the top 10 rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('customer_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of observations and features\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top rows\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Investigate the available features and their data types. Do they look OK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature types\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3:** Compute descriptive statistics across all features. For the `Channel` and `Region` features, count how often the feature values occur. What's the most common Channel and Region?\n",
    "\n",
    "Hint: Use the `value_counts()` function in Pandas to count feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key statistics\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Channel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Region'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common Channel is _HoReCa (Hotel/Restaurant/Cafe)_. The most common Region is _Other_, the regions outside of Lisbon and Porto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2**: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several preprocessing steps before doing the clustering:\n",
    "\n",
    "* null values\n",
    "* categorical variables\n",
    "* standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Check for redundant and missing values. How many duplicate rows and missing values are there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop possible duplicate rows\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop possible missing values\n",
    "data.dropna(inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicates and missing values in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** One-hot encode the categorical featues `Channel` and `Region`. What's the new shape of the dataset? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_transform = [ 'Channel', 'Region']\n",
    "data_with_dummies = pd.get_dummies(data = data, columns = cols_to_transform )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_dummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3:** Standardize data such that each feature has a mean of zero and a standard deviation of one. Validate the standardization has worked by computing descriptive statistics on the standardized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aside: check the underlying data \n",
    "X_scaled = preprocessing.scale(data_with_dummies.values)\n",
    "data_with_dummies_ready = pd.DataFrame(X_scaled, columns = data_with_dummies.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_dummies_ready.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_dummies_ready.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3**: k-means Clustering\n",
    "\n",
    "We know cluster the customers into three clusters, using k-means clustering. Why would that be helpful? Think about a marketing campaign that targets customers with shared characteristics, e.g. customers in a particular region of Portugal ordering similar amounts of different foods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Fit a k-means clustering model with three clusters.\n",
    "\n",
    "Hint: Don't forget using the `SEED` value to ensure consistent results across multiple runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering data into 3 clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=SEED).fit(data_with_dummies_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** What are the cluster centers/centroids and what do they represent? What clusters do the customers belong to?\n",
    "\n",
    "Hint: Use the `cluster_centers_` and `labels_` attributes of the model trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each observation belongs to the cluster with the nearest mean, i.e. the centroids are a proxy for the entire cluster they belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_clusters = data.copy()\n",
    "data_with_clusters['Cluster_k=3'] = kmeans.labels_\n",
    "\n",
    "data_with_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should pay attention to the following:\n",
    "\n",
    "* __Initial centroids__: k-mean clustering in sklearn is initializing centoids with different initial random seeds and will report the best result in terms of optimization score.  \n",
    "  \n",
    "* __Number of clusters__: to select the best number of clusters, one strategy is to maximize the \"Silhouette\" coefficient.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 4**: Selecting the optimal number of clusters, _k_\n",
    "\n",
    "There are different approaches to selecting the number of clusters, _k_. One approach uses domain knowledge of the problem at hand. Another approach evaluates how well different values for _k_ segment the space. In this part, we will use the Silhouette coefficient to find the (computationally) optimal number of clusters.\n",
    "\n",
    "The Silhouette coefficient is calculated using the mean distance of a data point to all other points in the same cluster (a) and the mean distance to all data points in the nearest-cluster (b). The Silhouette Coefficient for a sample is (b - a) / max(a, b). The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar. We can consider the mean Silhuette coefficient of all samples as a rough metric for clustering quality.  \n",
    "\n",
    "We change number of clusters from 2 to 20 and chose the one with best Silouette score. For each number of clusters, we visualize the silouette score of each data point and project our data using PCA into 2 dimensions to visualize detected clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_with_dummies_ready\n",
    "range_n_clusters = range(2,21)\n",
    "\n",
    "# For different number of clusters do the followings\n",
    "for n_clusters in range_n_clusters:\n",
    "    \n",
    "    # Define a subplot with one row and two columns\n",
    "    fig, (ax1,ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "    \n",
    "    # Obtain cluster labels for n_clusters\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=SEED)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "    \n",
    "    # Calculate average silhouette score \n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "\n",
    "    # Calculate silhouette score for each data point\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "    \n",
    "    # Visualize silhouette score of each data point as well as average silhouette score\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "       \n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10  \n",
    "\n",
    "    ax1.set_title('The silhouette plot for the various clusters.')\n",
    "    ax1.set_xlabel('The silhouette coefficient values')\n",
    "    ax1.set_ylabel('Cluster label')\n",
    "    ax1.axvline(x=silhouette_avg, color='red', linestyle='--')\n",
    "    ax1.set_yticks([])  \n",
    "    ax1.set_xticks([-1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    \n",
    "    # Project data into two dimensions using PCA, with differnt colors for each cluster\n",
    "    pca = PCA(n_components=2, svd_solver='full')\n",
    "    XX = pca.fit(X).transform(X)\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(XX[:, 0], XX[:, 1], marker='.', s=30, lw=0, alpha=0.7, c=colors, edgecolor='k')\n",
    "    centers = pca.transform(clusterer.cluster_centers_)\n",
    "   \n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title('The visualization of the clustered data.')\n",
    "    ax2.set_xlabel('Feature space for the 1st feature')\n",
    "    ax2.set_ylabel('Feature space for the 2nd feature')\n",
    "    \n",
    "    plt.suptitle(('\\n Silhouette analysis for KMeans clustering on sample data '\n",
    "                  'with n_clusters = %d (Avg score: %f)' % (n_clusters, silhouette_avg)), fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the silhouette score with different number of clusters and different random seeds\n",
    "range_seeds = range(0,10)\n",
    "range_n_clusters = range(2,21)\n",
    "list_all = []\n",
    "list_cluster = []\n",
    "for seed in range_seeds:\n",
    "    for n_clusters in range_n_clusters:\n",
    "        \n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=seed)\n",
    "        cluster_labels = clusterer.fit_predict(data_with_dummies_ready)\n",
    "        silhouette_avg = silhouette_score(data_with_dummies_ready, cluster_labels)\n",
    "        \n",
    "        list_cluster.append(silhouette_avg)\n",
    "        list_cluster_tmp = list(list_cluster)\n",
    "        \n",
    "    list_all.append(list_cluster_tmp)\n",
    "    list_cluster.clear()\n",
    "\n",
    "# Average across all runs\n",
    "gather_list_all = []\n",
    "for i in range_seeds:\n",
    "    gather_list_all.append(list_all[i])\n",
    "\n",
    "results_avg = [float(sum(col))/len(col) for col in zip(*gather_list_all)]\n",
    "\n",
    "print('Average Silhouette scores for different number of clusters:\\n{}'.format(results_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Visualize the average Silhouette scores for different number of clusters and different random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.grid(True)\n",
    "plt.xticks(np.arange(min(range_n_clusters),max(range_n_clusters)+1,1.0))\n",
    "plt.plot(range_n_clusters, results_avg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Re-fit with the optimal number of clusters identified above. How does the result differ from using _k=3_ earlier in the project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therefore we choose number of clusters equal to 9\n",
    "\n",
    "clusterer = KMeans(n_clusters=9, random_state=SEED)\n",
    "cluster_labels = clusterer.fit_predict(data_with_dummies_ready)\n",
    "\n",
    "data_with_clusters['Cluster_k=9'] = cluster_labels\n",
    "data_with_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 5**: 2D interactive visualization using T-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each observation is now associated with a cluster – that's great! However, we have no intuitive understanding of how similar the clusters are to each other. To find out, we will project our multi-dimensional data down into two dimensions and visualize it together with our clustering results.\n",
    "\n",
    "PCA is able to grasp only **linear** projection of high-dimensional space into lower dimensions. In many cases, what is more precise in to learn the non-linear manifolds in which the data shows the highest variance along with. There are different techniques to achieve this goal, all with their own pros and cons. t-Distributed Stochastic Neighbor Embedding (T-SNE) is an state of the art technique to achieve this goal. Following example shows the main intuition behind this technique. For more information, you can have a look [here](https://lvdmaaten.github.io/tsne/). To understand the difference better, look at the following toy example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired from work by Jake Vanderplas -- <vanderplas@astro.washington.edu>\n",
    "\n",
    "n_points = 1000\n",
    "X_sample, color = datasets.make_s_curve(n_points, random_state=0)\n",
    "n_components = 2\n",
    "\n",
    "# Original space\n",
    "fig = plt.figure(figsize=(32, 12))\n",
    "ax = fig.add_subplot(251, projection='3d')\n",
    "ax.scatter(X_sample[:, 0], X_sample[:, 1], X_sample[:, 2], c=color, cmap=plt.cm.Spectral)\n",
    "ax.view_init(4, -72)\n",
    "\n",
    "# PCA space\n",
    "t0 = time()\n",
    "pca = PCA(n_components=n_components, random_state=0)\n",
    "Y_sample = pca.fit_transform(X_sample)\n",
    "t1 = time()\n",
    "print(\"PCA: %.2g sec\" % (t1 - t0))\n",
    "ax = fig.add_subplot(252)\n",
    "plt.scatter(Y_sample[:, 0], Y_sample[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"PCA (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "\n",
    "# TSNE space\n",
    "t0 = time()\n",
    "tsne = TSNE(n_components=n_components, init='pca', random_state=0)\n",
    "Y_sample = tsne.fit_transform(X_sample)\n",
    "t1 = time()\n",
    "print(\"t-SNE: %.2g sec\" % (t1 - t0))\n",
    "ax = fig.add_subplot(253)\n",
    "plt.scatter(Y_sample[:, 0], Y_sample[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"t-SNE (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In brief, T-SNE works by minimizing the divergence between two probability distribution between pairs, proportional to their distances, one in the original space and the other in the reduced dimension space. Unlike PCA, T-SNE is sensitive to its configuration parameters. Among them, following parameters worth mentioning here:\n",
    "\n",
    "* **perplexity**: A parameter representing the number of nearest neighbors considered as 'close' in the original space. \n",
    "* **early_exaggeration**: Controls how tight natural clusters in the original space are in the embedded space and how much space will be between them. \n",
    "* **learning_rate**: The step size in solving optimization problem using gradient descent. it can make your optimization to converge fast or to escape from local minima. Note that different runs of this algorithm with different initial points can lead to different results due to non-convexity of the optimization surface. One might run the algorithm several times with different initial values and choose the one with minimum divergence score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Transform our customer data into lower dimensions using the distances from the fitted k-means model.\n",
    "\n",
    "Hint: Use `clusterer.transform(<data>)` to get the distances of each data point to the 9 centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kmeans_distances = clusterer.transform(data_with_dummies_ready)\n",
    "X_kmeans_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Apply T-SNE to the distance matrix and transform the data to 2D space. What's the shape of the transformed data and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne2 = TSNE(n_components=2, verbose=1, random_state=1, method='exact')\n",
    "X_kmeans_distances_tsne2 = tsne2.fit_transform(X_kmeans_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kmeans_distances_tsne2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Visualize the 2D data. Color the points according to what cluster they belong to. How are the clusters related to each other? Does that make sense?\n",
    "\n",
    "This question does not require writing any code. Focus on interpreting the visualization and the steps we went through to get there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a colormap of random colors\n",
    "colormap = np.array([\"#6d8dca\", \"#69de53\", \"#723bca\", \"#c3e14c\", \"#c84dc9\", \"#68af4e\", \"#6e6cd5\",\n",
    "\"#e3be38\", \"#4e2d7c\", \"#5fdfa8\", \"#d34690\", \"#3f6d31\", \"#d44427\", \"#7fcdd8\", \"#cb4053\", \"#5e9981\",\n",
    "\"#803a62\", \"#9b9e39\", \"#c88cca\", \"#e1c37b\", \"#34223b\", \"#bdd8a3\", \"#6e3326\", \"#cfbdce\", \"#d07d3c\",\n",
    "\"#52697d\", \"#7d6d33\", \"#d27c88\", \"#36422b\", \"#b68f79\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We transform the 2 dimensioanl tsne data into a data frame with associated cluster number and color\n",
    "\n",
    "def calculate_color(cluster):\n",
    "    color = colormap[cluster]\n",
    "    return color\n",
    "\n",
    "dataset_kmeans_vis = pd.DataFrame(X_kmeans_distances_tsne2, columns=['x', 'y'])\n",
    "dataset_kmeans_vis['cluster'] = cluster_labels\n",
    "dataset_kmeans_vis['color'] = dataset_kmeans_vis.cluster.apply(calculate_color)\n",
    "dataset_kmeans_vis['channel'] = data['Channel']\n",
    "dataset_kmeans_vis['region'] = data['Region']\n",
    "\n",
    "dataset_kmeans_vis.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize using bokeh library which is used for interactive visualization\n",
    "# BokehJS might return an error the first time running this \n",
    "\n",
    "source = ColumnDataSource(data=dataset_kmeans_vis)\n",
    "\n",
    "plot_kmeans = bp.figure(plot_width=800, plot_height=600, title='KMeans clustering of wholesale customers',\n",
    "    tools='pan,wheel_zoom,box_zoom,reset,hover',\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "output_notebook()\n",
    "plot_kmeans.scatter(x='x', y='y', color='color', size=5, source=source)\n",
    "hover = plot_kmeans.select(dict(type=HoverTool))\n",
    "hover.tooltips=[(\"index\", \"$index\"),('cluster','@cluster'), ('channel', '@channel'), ('region', '@region')]\n",
    "show(plot_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 6**: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What could be the semantics behind each cluster? \n",
    "* How might the set of features in our dataset affect the cluster semantics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
