{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DSFM Project**: Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creator: [Data Science for Managers - EPFL Program](https://www.dsfm.ch)  \n",
    "Source:  [https://github.com/dsfm-org/code-bank.git](https://github.com/dsfm-org/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project explores how __Bias__ and __Variance__ change as a linear regression is expanded with additional polynomial terms. \n",
    "\n",
    "First we define a __Data Generating Process__ (DGP) as a sigmoid function; then draw repeated samples from the DGP to allow for random variation; and finally we show the degree of __Bias__ and __Variance__ of the model at a given value of X. At the end of the project we can copare how the model performs at different levels of the polynomial expansion.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 0**: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import math \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0         = 4       # value of the arbitrary point where bias will be measured\n",
    "NOISE      = 0.2     # offset for draw from uniform distribution \n",
    "FONTSIZE   = 18\n",
    "OFFSET     = 0.1    # plot offset on x and y axes\n",
    "DRAWS      = 100    # number of draws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "We will help you get started by defining some helper functions\" for you to use, that just work. There is nothing for you to do in this section, and you don't need to worry about the internal aspects of each function, other than to just understand at a high level what each function returns or accomplishes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Generating Function__:  \n",
    "We define the true data generating function (DGP) for this example to be a logistic sigmoid function. The sigmoid function *without* any noise is the true function that we aim to recover. A sigmoid function is defined as:  $sigmoid(x) = \\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    y = 1 / (1 + math.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Custom Plotting Function__:  \n",
    "The following function will plot the data and the given model predictions. This custom functioin simply \"wraps\" a basic matplotlib plotting function, but it does so in a standardized way that fits the needs of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myPlot(X, y, y_DGP, y_pred, draw, symbol = 'o'):\n",
    "    \n",
    "    font = {'size'   : FONTSIZE}\n",
    "    matplotlib.rc('font', **font)\n",
    "    \n",
    "    plt.figure(figsize=(16, 12))\n",
    "    plt.plot(X, y, symbol, markersize=12, linewidth=3, label='Sampled data')\n",
    "    plt.plot(X, y_DGP, markersize=12, linewidth=3, label='Ground truth')\n",
    "    plt.ylim(0 - OFFSET, 1 + OFFSET)\n",
    "    plt.xlim(min(X) - OFFSET, max(X) + OFFSET)\n",
    "    plt.hlines(0, xmin = min(X), xmax = max(X), colors='black', linewidth=3)\n",
    "    plt.hlines(1, xmin = min(X), xmax = max(X), colors='black', linewidth=3)\n",
    "    \n",
    "    if type(y_pred) == float:\n",
    "        plt.hlines(y_pred, xmin = min(X), xmax = max(X), colors='red', label='Latest estimate', linewidth=3)\n",
    "    if type(y_pred) == np.ndarray:\n",
    "        plt.plot(X, y_pred, '-', markersize = 12, linewidth=3, color = 'red', label='Latest estimate')\n",
    "        \n",
    "    plt.ylabel('y')\n",
    "    plt.xlabel('X')\n",
    "    plt.title('Draw: {}'.format(draw))\n",
    "    plt.legend()\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test above two functions to see them in action__..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(range(-50, 50))\n",
    "X = [i/10 for i in X if i%2 == 0]\n",
    "y_DGP = [sigmoid(i) for i in X]\n",
    "y = [i + np.random.uniform(-NOISE, +NOISE) for i in y_DGP]\n",
    "myPlot(X, y, y_DGP, None, 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1**: Model 1 - The Mean\n",
    "\n",
    "The simplest model of all predicts the mean of the outcome variable. The model has a high bias, but a low variance. \n",
    "\n",
    "We will now resample from DGP and measure bias/variance at an arbitrary point to demonstrate this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "estimates = []\n",
    "draw = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q 1: Draw data, estimate model, plot the result__:  \n",
    "Repeatedly execute the following block of code to draw a new sample, estimate the model, and inspect the variance and the bias at an arbitrary point X0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sample \n",
    "draw += 1\n",
    "y_DGP = [sigmoid(i) for i in X]\n",
    "y = [i + np.random.uniform(-NOISE, +NOISE) for i in y_DGP]\n",
    "\n",
    "# CODE HERE\n",
    "# Estimate: compute the mean of y\n",
    "y_pred = None\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert y_pred is not None, 'HINT: you need to complete the code to proceed.'\n",
    "\n",
    "estimates.append(y_pred)\n",
    "\n",
    "# Plot\n",
    "p = myPlot(X, y, y_DGP, y_pred, draw)   # Plot data\n",
    "for est in estimates[:-1]:      # Plot estimates \n",
    "    p.hlines(est, xmin = min(X), xmax = max(X), colors='gray', linestyle='dashed', linewidth=2)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q 2: Summarize Bias and Variance at X0__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw from the DGP and compute the difference to the prediction\n",
    "biases = []\n",
    "for draw in range(DRAWS):\n",
    "    biases.append(y_pred - sigmoid(X0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Bias at point X0: {}'.format(round(abs(np.mean(biases)), 4)))\n",
    "print('Variance at point X0: {}'.format(round(np.var(biases), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2**: Model 2 - A Line\n",
    "\n",
    "The next most complicated model predicts a linear relationship between X and the outcome variable Y. The model still has moderate bias, but somewhat lower variance. \n",
    "\n",
    "We will now resample from DGP and measure bias/variance at an arbitrary point to demonstrate this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "estimates = []\n",
    "draw = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q 1: Draw data, estimate model, plot the result__:  \n",
    "Repeatedly execute the following block of code to draw a new sample, estimate the model, and inspect the variance and the bias at an arbitrary point X0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sample \n",
    "draw += 1\n",
    "y_DGP = [sigmoid(i) for i in X]\n",
    "y = [i + np.random.uniform(-NOISE, +NOISE) for i in y_DGP]\n",
    "\n",
    "# CODE HERE \n",
    "# Estimate: fit a LinearRegression() model\n",
    "model = None\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert model is not None, 'HINT: you need to complete the code to proceed.'\n",
    "\n",
    "y_pred = model.predict(np.array(X).reshape(-1, 1))\n",
    "estimates.append(y_pred)\n",
    "\n",
    "# Plot\n",
    "p = myPlot(X, y, y_DGP, y_pred, draw)      # Plot data\n",
    "for est in estimates[:-1]:                 # Plot estimates \n",
    "    plt.plot(X, est, '-', markersize = 12, linewidth=2, color = 'gray', linestyle='dashed')\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q 2: Summarize Bias and Variance at X0__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw from the DGP and compute the difference to the prediction\n",
    "biases = []\n",
    "for draw in range(DRAWS):\n",
    "    y = [i + np.random.uniform(-NOISE, +NOISE) for i in y_DGP]\n",
    "    model = LinearRegression().fit(np.array(X).reshape(-1, 1), y)\n",
    "    y_pred = model.predict(np.array(X).reshape(-1, 1))\n",
    "    biases.append(y_pred[X.index(X0)] - sigmoid(X0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Bias at point X0: {}'.format(round(abs(np.mean(biases)), 4)))\n",
    "print('Variance at point X0: {}'.format(round(np.var(biases), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3**: Model 3 - A Higher Order Polynomial Regression\n",
    "\n",
    "We can expand the predictor space for X with higher order polynomials (X*X, X*X*X, etc...) to allow for non-linearities in the relationshio between X and Y. These modeled will bring down bias, but increase variance as each model chases the randomness of sampling variation. \n",
    "\n",
    "We will now resample from DGP and measure bias/variance at an arbitrary point to demonstrate this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "estimates = []\n",
    "draw = 0\n",
    "\n",
    "degree = 4   # Set the polynomial basis expansion (try 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q 1: Draw data, estimate model, plot the result__:  \n",
    "Repeatedly execute the following block of code to draw a new sample, estimate the model, and inspect the variance and the bias at an arbitrary point X0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sample \n",
    "draw += 1\n",
    "y_DGP = [sigmoid(i) for i in X]\n",
    "y = [i + np.random.uniform(-NOISE, +NOISE) for i in y_DGP]\n",
    "\n",
    "# Generate a polynomial \"basis expansion\" -- new feature variables to add into the regression \n",
    "polynomial = PolynomialFeatures(degree=degree, include_bias=True)\n",
    "X_ = polynomial.fit_transform(np.array(X).reshape(-1, 1))\n",
    "\n",
    "# Estimate \n",
    "model = LinearRegression().fit(X_, y)\n",
    "\n",
    "# CODE HERE\n",
    "# Use the fitted model to predict values for X_\n",
    "y_pred = None\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert y_pred is not None, 'HINT: you need to complete the code to proceed.'\n",
    "\n",
    "# Smooth estimation line so we can see the prediction better\n",
    "X_smooth = list(range(-500, 491, 1))\n",
    "X_smooth = [i/100 for i in X_smooth]\n",
    "y_polynomial = [sigmoid(i) for i in X_smooth]\n",
    "X_polynomial = polynomial.transform(np.array(X_smooth).reshape(-1, 1))  # fit on only 10 datapoints\n",
    "y_polynomial_pred = model.predict(X_polynomial)\n",
    "estimates.append(y_polynomial_pred)\n",
    "\n",
    "# Plot\n",
    "p = myPlot(X, y, y_DGP, y_pred, draw)    # Plot data\n",
    "for est in estimates[:-1]:       # Plot estimates \n",
    "    p.plot(X_smooth, est, '-', markersize = 12, linewidth=2, color = 'gray', linestyle='dashed')\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q 2: Summarize Bias and Variance at X0__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw from the DGP and compute the difference to the prediction\n",
    "biases = []\n",
    "for draw in range(DRAWS):\n",
    "    y = [i + np.random.uniform(-NOISE, +NOISE) for i in y_DGP]\n",
    "    model = LinearRegression().fit(X_, y)\n",
    "    y_pred = model.predict(np.array(X_))\n",
    "    biases.append(y_pred[X.index(X0)] - sigmoid(X0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Bias at point X0: {}'.format(round(abs(np.mean(biases)), 4)))\n",
    "print('Variance at point X0: {}'.format(round(np.var(biases), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 4, Bonus**: Bias and variance across the domain of X\n",
    "\n",
    "We demonstrated the average bias and variance at one point, X0, which was arbitrarily set to a point defined by the constant X0. To get a complete and reliable assessment of bias and variance for the model _overall_, however, one would have to average bias and variance across the domain of possible levels of X. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you copy and adapt the code above to loop the tested point X0 across the domain of X? You might want to step across the range in steps of 0.1; you will need to average across the averages you are already doing; and for greater reliability, you should complete 1000 draws at each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
