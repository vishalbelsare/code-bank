{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DSFM Demo**: Exploratory Data Analysis - Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creator: [Data Science for Managers - EPFL Program](https://www.dsfm.ch)  \n",
    "Source:  [https://github.com/dsfm-org/code-bank.git](https://github.com/dsfm-org/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will explore the likelihood of surviving the sinking of the RMS Titanic. The Titanic hit a iceberg in 1912 and quickly sank, killing 1,502 out of the 2,224 passengers and crew on board. One most important reason so many people died was that there were not enough lifeboats to serve everyone. Accordingly, it has also been frequently noted that the most **_likely_** people to survive the disaster were women, children, and members of the upper-class. Let's see if that is true.\n",
    "\n",
    "The Titanic case is a classic problem in data science, and it is still an ongoing [Kaggle competition](https://www.kaggle.com/c/titanic). There are many other examples of the Titanic dataset in introductory statistics and Data Science courses, so we also encourage you to look around and see how others have approached the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/95/Titanic_sinking%2C_painting_by_Willy_St%C3%B6wer.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "Image source: https://upload.wikimedia.org/wikipedia/commons/9/95/Titanic_sinking%2C_painting_by_Willy_St%C3%B6wer.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will conduct our EDA and visualization analysis in three parts:\n",
    "\n",
    "  1. Base-rates \n",
    "  2. Interactive plots\n",
    "  3. Swarm and violin plots\n",
    "  \n",
    "To prepare, let's first review the data tools, visualization tools, and actual data for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three basic options for loading and working with data in Python:\n",
    "\n",
    "  * Pure `Python 3.x`\n",
    "  \n",
    "    In this approach, you load data directly into \"pure\" Python data objects, such as lists, sets, and dictionaries (or nested hiearchies of such objects, such as lists-within-lists, lists-within-dicts, dicts-within-dicts, and so on). Although operation on \"pure Python\" objects can be slow, it also is extremely flexible.\n",
    "    \n",
    "  * `NumPy`\n",
    "  \n",
    "    The basic data structures for holding arrays, vectors, and matrices of data are provided by a core package called `NumPy`. NumPy also has a set of linear algebra and numerical fuinctions, but in general such functions are now provided by another package called `scipy` (for scientific computing) and numerical computation is usually done there. `NumPy` has been optimized to run with primitive routines written in `C` (or even `fortran`) and so is orders-of-magnitude faster-running than doing calculations with pure Python. Nevertheless, the data access, subscripting, and slicing of elements for `NumPy` still conforms to the same syntax as pure Python.\n",
    "    \n",
    "  * `pandas`\n",
    "  \n",
    "    Most applied Data Science projects (that fit into memory/RAM), now use an \"Excel-like\" package called `pandas`. Pandas stores data in objects called **dataframes**. Dataframes will become the central type of data object for your work in Data Science (much as Excel Spreadsheets often were for Business Analysts). Dataframes provide many different properties and methods that help you to work with your data more effectively. We will use `pandas` for most of the examples and problems in the class.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to visualize data and results in Python. Sometimes that is a good thing - and sometimes it is a bad thing, for there are many ways to do it. Eventually you will want to learn multiple methods, as data scientists often use many different libraries. The following are the most common libraries, with links to their documentation:\n",
    "\n",
    "  * `matplotlib` and `pyplot` https://matplotlib.org/  \n",
    "  \n",
    "    Matplotlib is the core package for graphing in Python, and many other packages build on top of it (i.e., pyplot, pandas, and seaborn). The `matplotlib` object model can be somewhat confusing, which often means writing many lines of code and hours of debugging. To help, matplotlib also comes with an interface library called `pyplot` ([documentation here](https://matplotlib.org/api/pyplot_api.html)) that mimics the MatLab approach to graphing (helpful for many engineers). In general, however, `pyplot` has now been supplanted by the other options above. Although it is faster to get started with plotting by using one of the other options above, eventually you will find that you often need to return to matplotlib in order to \"tweak\" a layout or to work with more complicated graphs.  \n",
    "\n",
    "  * `pandas` https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html\n",
    "  \n",
    "    If you are in a hurry, it also is possible to generate many simple plots directly from the `pandas` library and `dataframe` object. This is a good option when you are moving fast and all you need to so is see a simple histogram or trend line and you already have your data in a pandas dataframe.  \n",
    "  \n",
    "  * `seaborn` https://seaborn.pydata.org/\n",
    "  \n",
    "    Seaborn is a simplified and better looking interface that sits on top of the standard `matplotlib` library. Seaborn is often used because it looks great, but also gives you all the ability to go into `matplotlib` to customize graphs for a particular need.\n",
    "  \n",
    "  * `plotly` and `plotly_express` https://www.plotly.express/   \n",
    "  \n",
    "    The commercial package `plotly` is a comprehensive toolkit for building interactive, D3 and WebGL charts. Interactive graphs are particularly good for online (web-based) dashboards. As you can see in the plot of Python visualization options below, `plotly` is quickly rising in popularity. To use plotly, however, you need to sign up for a [plotly account](https://plot.ly/python/) and you need an active internet connection. We won't need all of the features in plotly, however, and so we will develop examples using just `plotly_express`. Plotly express is a free, local-to-your-machine, and easier-to-use, API to the plotly service. [See here for documentation](https://www.plotly.express/plotly_express/) for the plotly express API.  \n",
    "    \n",
    "  * `bokeh` https://docs.bokeh.org/en/latest/\n",
    "  \n",
    "    Bokeh is a Python library for creating interactive visualizations. It helps you build beautiful graphics, ranging from simple plots to complex dashboards with streaming datasets. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://rougier.github.io/python-visualization-landscape/landscape-colors.png\" width=\"1000\" height=\"600\" align=\"center\"/>\n",
    "\n",
    "\n",
    "Image source: Nicolas P. Rougier (https://rougier.github.io/python-visualization-landscape/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Titanic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is taken from the [Kaggle Titanic Competition](https://www.kaggle.com/c/titanic). It is split between a \"training\" dataset (where you know the actual outcome) and a \"testing\" dataset (where you do not know the outcome). If we were actually competing in the Kaggle competition, then we would be trying to predict the unknown testing cases and submitting our predictions to Kaggle to see if we could win. But in this case, the objective is simply to get you started with Python and to familiarize you with the basic data structures and graphing libraries of the Data Science stack. We therefore will ignore the testing dataset and work only with the training data.\n",
    "\n",
    "All of the data that you will need for this demo is in the `titanic.csv` file, located within the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't know very much about the 891 passengers in the training dataset. The following features are available.\n",
    "\n",
    "  Feature name | Description  |\n",
    "  -------- | -------------- |\n",
    "  Survived | Target variable, i.e. survival, where 0 = No, 1 = Yes |\n",
    "  PassengerId | Id of the passenger |\n",
    "  Pclass   | Ticket class, wher 1 = 1st, 2 = 2nd, 3 = 3rd |\n",
    "  Name     | Passenger name |\n",
    "  Sex      | Sex |\n",
    "  Age      | Age in years |\n",
    "  SibSp    | Num of siblings or spouses aboard the Titanic |\n",
    "  Parch    | Num of parents or children aboard the Titanic |\n",
    "  Ticket   | Ticket number, i.e. record ID |\n",
    "  Fare     | Passenger fare |\n",
    "  Cabin    | Cabin number |\n",
    "  Embarked | Port of Embarkation, where C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Special Notes**\n",
    "\n",
    "  * **Pclass**: A proxy for socio-economic status (SES): 1st = Upper class; 2nd = Middle class; 3rd = Lower class.  \n",
    "  * **Age**: Age is fractional if less than 1.  If the age is estimated, is it in the form of xx.5  \n",
    "  * **SibSp**: The dataset defines family relations as: Sibling = brother, sister, stepbrother, stepsister; Spouse = husband, wife (mistresses and fiancés were ignored)  \n",
    "  * **Parch**: The dataset defines family relations as: Parent = mother, father; Child = daughter, son, stepdaughter, stepson; Some children travelled only with a nanny, therefore parch=0 for them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 0**: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all import statements at the top of your notebook -- import some basic and important ones here\n",
    "\n",
    "# Standard imports\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "\n",
    "# Visualization packages \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn           as sns\n",
    "import pandas_bokeh\n",
    "pandas_bokeh.output_notebook()\n",
    "\n",
    "# Special code to ignore un-important warnings \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1**: Analyze Base Rates and EDA\n",
    "\n",
    "The most basic \"model\" to run for any problem is the \"average\" (or \"base rate\") outcome. Before we work on more complicated models, it is a good idea to understand how a very simple heuristic will perform -- and then you can determine how much a more complicated model really improves the predictions. Your first model may be too simple and therefore highly \"biased\" (i.e., systematically low or systematically high for different groupings or levels within the data); but your simple model should also not vary much if/when we pull a new sample from the same underlying population/data generating function.  \n",
    "\n",
    "We will therefore conduct some **_exploratory data analysis_** (\"**EDA**\") and **_visualization_** in this demo to understand the distribution of the outcome for this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open dataset with Pandas\n",
    "\n",
    "data = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Count rows and coumns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1a**: EDA - the automated approach\n",
    "\n",
    "Instead of performing all of the steps above manually, you can also run a \"profile\" on the dataset first, and then drill down into specific cases of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the automated pandas profiling utility to examine the dataset\n",
    "data.profile_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1b**: EDA - the manual approach\n",
    "\n",
    "It is generally a good idea to go beyond the automated approach to EDA. Here are some useful steps for understanding and plotting the data in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the \"head\" of the dataset\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the \"tail\" of the dataset\n",
    "\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics (mean, min, max, std, etc.) for all variables and transpose the output\n",
    "\n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing data per feature\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing data  (Hint: use a seaborn heatmap to see distribution of isnull() for the dataframe\n",
    "# Sort values by Passenger Class in ascending order: What can we see? \n",
    "\n",
    "sns.heatmap(data.sort_values(by='Pclass', ascending=True).isnull(), cbar=False, cmap=\"YlGnBu_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Survival (the basic outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of Survivals and Deaths\n",
    "\n",
    "survivals = sum(data['Survived'])\n",
    "deaths = len(data[data['Survived'] == False])\n",
    "assert survivals + deaths == len(data)     # not necessary, but this should be true\n",
    "assert survivals + deaths == data.shape[0] # not necessary, but this also should be true\n",
    "print('Survivals: ', survivals)\n",
    "print('Deaths: ', deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The base-rate likelihood of survival: ', survivals/(survivals+deaths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the target feature \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of \"Survived\" using Pandas \n",
    "# The fastest way - this just uses the pandas dataframe  \n",
    "\n",
    "data['Survived'].hist()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Base-Rate Outcomes, by Condition\n",
    "\n",
    "Again, let's start by calculating the frequency (or \"base rate\") of the outcome variable for different conditions of interest. You can do this most easily by using the `.crosstab()` function from the `pandas` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pandas crosstab() function to count outcomes by condition\n",
    "\n",
    "pd.crosstab(data['Pclass'], data['Survived'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do a \"three-way\" crosstab for Class, Sex, Survival\n",
    "\n",
    "pd.crosstab([data['Sex'], data['Survived']], data['Pclass'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pandas dataframe to plot a histogram of Age \n",
    "\n",
    "data['Age'].hist(bins = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seaborn to plot the kernel density (a kdeplot) for Age\n",
    "\n",
    "facet = sns.FacetGrid(data, aspect = 4)\n",
    "facet.map(sns.kdeplot,'Age', shade = True)\n",
    "facet.set(xlim = (0, data['Age'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to plot a histogram of Survived, separated by Class\n",
    "# Hint: figsize is defined in inches\n",
    "\n",
    "lived = data[data['Survived'] == 1]['Pclass'].value_counts()\n",
    "died  = data[data['Survived'] == 0]['Pclass'].value_counts()\n",
    "df = pd.DataFrame([lived, died])\n",
    "df.index = ['Lived', 'Died']\n",
    "df.plot(kind = 'bar', stacked=True, figsize=(12, 5), title='Survival by Social Economic Class (1st, 2nd, 3rd)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to plot a histogram of Survived, separated by Sex\n",
    "\n",
    "lived = data[data['Survived'] == 1]['Sex'].value_counts()\n",
    "died  = data[data['Survived'] == 0]['Sex'].value_counts()\n",
    "df = pd.DataFrame([lived, died])\n",
    "df.index = ['Lived', 'Died']\n",
    "df.plot(kind = 'bar', stacked = True, figsize = (12, 5), title = 'Survival by Gender')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use some matplotlib customization to make your previous plot look cool \n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize = (16, 7))\n",
    "data['Survived'][data['Sex'] == 'male'].value_counts().plot.pie(explode=[0,0.2], autopct='%1.1f%%', ax = ax[0], shadow = True)\n",
    "data['Survived'][data['Sex'] == 'female'].value_counts().plot.pie(explode=[0,0.2], autopct='%1.1f%%', ax = ax[1], shadow = True, colors = ['C1', 'C0'])\n",
    "ax[0].set_title('Survived? (male)')\n",
    "ax[1].set_title('Survived? (female)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining multiple subplots into a composite figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED: combine histograms for many variables related to survival into one composite figure\n",
    "\n",
    "R = 2\n",
    "C = 3\n",
    "fields = ['Survived', 'Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked']\n",
    "\n",
    "fig, axs = plt.subplots(R, C, figsize = (12, 8))\n",
    "\n",
    "for row in range(0, R):\n",
    "    for col in range(0, C):  \n",
    "        i = row * C + col       \n",
    "        ax = axs[row][col]\n",
    "        sns.countplot(data[fields[i]], hue = data[\"Survived\"], ax = ax)\n",
    "        ax.set_title(fields[i], fontsize = 14)\n",
    "        ax.legend(title = \"survived\", loc = 'upper center') \n",
    "        \n",
    "plt.tight_layout()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2**: Explore Interactive Plots\n",
    "\n",
    "We use `bokeh` to visualize base rates in an interactive plot, which provides additional information when hovering over and an easy way to save the figure to disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked histogram of Survived by Sex\n",
    "df = pd.crosstab(data['Sex'], data['Survived'], margins=False)\n",
    "df.columns = ['Survived: 0', 'Survived: 1']\n",
    "df.plot_bokeh(kind='barh', stacked=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked histogram of Survived by Pclass\n",
    "df = pd.crosstab(data['Pclass'], data['Survived'], margins=False)\n",
    "df.columns = ['Survived: 0', 'Survived: 1']\n",
    "df.plot_bokeh(kind='barh', stacked=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked histogram of Survived by Age\n",
    "\n",
    "data['Age_binned'] = pd.cut(data['Age'], bins=list(range(0, 81, 10))).astype(str)\n",
    "df = pd.crosstab(data['Age_binned'], data['Survived'], margins=False)\n",
    "df.columns = ['Survived: 0', 'Survived: 1']\n",
    "df.plot_bokeh(kind='barh', stacked=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3**: Explore Swarm and Violin Plots\n",
    "\n",
    "Swarm and violin plots show the same data as plots of counts and/or density distributions (graphs you did earlier), but they also happen to look very cool and can also draw attention to details that you do not see in other plots. If you have extra time, try to make a few of these below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants (such as PALLET and FIGSIZE) so that all of your figures look consistent\n",
    "\n",
    "PALETTE = [\"lightgreen\" , \"lightblue\"]  # you can set a custom palette with a simple list of named color values\n",
    "FIGSIZE = (14, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a seaborn \"stripplot\" to examine survival by age and class.\n",
    "\n",
    "fig, ax = plt.subplots(figsize = FIGSIZE)\n",
    "sns.stripplot(x = 'Pclass', y = 'Age', hue = 'Survived', dodge = True, data = data, palette = PALETTE, size = 6, ax = ax)\n",
    "plt.title('Survival Events by Age and Class ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a seaborn \"swarmplot\" to examine survival by age and class.\n",
    "\n",
    "fig, ax = plt.subplots(figsize = FIGSIZE)\n",
    "sns.swarmplot(x = 'Pclass', y = 'Age', hue = 'Survived', dodge = True, data = data, palette = PALETTE, size = 6, ax = ax)\n",
    "plt.title('Survival Events by Age and Class ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a seaborn \"violinplot\" to examine survival by age and class. \n",
    "\n",
    "fig, ax = plt.subplots(figsize = FIGSIZE)\n",
    "sns.violinplot(x = \"Pclass\", y = \"Age\", hue = 'Survived', data=data, split=True, bw = 0.05 , palette = PALETTE, ax = ax)\n",
    "plt.title('Survival Distributions by Age and Class ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recap of basic Python visualization methods with matplotlib: https://machinelearningmastery.com/data-visualization-methods-in-python/ \n",
    "- Guide to Bokeh, an increasingly popular Python interactive visualization package: https://realpython.com/python-data-visualization-bokeh/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
