{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DSFM Workshop**: Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 3**: Active learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creator: [Data Science for Managers - EPFL Program](https://www.dsfm.ch)  \n",
    "Source:  [https://github.com/dsfm-org/code-bank.git](https://github.com/dsfm-org/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Overview**\n",
    "\n",
    "Labeled observations tend to be scarce in the business world and time-consuming to annotate. Hence, understanding whether it makes sense to obtain more labeled observations and, if so, which observations to label next can be of great use.\n",
    "\n",
    "Which new labeled observations might be most useful to the model? You often want to label observations in the uncertain region, i.e. in regions where the model predicts neither one of the classes. For example, in a binary classification, the uncertain region might be unlabeled observations with a predicted probability around 0.5. The approach to decide which observations to label next is generally known as your \"querying strategy\".\n",
    "\n",
    "The general workflow in active learning looks as follows (from the modAL GitHub repository): \n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/23e6a639d055d5ce91e89c12b27c008066a0d314/68747470733a2f2f6d6f64616c2d707974686f6e2e72656164746865646f63732e696f2f656e2f6c61746573742f5f696d616765732f6163746976652d6c6561726e696e672e706e67\" widt=500>\n",
    "\n",
    "[Image source](https://camo.githubusercontent.com/23e6a639d055d5ce91e89c12b27c008066a0d314/68747470733a2f2f6d6f64616c2d707974686f6e2e72656164746865646f63732e696f2f656e2f6c61746573742f5f696d616765732f6163746976652d6c6561726e696e672e706e67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Learning goals**\n",
    "\n",
    "- Understand the active learning workflow\n",
    "- Learn about different querying strategies (random vs. uncertainty sampling) to prioritize the unlabeled pool of observations\n",
    "- Experiment with `modAL`, a flexible package for active learning in Python\n",
    "- Explore a user-friendly annotation tool with active learning and transfer learning built-in: `prodi.gy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Useful resources**\n",
    "\n",
    "- Dataiku article on experimenting with different [Python packages for active learning](https://blog.dataiku.com/a-proactive-look-at-active-learning-packages)\n",
    "- `modAL` GitHub repository containing many [examples](https://github.com/modAL-python/modAL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://greendayonline.com/wp-content/uploads/2017/03/Recovering-From-Student-Loan-Default.jpg\" width=\"500\" height=\"600\" align=\"center\"/>\n",
    "\n",
    "\n",
    "[Image source](https://greendayonline.com/wp-content/uploads/2017/03/Recovering-From-Student-Loan-Default.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that all packages are installed \n",
    "# import sys\n",
    "# !{sys.executable} -m pip install modAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1:** Load data\n",
    "\n",
    "We will try to predict the probability of defaulting on a credit card account at a Taiwanese bank. A credit card default happens when a customer fails to pay the minimum due on a credit card bill for more than 6 months. \n",
    "\n",
    "We will use a dataset from a Taiwanese bank with 30,000 observations (Source: *Yeh, I. C., & Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2), 2473-2480.*). Each observation represents an account at the bank at the end of October 2005.  We renamed the variable default_payment_next_month to customer_default. The target variable to predict is `customer_default` -- i.e., whether the customer will default in the following month (1 = Yes or 0 = No). The dataset also includes 23 other explanatory features. \n",
    "\n",
    "Variables are defined as follows:\n",
    "\n",
    "| Feature name     | Variable Type | Description \n",
    "|------------------|---------------|--------------------------------------------------------\n",
    "| customer_default | Binary        | 1 = default in following month; 0 = no default \n",
    "| LIMIT_BAL        | Continuous    | Credit limit   \n",
    "| SEX              | Categorical   | 1 = male; 2 = female\n",
    "| EDUCATION        | Categorical   | 1 = graduate school; 2 = university; 3 = high school; 4 = others\n",
    "| MARRIAGE         | Categorical   | 0 = unknown; 1 = married; 2 = single; 3 = others\n",
    "| AGE              | Continuous    | Age in years  \n",
    "| PAY1             | Categorical   | Repayment status in September, 2005 \n",
    "| PAY2             | Categorical   | Repayment status in August, 2005 \n",
    "| PAY3             | Categorical   | Repayment status in July, 2005 \n",
    "| PAY4             | Categorical   | Repayment status in June, 2005 \n",
    "| PAY5             | Categorical   | Repayment status in May, 2005 \n",
    "| PAY6             | Categorical   | Repayment status in April, 2005 \n",
    "| BILL_AMT1        | Continuous    | Balance in September, 2005  \n",
    "| BILL_AMT2        | Continuous    | Balance in August, 2005  \n",
    "| BILL_AMT3        | Continuous    | Balance in July, 2005  \n",
    "| BILL_AMT4        | Continuous    | Balance in June, 2005 \n",
    "| BILL_AMT5        | Continuous    | Balance in May, 2005  \n",
    "| BILL_AMT6        | Continuous    | Balance in April, 2005  \n",
    "| PAY_AMT1         | Continuous    | Amount paid in September, 2005\n",
    "| PAY_AMT2         | Continuous    | Amount paid in August, 2005\n",
    "| PAY_AMT3         | Continuous    | Amount paid in July, 2005\n",
    "| PAY_AMT4         | Continuous    | Amount paid in June, 2005\n",
    "| PAY_AMT5         | Continuous    | Amount paid in May, 2005\n",
    "| PAY_AMT6         | Continuous    | Amount paid in April, 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/credit_data.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of fraudulent transactions\n",
    "count = df['customer_default'].value_counts()\n",
    "percentage_default = count[1] / (count[1] + count[0]) * 100\n",
    "print('{}% of customers default'.format(round(percentage_default, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into labeled data and unlabeled data (we pretend)\n",
    "# Shuffle data\n",
    "SEED = 7\n",
    "df = df.sample(len(df), replace=False, random_state=SEED)\n",
    "\n",
    "# Split df into equal-sized parts\n",
    "import numpy as np\n",
    "df1, df2, df3, df4, df5 = np.array_split(df, 5)\n",
    "\n",
    "# DF1: labeled data\n",
    "# DF2: pool of unlabeled data\n",
    "# DF3: test data for evaluation\n",
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "print(df3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2:** Active learning for classification\n",
    "\n",
    "In this classification example, we use the `entropy_sampling` querying strategy. In this strategy, the \"next best\" unlabeled observation is the one for which the predicted probabilities have the highest entropy. \n",
    "\n",
    "The `modAL` package includes other querying strategies, including **margin sampling**, which selects the instances where the difference between the first most likely and second most likely classes are the smallest. Other querying strategies include **expected model change** (label observations that would most change the current model), **expected error reduction**, and **query by committee** (train different models on labeled data and select the unlabeled observations where the predictions diverge the most).\n",
    "\n",
    "Let's investigate how the model performance changes with active learning. Note that due to the small number of training observations, the random seed play an unrealistically important role in the model performance we obtain.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modAL.models import ActiveLearner\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from modAL.uncertainty import entropy_sampling\n",
    "\n",
    "# Only train on few examples\n",
    "df1_small = df1.sample(30, random_state = SEED)\n",
    "\n",
    "# Split features from target variable\n",
    "X_df1 = df1_small.drop(columns=['customer_default'], inplace=False).values\n",
    "y_df1 = df1_small['customer_default'].values\n",
    "\n",
    "# initializing the learner\n",
    "learner = ActiveLearner(\n",
    "    estimator = GradientBoostingClassifier(random_state = SEED),\n",
    "    query_strategy = entropy_sampling,\n",
    "    X_training = X_df1, y_training = y_df1\n",
    ")\n",
    "\n",
    "df1_small['customer_default'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features from target variable\n",
    "X_df3 = df3.drop(columns=['customer_default'], inplace=False).values\n",
    "y_df3 = df3['customer_default'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of the initial classifier\n",
    "pred = learner.predict_proba(X_df3)\n",
    "print('AUC: {}'.format(roc_auc_score(y_df3, pred[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query another X newly labeled samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_queries = 20\n",
    "\n",
    "# Split features from target variable\n",
    "X_df2 = df2.drop(columns=['customer_default'], inplace=False).values\n",
    "y_df2 = df2['customer_default'].values\n",
    "\n",
    "for idx in range(n_queries):\n",
    "    query_idx, query_instance = learner.query(X_df2)\n",
    "    print('Observation {}'.format(query_idx[0]).ljust(20) +  'prediction {}'.format(round(learner.predict_proba(X_df2[query_idx])[0][1], 4)))\n",
    "    # Oracle labels the observation selected by our querying strategy\n",
    "    learner.teach(X_df2[query_idx], y_df2[query_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of the initial classifier\n",
    "pred = learner.predict_proba(X_df3)\n",
    "print('AUC: {}'.format(roc_auc_score(y_df3, pred[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions based on the Dataiku article\n",
    "\n",
    "For the article, click [here](https://blog.dataiku.com/a-proactive-look-at-active-learning-packages)\n",
    "\n",
    "1. What querying strategy do you think works best without any labeled data?\n",
    "2. Do you think active learning is more or less effective for classifications with many or few classes?\n",
    "3. What's a good way to select between different querying strategies?\n",
    "    \n",
    "- Answer 1: random strategy\n",
    "- Answer 2: active learning is more useful in situations with many classes\n",
    "- Answer 3: no unifying framework exists at the moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3:** Active learning for regression\n",
    "\n",
    "We now look at a time-series regression problem. The visualizations show that only learning on few data points results in poor performance. Furthermore, we compare querying random observations and querying observations with the highest predicted uncertainty to learn that querying strategies can make a difference for effectively labeling new observations to improve model performance. \n",
    "\n",
    "Note that we use a Gaussian process regression (GPR) as our model. GPR is a nonparametric, Bayesian approach to regression that provides uncertainty measurements for predictions, which is the main reason we are using this model here. Nonparametric simply means that the model is not bound to a particular function form (e.g. linear, quadratic). \n",
    "\n",
    "Code adapted from: [modAL on GitHub](https://github.com/modAL-python/modAL/blob/master/examples/active_regression.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the data\n",
    "SEED = 7\n",
    "STD = 0.3\n",
    "np.random.seed(SEED)\n",
    "X = np.random.choice(np.linspace(0, 20, 10000), size=200, replace=False).reshape(-1, 1)\n",
    "y = np.sin(X) + np.random.normal(scale=STD, size=X.shape)\n",
    "\n",
    "FIGSIZE = (10, 6)\n",
    "MARKERSIZE = 10\n",
    "LINEWIDTH = 3\n",
    "DPI = 120\n",
    "\n",
    "# Plotting the initial estimation\n",
    "with plt.style.context('seaborn-white'):\n",
    "    plt.figure(figsize=FIGSIZE, dpi=DPI)\n",
    "    x = np.linspace(0, 20, 1000)\n",
    "    y_true = np.sin(x)\n",
    "    plt.plot(x, y_true, c='k', linewidth=LINEWIDTH, label='True DGP')\n",
    "    plt.fill_between(x, y_true - STD, y_true + STD, alpha=0.2, color='gray', label='+/- 1 SD')\n",
    "    plt.scatter(X, y, c='k', s=MARKERSIZE, label='Sampled data')\n",
    "    plt.title('Randomly sampled data')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Query strategy: select observations at random\n",
    "def random_sampling(classifier, X_pool):\n",
    "    n_samples = len(X_pool)\n",
    "    query_idx = np.random.choice(range(n_samples))\n",
    "    return query_idx, X_pool[query_idx]\n",
    "\n",
    "# Assembling initial training set with few data points\n",
    "n_initial = 5\n",
    "COLOR = 'cornflowerblue'\n",
    "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
    "X_initial, y_initial = X[initial_idx], y[initial_idx]\n",
    "\n",
    "# Defining the kernel for the Gaussian process\n",
    "kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
    "         + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
    "\n",
    "# Initializing the active learner\n",
    "regressor = ActiveLearner(\n",
    "    estimator = GaussianProcessRegressor(kernel=kernel, random_state=SEED),\n",
    "    query_strategy = random_sampling,\n",
    "    X_training = X_initial.reshape(-1, 1), y_training=y_initial.reshape(-1, 1)\n",
    ")\n",
    "\n",
    "pred, std = regressor.predict(X.reshape(-1,1), return_std=True)\n",
    "rmse = sqrt(mean_squared_error(y.reshape(1,-1)[0], pred[:,0]))\n",
    "print('RMSE with initial data: {}'.format(round(rmse, 4)))\n",
    "\n",
    "# plotting the initial estimation\n",
    "with plt.style.context('seaborn-white'):\n",
    "    x = np.linspace(0, 20, 1000)\n",
    "    \n",
    "    # DGP\n",
    "    plt.figure(figsize=FIGSIZE, dpi=DPI)\n",
    "    plt.plot(x, y_true, c='k', linewidth=LINEWIDTH, label='True DGP', alpha=0.2)\n",
    "    plt.fill_between(x, y_true - STD, y_true + STD, alpha=0.2, color='gray', label='+/- 1 SD')\n",
    "    plt.scatter(X, y, c='k', s=MARKERSIZE, label='Sampled data', alpha=0.2)\n",
    "    \n",
    "    # Prediction\n",
    "    pred, std = regressor.predict(x.reshape(-1,1), return_std=True)\n",
    "    plt.plot(x, pred, color=COLOR)\n",
    "    plt.fill_between(x, pred.reshape(-1, )-std, pred.reshape(-1, )+std, alpha=0.2, color=COLOR)\n",
    "    # Plot initial data\n",
    "    plt.scatter(X_initial, y_initial, s=MARKERSIZE+30, color=COLOR, label='Initial data')\n",
    "    plt.title('Initial estimation based on %d points' % n_initial)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the active learner\n",
    "regressor = ActiveLearner(\n",
    "    estimator = GaussianProcessRegressor(kernel=kernel, random_state=SEED),\n",
    "    query_strategy = random_sampling,\n",
    "    X_training = X_initial.reshape(-1, 1), y_training=y_initial.reshape(-1, 1)\n",
    ")\n",
    "    \n",
    "# Active learning\n",
    "n_queries = 10\n",
    "x = np.linspace(0, 20, 1000)\n",
    "X_new, y_new = [], []\n",
    "predictions, stds = [], []\n",
    "for idx in range(n_queries):\n",
    "    query_idx, query_instance = regressor.query(X)\n",
    "    regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
    "    pred, std = regressor.predict(x.reshape(-1,1), return_std=True)\n",
    "    \n",
    "    X_new.append(X[query_idx].reshape(1, -1))\n",
    "    y_new.append(y[query_idx].reshape(1, -1))\n",
    "    predictions.append(pred)\n",
    "    stds.append(std)\n",
    "    \n",
    "# Plotting after active learning\n",
    "for i in range(len(X_new)):\n",
    "    \n",
    "    with plt.style.context('seaborn-white'):\n",
    "\n",
    "        # DGP\n",
    "        plt.figure(figsize=FIGSIZE, dpi=DPI)\n",
    "        plt.plot(x, y_true, c='k', linewidth=LINEWIDTH, label='True DGP', alpha=0.2)\n",
    "        plt.fill_between(x, y_true - STD, y_true + STD, alpha=0.2, color='gray', label='+/- 1 SD')\n",
    "        plt.scatter(X, y, c='k', s=MARKERSIZE, label='Sampled data', alpha=0.2)\n",
    "\n",
    "        # Prediction\n",
    "        plt.plot(x, predictions[i], color='red', label='Latest estimate')\n",
    "        # Previous estimates\n",
    "        for j in range(i):\n",
    "            plt.plot(x, predictions[j], color='red', linestyle='dashed', linewidth=2, alpha=(j+1)/(i+1)*0.5)\n",
    "\n",
    "        plt.fill_between(x, predictions[i].reshape(-1, ) - stds[i], predictions[i].reshape(-1, ) + stds[i], alpha=0.3, color='red')\n",
    "        # Plot latest observation\n",
    "        plt.scatter(X_new[i:i+1], y_new[i:i+1], marker='*', s=MARKERSIZE+200, color='red', label='Latest queried obs.')\n",
    "        # Plot previous observations\n",
    "        if len(X_new[:1]) > 0: plt.scatter(X_new[:i], y_new[:i], s=MARKERSIZE+30, color='red', label='Previously queried obs.')\n",
    "        # Plot initial data \n",
    "        plt.scatter(X_initial, y_initial, s=MARKERSIZE+30, color=COLOR, label='Initial data', alpha=0.5)\n",
    "        plt.title('RANDOM SAMPLING: Estimation after an additional %d observation(s)' % (i+1))\n",
    "        plt.grid()\n",
    "        plt.legend(loc='lower left')\n",
    "        plt.show()\n",
    "\n",
    "pred, std = regressor.predict(X.reshape(-1,1), return_std=True)\n",
    "rmse = sqrt(mean_squared_error(y.reshape(1,-1)[0], pred[:,0]))\n",
    "print('RMSE with {} new observations: {}'.format(n_queries, round(rmse, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query strategy: select observation with the largest standard deviation\n",
    "def GP_regression_std(regressor, X):\n",
    "    _, std = regressor.predict(X, return_std=True)\n",
    "    query_idx = np.argmax(std)\n",
    "    return query_idx, X[query_idx]\n",
    "\n",
    "# Initializing the active learner\n",
    "regressor = ActiveLearner(\n",
    "    estimator = GaussianProcessRegressor(kernel=kernel, random_state=SEED),\n",
    "    query_strategy = GP_regression_std,\n",
    "    X_training = X_initial.reshape(-1, 1), y_training=y_initial.reshape(-1, 1)\n",
    ")\n",
    "    \n",
    "# Active learning\n",
    "n_queries = 10\n",
    "x = np.linspace(0, 20, 1000)\n",
    "X_new, y_new = [], []\n",
    "predictions, stds = [], []\n",
    "for idx in range(n_queries):\n",
    "    query_idx, query_instance = regressor.query(X)\n",
    "    regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
    "    pred, std = regressor.predict(x.reshape(-1,1), return_std=True)\n",
    "    \n",
    "    X_new.append(X[query_idx].reshape(1, -1))\n",
    "    y_new.append(y[query_idx].reshape(1, -1))\n",
    "    predictions.append(pred)\n",
    "    stds.append(std)\n",
    "    \n",
    "# Plotting after active learning\n",
    "for i in range(len(X_new)):\n",
    "\n",
    "    with plt.style.context('seaborn-white'):\n",
    "\n",
    "        # DGP\n",
    "        plt.figure(figsize=FIGSIZE, dpi=DPI)\n",
    "        plt.plot(x, y_true, c='k', linewidth=LINEWIDTH, label='True DGP', alpha=0.2)\n",
    "        plt.fill_between(x, y_true - STD, y_true + STD, alpha=0.2, color='gray', label='+/- 1 SD')\n",
    "        plt.scatter(X, y, c='k', s=MARKERSIZE, label='Sampled data', alpha=0.2)\n",
    "\n",
    "        # Prediction\n",
    "        plt.plot(x, predictions[i], color='red', label='Latest estimate')\n",
    "        # Previous estimates\n",
    "        for j in range(i):\n",
    "            plt.plot(x, predictions[j], color='red', linestyle='dashed', linewidth=2, alpha=(j+1)/(i+1)*0.5)\n",
    "\n",
    "        plt.fill_between(x, predictions[i].reshape(-1, )-stds[i], predictions[i].reshape(-1, )+stds[i], alpha=0.3, color='red')\n",
    "        # Plot latest observation\n",
    "        plt.scatter(X_new[i:i+1], y_new[i:i+1], marker='*', s=MARKERSIZE+200, color='red', label='Latest queried obs.')\n",
    "        # Plot previous observations\n",
    "        if len(X_new[:1]) > 0: plt.scatter(X_new[:i], y_new[:i], s=MARKERSIZE+30, color='red', label='Previously queried obs.')\n",
    "        # Plot initial data \n",
    "        plt.scatter(X_initial, y_initial, s=MARKERSIZE+30, color=COLOR, label='Initial data', alpha=0.5)\n",
    "        plt.title('UNCERTAINTY SAMPLING: Estimation after an additional %d observation(s)' % (i+1))\n",
    "        plt.grid()\n",
    "        plt.legend(loc='lower left')\n",
    "        plt.show()\n",
    "\n",
    "pred, std = regressor.predict(X.reshape(-1,1), return_std=True)\n",
    "rmse = sqrt(mean_squared_error(y.reshape(1,-1)[0], pred[:,0]))\n",
    "print('RMSE with {} new observations: {}'.format(n_queries, round(rmse, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 4:** prodigy annotation tool demo\n",
    "\n",
    "prodigy is a software tool for active learning. It was developed by Explosion AI, the makers of the `spaCy` NLP package. Let's demo what prodigy looks like to see what effective annotation can look like. \n",
    "\n",
    "Of course, there are numerous alternatives to doing your own annotation, including Amazon Mechanical Turk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Click here for the prodigy demo](https://prodi.gy/demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bonus questions**:\n",
    "\n",
    "1. In what scenario might a \"margin sampling\" querying strategy work well?\n",
    "2. What's another simple approach to answering \"Is it worth collecting more data?\"\n",
    "\n",
    "- Answer 1: multi-class problems\n",
    "- Answer 2: learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
