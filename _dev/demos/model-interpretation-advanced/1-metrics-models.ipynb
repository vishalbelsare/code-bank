{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DSFM Workshop**: Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 1**: Evaluation metrics and interpretable models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creator: [Data Science for Managers - EPFL Program](https://www.dsfm.ch)  \n",
    "Source:  [https://github.com/dsfm-org/code-bank.git](https://github.com/dsfm-org/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Workshop introduction**\n",
    "\n",
    "Algorithms take an increasingly prominent place in our personal and professional environment. To ensure that the decisions that these algorithms contribute to are based on fair, trustworthy, and compliant predictions, we have to interpret prediction models. In other words, we open the black box.\n",
    "\n",
    "Model interpretation can mean different things to different people. We will focus on methods and tools that help us better understand how a model makes a prediction and why. The benefits to interpreting models can be numerous, for example:\n",
    "\n",
    "- **Feature selection**: understand which features are most predictive; focus your resources accordingly\n",
    "- **Debugging**: understand why the model makes particulary prediction errors\n",
    "- **Fairness**: detect whether the model systematically discriminates in an undesirable way\n",
    "- **Regulatory compliance**: ensure that the model satisfies legal requirements\n",
    "- **Trust**: increase stakeholders' trust into the model's predictions\n",
    "\n",
    "We split this workshop into four sections - we are currently in Section 1. Each section contains one fully-worked Jupyter Notebook with different data to illustrate the concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Learning goals**\n",
    "\n",
    "- Review common evaluation metrics for regression: RMSE, MAE, MAPE\n",
    "- Learn about interpretable models: regression coefficients, lasso, decision trees, and explainable boosting regression\n",
    "- Experiment with `InterpretML`, a powerful package for explainable data science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Useful resources**\n",
    "\n",
    "- Chapter 2 and 4, Molnar (2019), i.e. Molnar, Christoph. \"Interpretable machine learning. A Guide for Making Black Box Models Explainable\", 2019. https://christophm.github.io/interpretable-ml-book/.\n",
    "\n",
    "- A brief overview of the [main model explainability methods](https://everdark.github.io/k9/notebooks/ml/model_explain/model_explain.nb.html)\n",
    "- Microsoft's `InterpretML` GitHub repository containing many [example notebooks](https://github.com/interpretml/interpret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://images.unsplash.com/photo-1458086294493-3a5a041289ff?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=2250&q=80\n",
    "\" width=600></center>\n",
    "\n",
    "A red brick house in Boston. [Image source](https://images.unsplash.com/photo-1458086294493-3a5a041289ff?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=2250&q=80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that all packages are installed \n",
    "# import sys\n",
    "# !{sys.executable} -m pip install interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1:** Data and EDA\n",
    "\n",
    "We will use a common dataset for this notebook, called the \"Boston Housing\" dataset. It contains information collected by the U.S Census Service on housing the Boston area. The target variable is `MEDV`, the median value of owner-occupied homes in $1000's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston = load_boston()\n",
    "feature_names = list(boston.feature_names)\n",
    "df = pd.DataFrame(boston.data, columns=feature_names)\n",
    "df[\"target\"] = boston.target\n",
    "\n",
    "train_cols = df.columns[0:-1]\n",
    "label = df.columns[-1]\n",
    "X = df[train_cols]\n",
    "y = df[label]\n",
    "\n",
    "SEED = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = SEED)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original shape: {}'.format(X.shape))\n",
    "print('Train shapes: {} {} '.format(X_train.shape, X_test.shape) + 'Test shapes: {} {}'.format(y_train.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret import show, preserve\n",
    "from interpret.data import Marginal\n",
    "\n",
    "marginal = Marginal().explain_data(X_train, y_train, name = 'Train Data')\n",
    "\n",
    "# Use show(.) on your local machine\n",
    "# We use preserve(.) because the Google VM does not allow background server access\n",
    "# show(marginal)\n",
    "preserve(marginal, file_name='outputs/1-marginal.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2:** Linear regression and lasso\n",
    "\n",
    "The linear regression and lasso regression are useful and interpretable baseline models.\n",
    "\n",
    "Do we need to standardize data? No, if we just care about making predictions. Yes, if we also care about comparing coefficient magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Build pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('lr', LinearRegression()))\n",
    "pipeline_lr = Pipeline(estimators)\n",
    "\n",
    "lr = pipeline_lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "lr_r2   = explained_variance_score(y_test, lr_pred)\n",
    "lr_mae  = mean_absolute_error(y_test, lr_pred)\n",
    "lr_rmse = sqrt(mean_squared_error(y_test, lr_pred))\n",
    "\n",
    "print('EV:   {}'.format(round(lr_r2, 4)))\n",
    "print('MAE:  {}'.format(round(lr_mae, 4)))\n",
    "print('RMSE: {}'.format(round(lr_rmse, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "\n",
    "# Build pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('lasso', Lasso(alpha = 0.03)))\n",
    "pipeline_lasso = Pipeline(estimators)\n",
    "\n",
    "lasso = pipeline_lasso.fit(X_train, y_train)\n",
    "lasso_pred = lasso.predict(X_test)\n",
    "\n",
    "lasso_r2   = explained_variance_score(y_test, lasso_pred)\n",
    "lasso_mae  = mean_absolute_error(y_test, lasso_pred)\n",
    "lasso_rmse = sqrt(mean_squared_error(y_test, lasso_pred))\n",
    "\n",
    "print('EV:   {}'.format(round(lasso_r2, 4)))\n",
    "print('MAE:  {}'.format(round(lasso_mae, 4)))\n",
    "print('RMSE: {}'.format(round(lasso_rmse, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('REGULARIZATION:'.center(22), 'NONE'.center(10), 'LASSO'.center(10))\n",
    "print('=' * 50)\n",
    "for (varname, lm_coef, lml1_coef) in zip(feature_names, pipeline_lr['lr'].coef_, pipeline_lasso['lasso'].coef_):\n",
    "    lm_coeff  = \"{0:.4f}\".format(lm_coef).rjust(10)\n",
    "    lml1_coef = \"{0:.4f}\".format(lml1_coef).rjust(10) if abs(lml1_coef) > 0.0001 else \"\"\n",
    "    print(str(varname).center(20), lm_coeff, lml1_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3:** Decision tree\n",
    "\n",
    "Tree-based models are another example of interpretable models. A tree makes its first split on the feature that it finds most useful in predicting the target variable and continues splitting recursively.\n",
    "\n",
    "Do we need to standardize data? No, each tree node can find an appropriate splitting point across the entire range of values for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Fit decision tree  \n",
    "dt = DecisionTreeRegressor(criterion='mse', max_depth = 4)  # just one tree, so no pipeline needed\n",
    "dt.fit(X_train, y_train)\n",
    "dt_pred = dt.predict(X_test)\n",
    "\n",
    "dt_r2   = explained_variance_score(y_test, dt_pred)\n",
    "dt_mae  = mean_absolute_error(y_test, dt_pred)\n",
    "dt_rmse = sqrt(mean_squared_error(y_test, dt_pred))\n",
    "\n",
    "print('EV:   {}'.format(round(dt_r2, 4)))\n",
    "print('MAE:  {}'.format(round(dt_mae, 4)))\n",
    "print('RMSE: {}'.format(round(dt_rmse, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('Feature'.center(12), '   ',  'Importance')\n",
    "print('=' * 30)\n",
    "for index in reversed(np.argsort(dt.feature_importances_)):\n",
    "    print(str(feature_names[index]).center(12) , '   ', '{0:.4f}'.format(dt.feature_importances_[index]).center(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Source\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "Source(export_graphviz(dt, out_file=None, feature_names=feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \n",
    "print(X_test.iloc[0, ])\n",
    "print()\n",
    "print('Prediction: {}'.format(dt_pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 4:** Explainable boosting machine (EBM)\n",
    "\n",
    "Microsoft Research has developed an open-source tool called [InterpretML](https://github.com/interpretml/interpret), which combines data science techniques like bagging, gradient boosting, and automatic interaction detection. Explainable boosting machines (EBMs) produce lossless explanations while performing as well as gradient boosting and random forest methods. For more details on the EBM algorithm watch [this video](https://www.youtube.com/watch?v=MREiHgHgl0k)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EBM performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import ExplainableBoostingRegressor, LinearRegression, RegressionTree\n",
    "from interpret import show\n",
    "from interpret.perf import RegressionPerf\n",
    "\n",
    "ebm = ExplainableBoostingRegressor()\n",
    "ebm.fit(X_train, y_train)\n",
    "ebm_pred = ebm.predict(X_test)\n",
    "\n",
    "ebm_perf = RegressionPerf(ebm.predict).explain_perf(X_test, y_test, name='EBM')\n",
    "\n",
    "# show(ebm_perf)\n",
    "preserve(ebm_perf, file_name='outputs/1-ebm_perf.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_r2   = explained_variance_score(y_test, ebm_pred)\n",
    "ebm_mae  = mean_absolute_error(y_test, ebm_pred)\n",
    "ebm_rmse = sqrt(mean_squared_error(y_test, ebm_pred))\n",
    "\n",
    "print('EV:   {}'.format(round(ebm_r2, 4)))\n",
    "print('MAE:  {}'.format(round(ebm_mae, 4)))\n",
    "print('RMSE: {}'.format(round(ebm_rmse, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Explanations: What the model learned overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_global = ebm.explain_global(name='EBM')\n",
    "\n",
    "# show(ebm_global)\n",
    "preserve(ebm_global, file_name='outputs/1-ebm_global.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import RegressionTree\n",
    "\n",
    "rt = RegressionTree()\n",
    "rt.fit(X_train, y_train)\n",
    "\n",
    "rt_global = rt.explain_global(name='Regression Tree')\n",
    "rt_perf = RegressionPerf(rt.predict).explain_perf(X_test, y_test, name='Regression Tree')\n",
    "\n",
    "# Unfortunately, visualizing the decision tree only works on a local machine \n",
    "# show(rt_global)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Explanations: How an individual prediction was made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_local = ebm.explain_local(X_test[:5], y_test[:5], name='EBM')\n",
    "\n",
    "# show(ebm_local)\n",
    "preserve(ebm_local, selector_key = ebm_local.selector[ebm_local.selector.columns[0]], file_name='outputs/1-ebm_local.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard: look at everything at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do everything in one shot with the InterpretML Dashboard by passing a list into show\n",
    "\n",
    "# Unfortunately, visualizing the dashboard only works on a local machine \n",
    "# show([marginal, rt_global, rt_perf, ebm_global, ebm_perf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Summary of RMSE scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of RMSE scores \n",
    "width     = 30\n",
    "width_box = 100\n",
    "models    = ['Linear Regression', 'Lasso', 'Decision Tree', 'EBM']\n",
    "results   = [lr_rmse, lasso_rmse, dt_rmse, ebm_rmse]\n",
    "\n",
    "print(str('=' * width).center(width_box))\n",
    "print('Summary of RMSE Scores'.center(width_box))\n",
    "print(str('=' * width).center(width_box))\n",
    "for i in range(len(models)):\n",
    "    line = models[i].center(width - 8) + '{0:.4f}'.format(results[i])\n",
    "    print(line.center(width_box))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bonus questions**:\n",
    "\n",
    "1. What's generally an advantage of how RMSE calculates the prediction error?\n",
    "2. Can you think of scenarios in which using the RMSE might not be appropriate?\n",
    "\n",
    "- Answer 1: large residuals/errors are punished more\n",
    "- Answer 2: when prediction errors cost more for large values (e.g. reputation costs when making valuation errors for expensive houses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
