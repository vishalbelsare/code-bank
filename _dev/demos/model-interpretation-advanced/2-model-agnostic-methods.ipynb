{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DSFM Workshop**: Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 2**: Model-agnostic methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creator: [Data Science for Managers - EPFL Program](https://www.dsfm.ch)  \n",
    "Source:  [https://github.com/dsfm-org/code-bank.git](https://github.com/dsfm-org/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Overview**\n",
    "\n",
    "Model-agnostic methods separate the explanations from the prediction model. There are many different model-agnostic methods, including partial dependence plots (PDPs), accumulated local effects (ALEs), permutation feature importance, local surrogate (LIME), and Shapley values with different advantages and disadvantages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Learning goals**\n",
    "\n",
    "- Learn how to interpret different model-agnostic methods, such as partial dependence plots, permutation feature importance, and Shapley values\n",
    "- Get an intuition about the advantages and disadvantages\n",
    "- Experiment with `SHAP`, a powerful interpretability package for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Useful resources**\n",
    "\n",
    "- Chapter 5, Molnar (2019)\n",
    "- `SHAP` GitHub repository containing many [example notebooks](https://github.com/slundberg/shap)\n",
    "- `LIME` GitHub repository, another common [model-agnostic interpretation method](https://github.com/marcotcr/lime)\n",
    "- NIPS paper introducing Shapley value for model interpretation: http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://images.unsplash.com/photo-1584748452591-640305621fc5?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=908&q=80\n",
    "\" width=300></center>\n",
    "\n",
    "A US Census Bureau letter. [Image source](https://images.unsplash.com/photo-1584748452591-640305621fc5?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=908&q=80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1:** Load data\n",
    "\n",
    "We will use the **Adult income dataset** from the UCI machine learning repository, which has 12 variables and a binary target feature. We predict the probability of an individual making over $50k a year in annual income. Data source: https://archive.ics.uci.edu/ml/datasets/adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "\n",
    "X, y = shap.datasets.adult()\n",
    "\n",
    "# create a train/test split\n",
    "SEED = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=SEED)\n",
    "\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original shape: {}'.format(X.shape))\n",
    "print('Train shapes: {} {} '.format(X_train.shape, X_test.shape) + 'Test shapes: {} {}'.format(y_train.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the target feature base rate?\n",
    "round(sum(y_train)/len(y_train), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2:** Partial dependence plots (PDPs)\n",
    "\n",
    "A simple method to interpret the impact of each predictor on the target variable are partial dependence plot. Assuming that input variables are uncorrelated, these plot show how the average prediction changes when the i'th feature value changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import plot_partial_dependence, partial_dependence\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [16, 14]\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=SEED).fit(X_train, y_train)\n",
    "# clf = LogisticRegression(random_state=SEED, solver='liblinear').fit(X_train, y_train)\n",
    "print('AUC: {}'.format(roc_auc_score([int(i) for i in y_test], clf.predict_proba(X_test)[:, 1])))\n",
    "\n",
    "# Note: the y-axes are clipped at the 5th and 95th percentiles\n",
    "fig = plot_partial_dependence(clf, X_train, features = X_train.columns.values, grid_resolution = 300) \n",
    "fig.figure_.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "features = ('Education-Num', 'Age')\n",
    "pdp, axes = partial_dependence(clf, X_train, features=features, grid_resolution=20, percentiles=(0,1))\n",
    "XX, YY = np.meshgrid(axes[0], axes[1])\n",
    "Z = pdp[0].T\n",
    "ax = Axes3D(fig)\n",
    "surf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1, cmap=plt.cm.BuPu, edgecolor='k')\n",
    "ax.set_xlabel(features[0])\n",
    "ax.set_ylabel(features[1])\n",
    "ax.set_zlabel('Partial dependence')\n",
    "#  Pretty init view\n",
    "ax.view_init(elev=22, azim=122)\n",
    "plt.colorbar(surf)\n",
    "plt.suptitle('Partial dependence of house value on median\\n'\n",
    "             'age and average occupancy, with Gradient Boosting')\n",
    "plt.subplots_adjust(top=0.9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review questions**:\n",
    "\n",
    "- What would the PDPs look like when using fitting a linear regression? What about a logistic regression?\n",
    "- What do the tick marks on the X-axes represent? Answer: the deciles, i.e. 10% percentile marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In summary**, partial dependence plots show the marginal impact of a feature on the predicted outcome. \n",
    "\n",
    "**Advantages**: \n",
    "\n",
    "- Intuitive interpretation: each point on the partial depence function simulates the average prediction if we were to force all data points to assume that value\n",
    "- Computationally efficient \n",
    "\n",
    "**Disadvantages**:\n",
    "\n",
    "- Features are assumed to be independent (hardly ever true): correlated features can lead to unrealistic feature combinations that make averaging predictions unreliable\n",
    "- Heterogeneous effects might be hidden because plots only show *average* marginal effects\n",
    "- Partial dependence plots might not show the feature distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3:** Individual conditional expectation (ICE)\n",
    "\n",
    "Similar to PDPs, an individual conditional expectation (ICE) plot visualizes the dependence between the feature of interest and the outcome. However, unlike PDPs, which show the average effect of the features of interest, ICE plots visualize the dependence of the prediction on a feature for *each sample separately*, with one line per sample. Only one feature of interest is supported for ICE plots.\n",
    "\n",
    "We will use the `pdpbox` package to draw ICE plots - for detail see [here](https://github.com/SauceCat/PDPbox)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdpbox import pdp, info_plots\n",
    "pdp_age = pdp.pdp_isolate(model=clf, dataset=X_train, num_grid_points=10, model_features=X_train.columns, n_jobs=-1, feature='Age')\n",
    "\n",
    "#ICE Plot\n",
    "fig, axes = pdp.pdp_plot(pdp_age, 'Age', plot_lines=True, center=True, frac_to_plot=0.5, plot_pts_dist=True, figsize=[16, 12],\n",
    "                         x_quantile=True, show_percentile=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 4:** Permutation feature importance\n",
    "\n",
    "The idea of permutation feature importance is to **shuffle the values of one feature at a time** as to break its relation with the target variable. We then measure the increase in prediction error. A feature is \"more important\" if shuffling its values increases the model error, because the model relies more on the feature for the prediction. A feature is \"less important\" if shuffling its values leaves the model error unchanged, because the model does not find the feature useful for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "pi = permutation_importance(clf, X_test, y_test, n_repeats=30, random_state=SEED)\n",
    "pi_sorted = pi.importances_mean.argsort()\n",
    "\n",
    "# Print feature importance \n",
    "for i in pi_sorted[::-1]:\n",
    "    if pi.importances_mean[i] - 2 * pi.importances_std[i] > 0:\n",
    "        print('{}'.format(X_train.columns[i]).ljust(20) + 'mean = {}'.format(round(pi.importances_mean[i], 2)).ljust(15) + 'std = {}'.format(round(pi.importances_std[i], 4)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot importance \n",
    "plt.rcParams['figure.figsize'] = [16, 8]\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(pi.importances[pi_sorted].T, vert = False, labels = X_train.columns[pi_sorted])\n",
    "ax.set_title(\"Permutation Importances (TEST set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In summary**, the permutation importance approach tells us how much the prediction error would increase without a particular feature. \n",
    "\n",
    "**Advantages**: \n",
    "\n",
    "- Intuitive interpretation: feature importance is the increase in model error when the feature's information is destroyed\n",
    "- Very compressed, global insight into the model\n",
    "- Permutation also destroys interaction with all other features\n",
    "- No re-training required\n",
    "\n",
    "**Disadvantages**:\n",
    "\n",
    "- Unclear whether you should use training or testing data: experiment with both\n",
    "- Shuffling features at random requires many repeats to get reliable estimates\n",
    "- Beware of the feature importance of highly correlated features; these can be unstable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 5:** Shapley values\n",
    "\n",
    "Next, we can exploit the game theory concept of Shapley values to unpack the final prediction into its feature components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Shapley values?\n",
    "\n",
    "The setup is as follows: a coalition of players cooperates, and obtains a certain overall gain from that cooperation. Since some players may contribute more to the coalition than others or may possess different bargaining power (for example threatening to destroy the whole coalition), what final distribution of generated surplus among the players should arise in any particular game? Or phrased differently: how important is each player to the overall cooperation, and what payoff can he or she reasonably expect? The Shapley value provides one possible answer to this question. (Wikipedia)\n",
    "\n",
    "Let's consider a concrete example from Molnar (2019). Suppose you want to predict apartment prices. You get the following predictions for two apartments, one that allows cats and one that does not: \n",
    "\n",
    "<img src=\"https://christophm.github.io/interpretable-ml-book/images/shapley-instance-intervention.png\" width=600>\n",
    "\n",
    "Image source: https://christophm.github.io/interpretable-ml-book/images/shapley-instance-intervention.png\n",
    "\n",
    "Loosely speaking, what's the contribution of the `cat-banned` feature? It's EUR 320,000 - EUR 310,000 = EUR 10,000. When repeat this computation for all possible coalition, i.e. combination of features. For a detailed description of this example see: https://christophm.github.io/interpretable-ml-book/shapley.html\n",
    "\n",
    "The more formal interpretation of the Shapley value by Molnar goes as follows: \"Given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated Shapley value.\"\n",
    "\n",
    "**Gradient boosting machine** methods are state-of-the-art for these types of prediction problems with tabular style input data of many modalities. We will use an implementation called \"Tree SHAP\" that allows for the exact computation of SHAP values for tree ensemble methods, and has been integrated directly into the C++ LightGBM code base. This allows fast exact computation of SHAP values without sampling and without providing a background dataset (since the background is inferred from the coverage of the trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Print the JS visualization code to the notebook\n",
    "shap.initjs()\n",
    "\n",
    "d_train = lgb.Dataset(X_train, label=y_train)\n",
    "d_test = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    \"max_bin\": 512,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"num_leaves\": 10,\n",
    "    \"verbose\": -1,\n",
    "    \"min_data\": 100,\n",
    "    \"boost_from_average\": True\n",
    "}\n",
    "\n",
    "model = lgb.train(params, d_train, 10000, valid_sets=[d_test], early_stopping_rounds=50, verbose_eval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP summary plot of feature importances\n",
    "\n",
    "Rather than use a typical feature importance bar chart, we use a density scatter plot of SHAP values for each feature to identify how much impact each feature has on the model output for individuals in the validation dataset. Features are sorted by the **sum of the SHAP value magnitudes across all samples**. It is interesting to note that the relationship feature has more total model impact than the captial gain feature, but for those samples where capital gain matters it has more impact than age. In other words, capital gain effects a few predictions by a large amount, while age effects all predictions by a smaller amount.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the Tree SHAP implementation integrated into Light GBM to explain the entire dataset (32561 samples).\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize one prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we visualize a single prediction\n",
    "idx = 0  # Change the sample index to experiment\n",
    "X_display,y_display = shap.datasets.adult(display=True)\n",
    "shap.force_plot(base_value = explainer.expected_value[1], \n",
    "                shap_values = shap_values[1][idx,:], \n",
    "                features = X_display.iloc[idx,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize many predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[1], shap_values[1][:1000,:], X_display.iloc[:1000,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependence plots\n",
    "\n",
    "SHAP dependence plots show the effect of a single feature across the whole dataset. They plot a feature's value vs. the SHAP value of that feature across many samples. SHAP dependence plots are similar to partial dependence plots, but **account for the interaction effects present in the features**, and are only defined in regions of the input space supported by data. The vertical dispersion of SHAP values at a single feature value is driven by interaction effects, and another feature is chosen for coloring to highlight possible interactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot depence plots for the first X features (vertical dispersion measures the interaction effects)\n",
    "# Note: the interaction features is selected automatically as the interaction with the largest effect\n",
    "\n",
    "n_features = 10\n",
    "i = 0\n",
    "for name in X_train.columns:\n",
    "    shap.dependence_plot(name, shap_values[1], X, display_features = X_display)\n",
    "    i += 1\n",
    "    if i == n_features: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In summary**, SHAP is an easy-to-use package that uses the concept of Shapley values to unpack the conrtibutions of each feature to the prediction. \n",
    "\n",
    "**Advantages**: \n",
    "\n",
    "- Shapley values guarantee that predictions are fairly distributed among the features, i.e. the *Efficiency* property of Shapley values\n",
    "- Relatively intuitive interpretation: predictions as a coalition of feature values\n",
    "- Grounded in extensive theory with four core axioms - efficiency, symmetry, dummy, additivity \n",
    "\n",
    "**Disadvantages**: \n",
    "\n",
    "- Computing the Shapley values is computationally expensive, i.e. exponential in the number of features :-(\n",
    "- Can use unrealistic data instances when features are correlated (e.g. height and weight)\n",
    "- Shapley provides only a value for each feature, not a prediction model like \"if this feature was equal to X, then the prediction would be Y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
