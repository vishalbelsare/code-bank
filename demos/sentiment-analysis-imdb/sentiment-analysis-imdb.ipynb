{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sentiment Analysis**: NLP, Text Embedding, and RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:  [https://github.com/d-insight/code-bank.git](https://github.com/d-insight/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is a challenging subject in machine learning. People express their emotions in a way that can be very ambiguous for both humans and computers. In this demo, we analyze sentiments of a set of IMDB movie reviews. The dataset consists of review texts as well as a binary sentiment label (1: positive, 0: negative). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://barnraisersllc.com/wp-content/uploads/2017/01/Sentiment-Analysis.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "Image source: http://barnraisersllc.com/wp-content/uploads/2017/01/Sentiment-Analysis.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset source: *Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). \"Learning Word Vectors for Sentiment Analysis.\" The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 0**: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all import statements at the top of your notebook\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "\n",
    "# Data science packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection         import KFold, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble                import RandomForestClassifier\n",
    "from sklearn.pipeline                import Pipeline\n",
    "from sklearn.model_selection         import train_test_split\n",
    "\n",
    "# Neural networks\n",
    "from tensorflow.keras.models                 import Sequential, load_model\n",
    "from tensorflow.keras.layers                 import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text     import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# Text processing packages\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem   import SnowballStemmer\n",
    "import gensim\n",
    "from gensim.models import doc2vec, word2vec\n",
    "\n",
    "# Visualization packages\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants \n",
    "\n",
    "# Set a seed for replication\n",
    "SEED = 10\n",
    "\n",
    "# Set performance metric\n",
    "SCORE = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested cross validation helper function\n",
    "def nested_cv(X, y, est_pipe, p_grid, p_score, n_splits_inner = 3, n_splits_outer = 3, n_cores = 1, seed = 0):\n",
    "\n",
    "    # Cross-validation schema for inner and outer loops (stratified if it is a classification)\n",
    "    inner_cv = KFold(n_splits = n_splits_inner, shuffle = True, random_state = seed)\n",
    "    outer_cv = KFold(n_splits = n_splits_outer, shuffle = True, random_state = seed)\n",
    "    \n",
    "    # Grid search to tune hyper parameters\n",
    "    est = GridSearchCV(estimator = est_pipe, param_grid = p_grid, cv = inner_cv, scoring = p_score, n_jobs = n_cores)\n",
    "\n",
    "    # Nested CV with parameter optimization\n",
    "    nested_scores = cross_val_score(estimator = est, X = X, y = y, cv = outer_cv, scoring = p_score, n_jobs = n_cores)\n",
    "    \n",
    "    print('Average score: %0.4f (+/- %0.4f)' % (nested_scores.mean(), nested_scores.std() * 1.96))\n",
    "    \n",
    "    return nested_scores.mean(), nested_scores.std() * 1.96\n",
    "\n",
    "# Define a function to split a review into clean list of words\n",
    "def review_to_wordlist(review):\n",
    "    \n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "   \n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    \n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in words if not w in stops]\n",
    "    \n",
    "    stemmer = SnowballStemmer('english')\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    \n",
    "    return(words)\n",
    "\n",
    "# Define a function to split a review into parsed sentences, where each sentence is a word list\n",
    "def review_to_sentences(review, tokenizer):\n",
    "    \n",
    "    raw_sentences = tokenizer.tokenize(review.strip())  \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:      \n",
    "        if len(raw_sentence) > 0:           \n",
    "            sentences.append( review_to_wordlist( raw_sentence ))\n",
    "   \n",
    "    return sentences\n",
    "\n",
    "# Function to average all of the word vectors in a given paragraph\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,), dtype='float32')\n",
    "    nwords = 0.\n",
    "     \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "# Given a set of reviews (each one a list of words), calculate \n",
    "# the average feature vector for each one and return a 2D numpy array\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype='float32')\n",
    "     \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       \n",
    "       # Print a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print ('Review %d of %d' % (counter, len(reviews)))\n",
    "       \n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "       \n",
    "        # Increment the counter\n",
    "        counter = counter + 1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1**: Data Preprocessing and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('sentiment_data.tsv', header=0, delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample observation\n",
    "print ('id: \\t\\t', data['id'][0])\n",
    "print ('sentiment: \\t', data['sentiment'][0], '\\n')\n",
    "print (data['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target\n",
    "pd.DataFrame.hist(data,column='sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word distributions using the word cloud library\n",
    "\n",
    "# Concatenate all rows of review column\n",
    "text = data['review'].str.cat(sep=' ')\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=1600, height=800).generate(text)\n",
    "\n",
    "# Display the generated image\n",
    "wordcloud = WordCloud(max_font_size=60).generate(text)\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and drop corresponding rows, if any\n",
    "data.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML tags \n",
    "data_no_html = BeautifulSoup(data['review'][0])  \n",
    "\n",
    "print('Original:\\n{}\\n'.format(data['review'][0]))\n",
    "print('Without HTML tags:\\n{}\\n'.format(data_no_html.get_text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only alphabetical terms\n",
    "data_no_digits = re.sub('[^a-zA-Z]', ' ', data_no_html.get_text())  \n",
    "print('No digits:\\n{}'.format(data_no_digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case and separate into tokens\n",
    "lower_case = data_no_digits.lower()        \n",
    "words = lower_case.split(' ')\n",
    "print('Lower-cased and token-separated:\\n{}'.format(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the stop words dataset of NLTK library\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "print('Stop words in NLTK:\\n{}\\n'.format(stopwords.words('english')))\n",
    "words_no_stop = [w for w in words if not w in stopwords.words('english')]\n",
    "print('Without stop words:\\n{}'.format(words_no_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem words\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "print('grows  --> {}'.format(stemmer.stem('grows')))\n",
    "print('leaves --> {}'.format(stemmer.stem('leaves')))\n",
    "print('fairly --> {}\\n'.format(stemmer.stem('fairly')))\n",
    "\n",
    "words_stemmed = [stemmer.stem(w) for w in words_no_stop]\n",
    "print('Stemmed:\\n{}'.format(words_stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a raw review to a string of words: \n",
    "# the input is a single string (a raw movie review) and\n",
    "# the output is a single string (a preprocessed movie review)\n",
    "\n",
    "def review_to_words(raw_review):\n",
    "\n",
    "    # Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    \n",
    "    # Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    \n",
    "    # In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "     \n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # Stem words\n",
    "    stem_words = [stemmer.stem(w) for w in meaningful_words]\n",
    "    \n",
    "    # Join the words back into one string separated by space and return the result\n",
    "    return( \" \".join( stem_words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of text preprocessing\n",
    "\n",
    "clean_review = review_to_words(data['review'][0])\n",
    "\n",
    "print('Original:\\n{}\\n'.format(data['review'][0]))\n",
    "print('Cleaned:\\n{}'.format(clean_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and parse all movie reviews\n",
    "num_reviews = data['review'].size\n",
    "clean_data_reviews = []\n",
    "\n",
    "for i in range(0, num_reviews):\n",
    "    clean_data_reviews.append(review_to_words(data['review'][i]))\n",
    "    \n",
    "    # If the index is evenly divisible by 1000, print a message to show progress\n",
    "    if (i+1) % 1000 == 0:\n",
    "        print('Review {} of {}\\t{}%'.format(i+1, num_reviews, round((i+1)/num_reviews*100, 0)))\n",
    "    \n",
    "assert len(clean_data_reviews) == len(data), 'Error: the number of cleaned reviews does not match.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2**: Feature Extraction using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming pre-processed reviews to bag of words (BOW) feature representation\n",
    "vectorizer = TfidfVectorizer(max_features = 5000)\n",
    "features = vectorizer.fit_transform(clean_data_reviews)\n",
    "features = features.toarray()\n",
    "print (features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names (each term is a feature)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print (vocab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 10 vocabulary words and their corresponding TFIDF values\n",
    "\n",
    "dist = np.sum(features, axis=0)\n",
    "i = 0\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print(tag.ljust(15), count)\n",
    "    i = i + 1\n",
    "    if i > 10 : \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the values of feature 0\n",
    "features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3**: Classification using Random Forest with TFIDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target\n",
    "\n",
    "target = data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define pipeline\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_clf__random_state = SEED)\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"rf_clf__n_estimators\": [int(i) for i in np.linspace(10.0, 50.0, 5)]}\n",
    "\n",
    "acc_rf, std_rf = nested_cv(X = features, y = target, est_pipe = rf_pipe, p_grid = p_grid, p_score = SCORE, n_cores = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 4**: Word Vectors - Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular vectorization method for words is a technique known as Word2Vec, which is implemented in the `gensim` library. In Word2Vec each word is assigned a low-dimensional vector which is learnt under the assumption that words that are close to each other in a document are semantically related. Word2Vec can be used as a base for vectorize documents in low dimensions. You can read more about it here: https://cs224d.stanford.edu/lectures/CS224d-Lecture2.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the punkt tokenizer for sentence splitting\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all sentences from all reviews\n",
    "sentences = []\n",
    "\n",
    "for review in data['review']:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of sentences\n",
    "print (len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first sentence\n",
    "print (sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train word vectors\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features   = 300  # word vector dimensionality                      \n",
    "min_word_count = 40   # minimum word count                        \n",
    "num_workers    = 16   # number of threads to run in parallel\n",
    "context        = 10   # context window size                                                                                    \n",
    "\n",
    "# Initialize and train the model \n",
    "print ('Training model...')\n",
    "w2v_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "print('Done !')\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient\n",
    "w2v_model.init_sims(replace=True)\n",
    " \n",
    "# Save the model for later use. you can load it later using Word2Vec.load()\n",
    "# model_name = 'w2v_imdb'\n",
    "# model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the word which is less similar to other words in a set\n",
    "w2v_model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the words which are similar to a focal word\n",
    "\n",
    "w2v_model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.most_similar(\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corresponding word vector of an example word\n",
    "\n",
    "w2v_model['flower']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph vectors - Average Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the word vectors show good semantical properties, using them to get the same sort of properties from sentences is not straight forward. The simplest solution is to average word vectors of a document to come up with the same dimensional paragraph vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average feature vectors for review data, using the functions we defined above.\n",
    "clean_data_reviews = []\n",
    "for review in data['review']:\n",
    "    clean_data_reviews.append( review_to_wordlist( review ))\n",
    "\n",
    "w2v_features = getAvgFeatureVecs(clean_data_reviews, w2v_model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define pipeline\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_clf__random_state = SEED)\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"rf_clf__n_estimators\": [int(i) for i in np.linspace(10.0, 50.0, 5)]}\n",
    "\n",
    "acc_rfW2V, std_rfW2V = nested_cv(X = w2v_features, y = target, est_pipe = rf_pipe, p_grid = p_grid, p_score = SCORE, n_cores = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 5**: Document Vectors - Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution to obtain review vectors is to obtain them directly. Paragraph Vectors are treating each document as a word itself and obtains their vectors directly. You can read more about it here: https://cs.stanford.edu/~quocle/paragraph_vector.pdf . Again, the `gensim` library implements this method in the `doc2vec` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec needs each review to be tagged with some sort of ids\n",
    "# Here we tag each review with the 'id' field\n",
    "\n",
    "tagged_clean_data_reviews = []\n",
    "for uid, review in zip(data['id'], clean_data_reviews):\n",
    "    tagged_clean_data_reviews.append(gensim.models.doc2vec.TaggedDocument(words=review, tags=['%s' % uid[1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of tagging\n",
    "\n",
    "tagged_clean_data_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train paragraph vectors\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features   = 300  # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers    = 4    # Number of threads to run in parallel\n",
    "context        = 10   # Context window size                                                                                    \n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "print('Training model...')\n",
    "d2v_model = doc2vec.Doc2Vec(tagged_clean_data_reviews, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "print('Done !')\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "d2v_model.init_sims(replace=True)\n",
    " \n",
    "# Save the model for later use. You can load it later using Doc2Vec.load().\n",
    "# d2v_model_name = 'w2v_imdb'\n",
    "# d2v_model.save(d2v_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corresponding document vector of a review given its id\n",
    "d2v_model.docvecs['5814_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of D2V features of all reviews to be used in down-stream predictions \n",
    "d2v_features = d2v_model.docvecs\n",
    "d2v_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the similarity of two reviews (based on cosine similarity) \n",
    "d2v_model.docvecs.similarity(d1='7759_3', d2='5814_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define pipeline\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_clf__random_state = SEED)\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"rf_clf__n_estimators\": [int(i) for i in np.linspace(10.0, 50.0, 5)]}\n",
    "\n",
    "acc_rfD2V, std_rfD2V = nested_cv(X = d2v_features, y = target, est_pipe = rf_pipe, p_grid = p_grid, p_score = SCORE, n_cores = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 6**: Classification using a type of Recurrent Neural Network (RNN), the LSTM\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks are a particular type of RNN optimized for learning long-term dependencies in sequential data such as text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize features into same size vectors, by assigning a numerical id to each term\n",
    "\n",
    "max_features = 5000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(clean_data_reviews)\n",
    "X = tokenizer.texts_to_sequences(clean_data_reviews)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "# One-hot encoding of target\n",
    "Y = pd.get_dummies(data['sentiment']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X shape: {}'.format(X.shape))\n",
    "print('Y shape: {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build recurrent neural network model. Architecture is taken from\n",
    "# https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras\n",
    "\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "def rnn_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_dim, input_length = X.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.4))\n",
    "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    \n",
    "    return(model)\n",
    "\n",
    "model = rnn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model,show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = SEED, stratify = Y)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train the model (needs approx. 1 hour)\n",
    "batch_size = 100\n",
    "epochs     = 5 \n",
    "TRAIN      = False\n",
    "\n",
    "if TRAIN:\n",
    "    model.fit(X_train, Y_train, epochs = epochs, batch_size = batch_size)\n",
    "    model.save('rnn.h5')\n",
    "else:\n",
    "    model = load_model('rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance in terms of loss and accuracy\n",
    "score, acc_RNN = model.evaluate(X_test, Y_test, batch_size = batch_size)\n",
    "print(\"Loss: %.4f\" % (score))\n",
    "print(\"Accuracy: %.4f\" % (acc_RNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(clean_data_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess sentiment of a new review\n",
    "rvw = ['a good movie but not excellent.']\n",
    "\n",
    "# Vectorizing the review\n",
    "rvw = tokenizer.texts_to_sequences(rvw)\n",
    "\n",
    "# Padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "rvw = pad_sequences(rvw, maxlen=1105, dtype='int32', value=0)\n",
    "\n",
    "# Predict sentiment of the review\n",
    "sentiment = model.predict(rvw, batch_size=1, verbose = 2)[0]\n",
    "\n",
    "print()\n",
    "print('NEGATIVE - POSITIVE')\n",
    "print(sentiment)\n",
    "print()\n",
    "\n",
    "if (np.argmax(sentiment) == 0):\n",
    "    print(\"negative\")\n",
    "    \n",
    "elif (np.argmax(sentiment) == 1):\n",
    "    print(\"positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SUMMARY OF ACCURACY SCORES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width       = 50\n",
    "models      = ['Baseline', 'Random Forest', 'Random Forest + word2vec', 'Random Forest + doc2vec', 'RNN']\n",
    "result_acc  = [0.5, acc_rf, acc_rfW2V, acc_rfD2V, acc_RNN]\n",
    "result_std  = [0,   std_rf, std_rfW2V, std_rfD2V, np.nan]\n",
    "\n",
    "print('', '=' * width, '\\n', 'Summary of Accuracy Scores'.center(width), '\\n', '=' * width)  \n",
    "for i in range(len(models)):\n",
    "    print(models[i].center(width-18), '{0:.4f}'.format(result_acc[i]), '+/-{0:.4f}'.format(result_std[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 6**: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Considering TFIDF representation, why cosine distance can be a better measure for similarity of documents compared to euclidean distance?\n",
    "- Despite good performance of W2V vectors to detect similarity and relationship between two words, why similar methods are not as powerful when it comes to detect similarity and relationships between two documents? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
