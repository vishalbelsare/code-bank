{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Credit Default** - Advanced Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:  [https://github.com/d-insight/code-bank.git](https://github.com/d-insight/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with the credit default problem we already know, we now implement more advanced models. Both the dummy and logit model achieve approximately 0.5 in AUC. Our goal is to surpass this baseline. We skip exploratory data analysis and compare the following models: logit with lasso regularization, K-nearest neighbors (KNN), decision tree, random forest, gradient boosted trees and (a very simple) ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://greendayonline.com/wp-content/uploads/2017/03/Recovering-From-Student-Loan-Default.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "\n",
    "Image: https://greendayonline.com/wp-content/uploads/2017/03/Recovering-From-Student-Loan-Default.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Credit Card Default Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to predict the probability of defaulting on a credit card account at a Taiwanese bank. A credit card default happens when a customer fails to pay the minimum due on a credit card bill for more than 6 months. \n",
    "\n",
    "We will use a dataset from a Taiwanese bank with 30,000 observations (Source: *Yeh, I. C., & Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2), 2473-2480.*). Each observation represents an account at the bank at the end of October 2005.  We renamed the variable default_payment_next_month to customer_default. The target variable to predict is `customer_default` -- i.e., whether the customer will default in the following month (1 = Yes or 0 = No). The dataset also includes 23 other explanatory features. \n",
    "\n",
    "Variables are defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature name     | Variable Type | Description \n",
    "|------------------|---------------|--------------------------------------------------------\n",
    "| customer_default | Binary        | 1 = default in following month; 0 = no default \n",
    "| LIMIT_BAL        | Continuous    | Credit limit   \n",
    "| SEX              | Categorical   | 1 = male; 2 = female\n",
    "| EDUCATION        | Categorical   | 1 = graduate school; 2 = university; 3 = high school; 4 = others\n",
    "| MARRIAGE         | Categorical   | 0 = unknown; 1 = married; 2 = single; 3 = others\n",
    "| AGE              | Continuous    | Age in years  \n",
    "| PAY1             | Categorical   | Repayment status in September, 2005 \n",
    "| PAY2             | Categorical   | Repayment status in August, 2005 \n",
    "| PAY3             | Categorical   | Repayment status in July, 2005 \n",
    "| PAY4             | Categorical   | Repayment status in June, 2005 \n",
    "| PAY5             | Categorical   | Repayment status in May, 2005 \n",
    "| PAY6             | Categorical   | Repayment status in April, 2005 \n",
    "| BILL_AMT1        | Continuous    | Balance in September, 2005  \n",
    "| BILL_AMT2        | Continuous    | Balance in August, 2005  \n",
    "| BILL_AMT3        | Continuous    | Balance in July, 2005  \n",
    "| BILL_AMT4        | Continuous    | Balance in June, 2005 \n",
    "| BILL_AMT5        | Continuous    | Balance in May, 2005  \n",
    "| BILL_AMT6        | Continuous    | Balance in April, 2005  \n",
    "| PAY_AMT1         | Continuous    | Amount paid in September, 2005\n",
    "| PAY_AMT2         | Continuous    | Amount paid in August, 2005\n",
    "| PAY_AMT3         | Continuous    | Amount paid in July, 2005\n",
    "| PAY_AMT4         | Continuous    | Amount paid in June, 2005\n",
    "| PAY_AMT5         | Continuous    | Amount paid in May, 2005\n",
    "| PAY_AMT6         | Continuous    | Amount paid in April, 2005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measurement scale for repayment status is:   \n",
    "\n",
    "    -2 = payment two months in advance   \n",
    "    -1 = payment one month in advance   \n",
    "    0 = pay duly   \n",
    "    1 = payment delay for one month   \n",
    "    2 = payment delay for two months   \n",
    "    3 = payment delay for three months   \n",
    "    4 = payment delay for four months   \n",
    "    5 = payment delay for five months   \n",
    "    6 = payment delay for six months   \n",
    "    7 = payment delay for seven months   \n",
    "    8 = payment delay for eight months   \n",
    "    9 = payment delay for nine months or more  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 0**: Setup\n",
    "\n",
    "Put all import statements, constants and helper functions at the top of your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "It is a good idea to put all of your import statements up at the top of a project in one location. You then can quickly see all of the requirements needed to run the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import itertools\n",
    "import pandas_profiling\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline  \n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.dummy        import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors    import KNeighborsClassifier\n",
    "from sklearn.tree         import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble     import RandomForestClassifier\n",
    "from sklearn.ensemble     import GradientBoostingClassifier\n",
    "\n",
    "# Supporting functions from scikit-learn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ignore some warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "\n",
    "It is a good idea to move as many of the constant values used in your project as possible, up to the top and to give then variable names ALL_IN_CAPS so you can quickly identify constant values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for replication\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also define a few \"helper functions\" to automate repetitive tasks that we will perform below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes=[0,1], normalize=False, title='Confusion Matrix', cmap=plt.cm.Reds):\n",
    "    \"\"\" \n",
    "    Function to plot a sklearn confusion matrix, showing number of cases per prediction condition \n",
    "    \n",
    "    Args:\n",
    "        cm         an sklearn confusion matrix\n",
    "        classes    levels of the class being predicted; default to binary outcome\n",
    "        normalize  apply normalization by setting `normalize=True`\n",
    "        title      title for the plot\n",
    "        cmap       color map\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, aspect='auto', interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.locator_params(nbins=2)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    thresh = cm.max() / 2.\n",
    "    # add FP, TP, FN, TN counts\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round (cm[i, j],2), horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(fpr, tpr, title='ROC Curve', note=''):\n",
    "    \"\"\"\n",
    "    Function to plot an ROC curve in a consistent way.\n",
    "    \n",
    "    Args:\n",
    "        fpr        False Positive Rate (list of multiple points)\n",
    "        tpr        True Positive Rate (list of multiple points)\n",
    "        title      Title above the plot\n",
    "        note       Note to display in the bottom-right of the plot\n",
    "    \"\"\"\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title(title)\n",
    "    if note: plt.text(0.6, 0.2, note)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature_importance(tree_model, feature_names):\n",
    "    \"\"\"\n",
    "    Function to print a list of features from an sklearn tree model (ranked by importance of the feature)\n",
    "    \n",
    "    Args:\n",
    "        tree_model       A sklearn DecisionTreeClassifier()\n",
    "        feature_names    A list of features used by the DecisionTreeClassifier\n",
    "    \"\"\"\n",
    "    print('Feature'.center(12), '   ',  'Importance')\n",
    "    print('=' * 30)\n",
    "    for index in reversed(np.argsort(tree_model.feature_importances_)):\n",
    "        print(str(feature_names[index]).center(12) , '   ', '{0:.4f}'.format(tree_model.feature_importances_[index]).center(8)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1**: Load, Preprocess and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = pd.read_csv('credit_data.csv')\n",
    "\n",
    "# Move target variable to first column (not necessary, but easier to see)\n",
    "data = data.set_index('customer_default').reset_index() \n",
    "\n",
    "# One-hot-encode SEX and MARRIAGE  \n",
    "data = pd.get_dummies(data=data, columns=['SEX', 'MARRIAGE'])\n",
    "\n",
    "# Remove 'id'\n",
    "data = data.drop(columns=['ID'])\n",
    "\n",
    "# Select target\n",
    "y = np.array(data['customer_default'])\n",
    "\n",
    "# Select features \n",
    "features = list(set(list(data.columns)) - set(['customer_default']))\n",
    "X = data.loc[:, features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data: Training, Validation, & Testing\n",
    "\n",
    "In all of the code that follows, we will now divide the data into three parts:  **training** (60%), **validation** (20%) and **test** (20%). In the python code, we refer to these subsets as: \n",
    "\n",
    "| Subset      |  Pct.  |  X code var     | Target code var |\n",
    "|-------------|--------|-----------------|-----------------|\n",
    "| training    |  60%   |  X_train_train  | y_train_train\n",
    "| validation  |  20%   | X_train_val     | y_train_val\n",
    "| testing     |  20%   | X_test          | y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(X_train, y_train, test_size=0.25, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2**: Logit Model with L1 Regularization (\"Lasso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline (not striclty necessary as pipeline only contains one step)\n",
    "estimators = []\n",
    "estimators.append(('logit_model_l1', LogisticRegression()))  # tell it to use a logit model\n",
    "pipeline = Pipeline(estimators) \n",
    "pipeline.set_params(logit_model_l1__penalty='l1')            # tell it to regularize with L1 norm\n",
    "pipeline.set_params(logit_model_l1__solver='liblinear')      # tell it to use the liblinear solver (see)\n",
    "\n",
    "# Tune C  \n",
    "results = []\n",
    "for c in np.logspace(-4, 5, 10):\n",
    "    pipeline.set_params(logit_model_l1__C=c) \n",
    "    pipeline.fit(X_train_train,y_train_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_train_val)       # use validation set during hyper-parameter tuning\n",
    "    auc_lml1 = roc_auc_score(y_train_val, y_train_pred[:,1])   \n",
    "    results.append( (auc_lml1, c)  )\n",
    "logit_model_l1 = pipeline.named_steps['logit_model_l1']      # capture model so we can use it later\n",
    "\n",
    "# View results \n",
    "print('C'.center(12), '   ', 'AUC'.center(8), '\\n', '=' * 25)\n",
    "for (auc, c) in results:\n",
    "    print('{0:.4f}'.format(c).rjust(12), '   ',  '{0:.4f}'.format(auc).center(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best C       NOTE: The \"best\" C does not have to have the highest AUC\n",
    "best_C = 0.100        # perhaps select a lower C with a slightly lower AUC so Lasso will find a simpler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test final model \n",
    "pipeline.set_params(logit_model_l1__C=best_C)\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_prob_logit_lasso = pipeline.predict_proba(X_test)\n",
    "fpr_logit_lasso, tpr_logit_lasso, _ = roc_curve(y_test, y_prob_logit_lasso[:, 1])\n",
    "best_auc_logit_lasso = roc_auc_score(y_test, y_prob_logit_lasso[:,1])\n",
    "print('L1 Regularized Logit Model: Final test score for C = {0:.4f} has AUC = {1:.4f}'.format(best_C, best_auc_logit_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(fpr_logit_lasso, tpr_logit_lasso, 'ROC Curve for L1 Regularized Logit Model', 'AUC = %2.4f' % best_auc_logit_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare coefficients between a standard logit model and a regularized logit model, to see how Lasso drops predictors\n",
    "logit_model = Pipeline([('s', StandardScaler()), ('m', LogisticRegression())]).fit(X_train_train, y_train_train).named_steps['m']\n",
    "print('REGULARIZATION'.center(20), 'NONE'.center(10), 'L1'.center(10))\n",
    "print('=' * 50)\n",
    "for (varname, lm_coef, lml1_coef) in zip(features, logit_model.coef_[0], logit_model_l1.coef_[0]):\n",
    "    lm_coeff  = \"{0:.4f}\".format(lm_coef).rjust(10)\n",
    "    lml1_coef = \"{0:.4f}\".format(lml1_coef).rjust(10) if abs(lml1_coef) > 0.0001 else \"\"\n",
    "    print(str(varname).center(20), lm_coeff, lml1_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3**: K-Nearest Neighbors (KNN)\n",
    "\n",
    "Do we need to standardize the data? Yes! Standardization is required as the KNN algorithm measures the distances between pairs of samples and these measurements depend on the measurement units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('knn_model', KNeighborsClassifier()))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Tune K\n",
    "results = []\n",
    "for k in range(5, 100, 10):\n",
    "    pipeline.set_params(knn_model__n_neighbors = k) \n",
    "    pipeline.fit(X_train_train ,y_train_train)\n",
    "    y_prob = pipeline.predict_proba(X_train_val)\n",
    "    auc = roc_auc_score(y_train_val, y_prob[:,1])\n",
    "    results.append( (auc, k) )\n",
    "    \n",
    "# View results \n",
    "print('K'.rjust(5), '   ', 'AUC'.center(8), '\\n', '=' * 20)\n",
    "for (auc, k) in results:\n",
    "    print('{0}'.format(k).rjust(5), '   ',  '{0:.4f}'.format(auc).center(8))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best K\n",
    "best_K = sorted(results)[-1][1]\n",
    "auc = sorted(results)[-1][0]\n",
    "print('Best value of K = %d, with AUC = %2.4f' % (best_K, auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test final model \n",
    "pipeline.set_params(knn_model__n_neighbors = best_K)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_prob_knn = pipeline.predict_proba(X_test)\n",
    "fpr_knn, tpr_knn, _ = roc_curve(y_test, y_prob_knn[:, 1])\n",
    "best_auc_knn = roc_auc_score(y_test, y_prob_knn[:,1])\n",
    "print('KNN Model: Final test score for K = %d has AUC = %2.4f' % (best_K, best_auc_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plot_roc(fpr_knn, tpr_knn, 'ROC Curve for KNN Model', 'K = %d \\n\\nAUC = %2.4f' % (best_K, best_auc_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 4**: Decision Trees\n",
    "\n",
    "Next, we turn to a model from information theory called **Decision Trees**. Unlike a **Logit Model** or a **KNN Classifier**, a decision tree is not sensitive to the scaling of categorical or numerical features -- a decision tree can find a cutting point along any arbitrary numeric or categorical feature. The implementation of decision trees in sklearn, however, expects that categorical are \"one-hot-encoded\" into separate variables (which is convenient for the implementation, but not technically required by the algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Decision Tree \n",
    "\n",
    "The basic decision tree model starts with just one tree. We will see that just one tree is likely to be unreliable, but we will fit one and show that next for illustration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model      \n",
    "tree_model = DecisionTreeClassifier(random_state=SEED)  # just one tree, so no pipeline needed\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Test model\n",
    "y_prob_tree = tree_model.predict_proba(X_test)\n",
    "fpr_tree, tpr_tree, _ = roc_curve(y_test, y_prob_tree[:, 1])\n",
    "auc_one_tree = roc_auc_score(y_test, y_prob_tree[:,1])\n",
    "print('One Decision Tree Model: Final test score has AUC = {0:0.4f}'.format(auc_one_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(fpr_tree, tpr_tree, 'ROC Curve for One Decision Tree', 'AUC = %2.4f' %  auc_one_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print feature importance\n",
    "print_feature_importance(tree_model, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tree    \n",
    "#     this may not work on all computers - requires graphviz to be installed\n",
    "#     install graphviz on a Mac computer by running:  brew install -v graphviz\n",
    "#     install graphviz on Linux computer by running:  sudo apt-get install graphviz\n",
    "Source(export_graphviz(tree_model, out_file=None, feature_names=features, max_depth=3))\n",
    "# !dot -Tpng tree.dot -o tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 5**: A Random Forest of Decision Trees\n",
    "\n",
    "Just one tree may be an arbitrary solution to the prediction problem. So next, we run a random forest of different trees and features, and average across them for a more reliable prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "estimators = []\n",
    "estimators.append(('forest_model', RandomForestClassifier()))\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.set_params(forest_model__random_state = SEED)\n",
    "    \n",
    "# Tune N   \n",
    "results = []\n",
    "for n in [10, 50, 150, 200, 250]:\n",
    "    pipeline.set_params(forest_model__n_estimators = n) \n",
    "    pipeline.fit(X_train_train, y_train_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_train_val)\n",
    "    auc = roc_auc_score(y_train_val, y_train_pred[:,1])\n",
    "    results.append( (auc, n))\n",
    "\n",
    "# View results \n",
    "print('N'.rjust(5), '   ', 'AUC'.center(8), '\\n', '=' * 20)\n",
    "for (auc, n) in results:\n",
    "    print('{0}'.format(n).rjust(5), '   ',  '{0:.4f}'.format(auc).center(8))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test final model\n",
    "best_N = 150   # AUC will generally improve with more interations, but eventually level off\n",
    "pipeline.set_params(forest_model__n_estimators=best_N)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_prob_forest = pipeline.predict_proba(X_test)\n",
    "fpr_forest, tpr_forest, _ = roc_curve(y_test, y_prob_forest[:, 1])\n",
    "best_auc_forest = roc_auc_score(y_test, y_prob_forest[:,1])\n",
    "print('Random Forest Model: Final test score for N = %d has AUC = %2.4f' % (best_N, best_auc_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(fpr_forest, tpr_forest, 'ROC Curve for a Random Forest', 'N = %d \\n\\nAUC = %2.4f' %  (best_N, best_auc_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print feature importance\n",
    "forest_model = pipeline.named_steps['forest_model']\n",
    "print_feature_importance(forest_model, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 6**: Gradient Boosted Trees \n",
    "\n",
    "Just one tree may be arbitrary and an unreliable model. So next we run many trees by gradient boosting through a sequence of such trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a pipeline\n",
    "estimators = []\n",
    "estimators.append(('g_boosted_trees', GradientBoostingClassifier()))\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.set_params(g_boosted_trees__random_state = SEED)\n",
    "    \n",
    "# Tune N using validation set \n",
    "results = []\n",
    "for n in [10, 50, 150, 200, 250]:\n",
    "    pipeline.set_params(g_boosted_trees__n_estimators = n) \n",
    "    pipeline.fit(X_train_train, y_train_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_train_val)\n",
    "    auc = roc_auc_score(y_train_val, y_train_pred[:,1])\n",
    "    results.append( (auc, n))\n",
    "\n",
    "# View results \n",
    "print('N'.rjust(5), '   ', 'AUC'.center(8), '\\n', '=' * 20)\n",
    "for (auc, n) in results:\n",
    "    print('{0}'.format(n).rjust(5), '   ',  '{0:.4f}'.format(auc).center(8))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test final model\n",
    "best_N = 150   # AUC will generally improve with more interations, but eventually level off\n",
    "pipeline.set_params(g_boosted_trees__n_estimators = best_N)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_prob_boosted = pipeline.predict_proba(X_test)\n",
    "fpr_boosted, tpr_boosted, _ = roc_curve(y_test, y_prob_boosted[:, 1])\n",
    "best_auc_boosted = roc_auc_score(y_test, y_prob_boosted[:,1])\n",
    "print('Random Forest Model: Final test score for N = %d has AUC = %2.4f' % (best_N, best_auc_boosted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(fpr_boosted, tpr_boosted, 'ROC Curve for Boosted Trees', 'N = %d \\nAUC = %2.4f' % (best_N, best_auc_boosted) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 7**: A (Very Simple) Ensemble Model  \n",
    "\n",
    "We now use a very simple approach to ensemble together the various models we have tried so far -- we will simply average together prdictions from:   \n",
    "  \n",
    "  * Logit + Lasso\n",
    "  * K-Nearest Neighbors\n",
    "  * Random forest\n",
    "  * Gradient boosted trees\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average predictions across 4 models\n",
    "y_prob_ensemble = np.mean( np.array([y_prob_logit_lasso[:,1], y_prob_knn[:,1], y_prob_forest[:,1], y_prob_boosted[:,1]]), axis=0)\n",
    "fpr_ensemble, tpr_ensemble, _ = roc_curve(y_test, y_prob_ensemble)\n",
    "ensembled_auc = roc_auc_score(y_test, y_prob_ensemble)\n",
    "print('Simple Ensemble: Final test score has AUC = {0:0.4f}'.format(ensembled_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC  \n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_ensemble, tpr_ensemble)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC Curve for Average Ensemble')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SUMMARY OF ROC CURVES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(fpr_logit_lasso, tpr_logit_lasso, 'ROC Curve for L1 Regularized Logit Model', 'AUC = %2.4f' % best_auc_logit_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(fpr_knn, tpr_knn, 'ROC Curve for KNN Model', 'K = %d \\n\\nAUC = %2.4f' % (best_K, best_auc_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(fpr_forest, tpr_forest, 'ROC Curve for a Random Forest', 'N = %d \\n\\nAUC = %2.4f' %  (best_N, best_auc_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(fpr_boosted, tpr_boosted, 'ROC Curve for Boosted Trees', 'N = %d \\nAUC = %2.4f' % (best_N, best_auc_boosted) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SUMMARY OF AUC VALUES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of AUC scores \n",
    "width     = 30\n",
    "width_box = 100\n",
    "models    = ['Baseline', 'Logit + Lasso', 'KNN', 'Random Forest', 'Gradient Boosting', 'Averaged Ensemble']\n",
    "results   = [0.5000, best_auc_logit_lasso, best_auc_knn, best_auc_forest, best_auc_boosted, ensembled_auc]\n",
    "\n",
    "print(str('=' * width).center(width_box))\n",
    "print('Summary of AUC Scores'.center(width_box))\n",
    "print(str('=' * width).center(width_box))\n",
    "for i in range(len(models)):\n",
    "    line = models[i].center(width - 8) + '{0:.4f}'.format(results[i])\n",
    "    print(line.center(width_box))\n",
    "print()\n",
    "\n",
    "# Plot ROC\n",
    "fig, ax1 = plt.subplots(1, 1)\n",
    "fig.set_size_inches(12, 12)\n",
    "ax1.xaxis.grid(True, linestyle='--', which='major', color='grey', alpha=.25)\n",
    "ax1.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=.25)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label = 'Baseline (AUC 0.5)')\n",
    "plt.plot(fpr_logit_lasso, tpr_logit_lasso, label = 'Logit + Lasso (AUC {})'.format(round(best_auc_logit_lasso, 4)))\n",
    "plt.plot(fpr_knn,         tpr_knn,         label = 'KNN (AUC {})'.format(round(best_auc_knn, 4)))\n",
    "plt.plot(fpr_forest,      tpr_forest,      label = 'Random Forest (AUC {})'.format(round(best_auc_forest, 4)))\n",
    "plt.plot(fpr_boosted,     tpr_boosted,     label = 'Gradient Boosting (AUC {})'.format(round(best_auc_boosted, 4)))\n",
    "plt.plot(fpr_ensemble,    tpr_ensemble,    label = 'Averaged Ensemble (AUC {})'.format(round(ensembled_auc, 4)) , linewidth=2)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.legend()\n",
    "ax1.legend=True\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
