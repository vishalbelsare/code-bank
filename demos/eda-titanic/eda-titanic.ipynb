{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploratory Data Analysis**: The Titanic Dataset (Extended version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:  [https://github.com/d-insight/code-bank.git](https://github.com/d-insight/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will explore the likelihood of surviving the sinking of the RMS Titanic. The Titanic hit a iceberg in 1912 and quickly sank, killing 1,502 out of the 2,224 passengers and crew on board. One most important reason so many people died was that there were not enough lifeboats to serve everyone. Accordingly, it has also been frequently noted that the most **_likely_** people to survive the disaster were women, children, and members of the upper-class. Let's see if that is true.\n",
    "\n",
    "The Titanic case is a classic problem in data science, and it is still an ongoing [Kaggle competition](https://www.kaggle.com/c/titanic). There are many other examples of the Titanic dataset in introductory statistics and Data Science courses, so we also encourage you to look around and see how others have approached the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/95/Titanic_sinking%2C_painting_by_Willy_St%C3%B6wer.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "Image source: https://upload.wikimedia.org/wikipedia/commons/9/95/Titanic_sinking%2C_painting_by_Willy_St%C3%B6wer.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will conduct our EDA and visualization analysis in three parts:\n",
    "\n",
    "  1. Analyze and visualize base-rates \n",
    "  2. Calculate new predictors that can help the analysis ('Feature Engineering)\n",
    "  3. Visualize advanced data characteristics\n",
    "  \n",
    "To prepare, let's first review the data tools, visualization tools, and actual data for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three basic options for loading and working with data in Python:\n",
    "\n",
    "  * Pure `Python 3.x`\n",
    "  \n",
    "    In this approach, you load data directly into \"pure\" Python data objects, such as lists, sets, and dictionaries (or nested hiearchies of such objects, such as lists-within-lists, lists-within-dicts, dicts-within-dicts, and so on). Although operation on \"pure Python\" objects can be slow, it also is extremely flexible.\n",
    "    \n",
    "  * `NumPy`\n",
    "  \n",
    "    The basic data structures for holding arrays, vectors, and matrices of data are provided by a core package called `NumPy`. NumPy also has a set of linear algebra and numerical fuinctions, but in general such functions are now provided by another package called `scipy` (for scientific computing) and numerical computation is usually done there. `NumPy` has been optimized to run with primitive routines written in `C` (or even `fortran`) and so is orders-of-magnitude faster-running than doing calculations with pure Python. Nevertheless, the data access, subscripting, and slicing of elements for `NumPy` still conforms to the same syntax as pure Python.\n",
    "    \n",
    "  * `pandas`\n",
    "  \n",
    "    Most applied Data Science projects (that fit into memory/RAM), now use an \"Excel-like\" package called `pandas`. Pandas stores data in objects called **dataframes**. Dataframes will become the central type of data object for your work in Data Science (much as Excel Spreadsheets often were for Business Analysts). Dataframes provide many different properties and methods that help you to work with your data more effectively. We will use `pandas` for most of the examples and problems in the class.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to visualize data and results in Python. Sometimes that is a good thing - and sometimes it is a bad thing, for there are many ways to do it. Eventually you will want to learn multiple methods, as data scientists often use many different libraries. The following are the most common libraries, with links to their documentation:\n",
    "\n",
    "  * `pandas` https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html\n",
    "  \n",
    "    If you are in a hurry, it also is possible to generate many simple plots directly from the `pandas` library and `dataframe` object. This is a good option when you are moving fast and all you need to so is see a simple histogram or trend line and you already have your data in a pandas dataframe.  \n",
    "  \n",
    "  * `seaborn` https://seaborn.pydata.org/\n",
    "  \n",
    "    Seaborn is a simplified and better looking interface that sits on top of the standard `matplotlib` library. Seaborn is often used because it looks great, but also gives you all the ability to go into `matplotlib` to customize graphs for a particular need.\n",
    "  \n",
    "  * `plotly` and `plotly_express` https://www.plotly.express/   \n",
    "  \n",
    "    The commercial package `plotly` is a comprehensive toolkit for building interactive, D3 and WebGL charts. Interactive graphs are particularly good for online (web-based) dashboards. As you can see in the plot of Python visualization options below, `plotly` is quickly rising in popularity. To use plotly, however, you need to sign up for a [plotly account](https://plot.ly/python/) and you need an active internet connection. We won't need all of the features in plotly, however, and so we will develop examples using just `plotly_express`. Plotly express is a free, local-to-your-machine, and easier-to-use, API to the plotly service. [See here for documentation](https://www.plotly.express/plotly_express/) for the plotly express API.  \n",
    "    \n",
    "  * `matplotlib` and `pyplot` https://matplotlib.org/  \n",
    "  \n",
    "    Matplotlib is the core package for graphing in Python, and many other packages build on top of it (i.e., pyplot, pandas, and seaborn). The `matplotlib` object model can be somewhat confusing, which often means writing many lines of code and hours of debugging. To help, matplotlib also comes with an interface library called `pyplot` ([documentation here](https://matplotlib.org/api/pyplot_api.html)) that mimics the MatLab approach to graphing (helpful for many engineers). In general, however, `pyplot` has now been supplanted by the other options above. Although it is faster to get started with plotting by using one of the other options above, eventually you will find that you often need to return to matplotlib in order to \"tweak\" a layout or to work with more complicated graphs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"viz-options.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "\n",
    "\n",
    "Image source: EPFL TIS Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Titanic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is taken from the [Kaggle Titanic Competition](https://www.kaggle.com/c/titanic). It is split between a \"training\" dataset (where you know the actual outcome) and a \"testing\" dataset (where you do not know the outcome). If we were actually competing in the Kaggle competition, then we would be trying to predict the unknown testing cases and submitting our predictions to Kaggle to see if we could win. But in this case, the objective is simply to get you started with Python and to familiarize you with the basic data structures and graphing libraries of the Data Science stack. We therefore will ignore the testing dataset and work only with the training data.\n",
    "\n",
    "All of the data that you will need for this demo is in the `titanic.csv` file, located within the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't know very much about the 891 passengers in the training dataset. The following features are available.\n",
    "\n",
    "  Feature name | Description  |\n",
    "  -------- | -------------- |\n",
    "  Survived | Target variable, i.e. survival, where 0 = No, 1 = Yes |\n",
    "  PassengerId | Id of the passenger |\n",
    "  Pclass   | Ticket class, wher 1 = 1st, 2 = 2nd, 3 = 3rd |\n",
    "  Name     | Passenger name |\n",
    "  Sex      | Sex |\n",
    "  Age      | Age in years |\n",
    "  SibSp    | Num of siblings or spouses aboard the Titanic |\n",
    "  Parch    | Num of parents or children aboard the Titanic |\n",
    "  Ticket   | Ticket number, i.e. record ID |\n",
    "  Fare     | Passenger fare |\n",
    "  Cabin    | Cabin number |\n",
    "  Embarked | Port of Embarkation, where C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Special Notes**\n",
    "\n",
    "  * **Pclass**: A proxy for socio-economic status (SES): 1st = Upper class; 2nd = Middle class; 3rd = Lower class.  \n",
    "  * **Age**: Age is fractional if less than 1.  If the age is estimated, is it in the form of xx.5  \n",
    "  * **SibSp**: The dataset defines family relations as: Sibling = brother, sister, stepbrother, stepsister; Spouse = husband, wife (mistresses and fianc√©s were ignored)  \n",
    "  * **Parch**: The dataset defines family relations as: Parent = mother, father; Child = daughter, son, stepdaughter, stepson; Some children travelled only with a nanny, therefore parch=0 for them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 0**: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all import statements at the top of your notebook -- import some basic and important ones here\n",
    "\n",
    "# Standard imports\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Visualization packages \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn           as sns\n",
    "from plotly.offline      import init_notebook_mode\n",
    "init_notebook_mode(connected=True)  \n",
    "\n",
    "# Special code to ignore un-important warnings \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1**: Analyze Base Rates and EDA\n",
    "\n",
    "The most basic \"model\" to run for any problem is the \"average\" (or \"base rate\") outcome. Before we work on more complicated models, it is a good idea to understand how a very simple heuristic will perform -- and then you can determine how much a more complicated model really improves the predictions. Your first model may be too simple and therefore highly \"biased\" (i.e., systematically low or systematically high for different groupings or levels within the data); but your simple model should also not vary much if/when we pull a new sample from the same underlying population/data generating function.  \n",
    "\n",
    "We will therefore conduct some **_exploratory data analysis_** (\"**EDA**\") and **_visualization_** in this demo to understand the distribution of the outcome for this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open dataset with Pandas\n",
    "\n",
    "data = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Count rows and coumns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1a**: EDA - the automated approach\n",
    "\n",
    "Instead of performing all of the steps above manually, you can also run a \"profile\" on the dataset first, and then drill down into specific cases of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the automated pandas profiling utility to examine the dataset\n",
    "data.profile_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1b**: EDA - the manual approach\n",
    "\n",
    "It is generally a good idea to go beyond the automated approach to EDA. Here are some useful steps for understanding and plotting the data in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data (variable names, count non-missing values, review variables types)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the \"head\" of the dataset\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the \"tail\" of the dataset\n",
    "\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics (mean, min, max, std, etc.) for all variables and transpose the output\n",
    "\n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing data per feature\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing data  (Hint: use a seaborn heatmap to see distribution of isnull() for the dataframe\n",
    "\n",
    "sns.heatmap(data.isnull(), cbar=False, cmap=\"YlGnBu_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: selecting columns in pandas\n",
    "\n",
    "As we will select columns from a pandas dataframe many times, it is important to note that there are generally two equivalent ways of doing this. We will use the first approach of passing the column name as a string in square brackets. This distinguishes column selection from method calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First approach (recommended): pass the column name as a string in square brackets\n",
    "\n",
    "data['Survived'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second approach: pass the column name as a method call\n",
    "\n",
    "data.Survived.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Survival (the basic outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of Survivals and Deaths\n",
    "\n",
    "survivals = sum(data['Survived'])\n",
    "deaths = len(data[data['Survived'] == False])\n",
    "assert survivals + deaths == len(data)     # not necessary, but this should be true\n",
    "assert survivals + deaths == data.shape[0] # not necessary, but this also should be true\n",
    "print('Survivals: ', survivals)\n",
    "print('Deaths: ', deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The base-rate likelihood of survival: ', survivals/(survivals+deaths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the target feature \"Survived\": 4 approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of \"Survived\" using Pandas \n",
    "# The fastest way - this just uses the pandas dataframe  \n",
    "\n",
    "data['Survived'].hist()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of \"Survived\" using Seaborn\n",
    "# The fastest way is to reference Column Names as Properties of the Dataframe\n",
    "\n",
    "sns.countplot(data[\"Survived\"])\n",
    "\n",
    "# Note that another, \"safe\" way to code is to pass parameters by name\n",
    "# sns.countplot(x='Survived', data=data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Base-Rate Outcomes, by Condition\n",
    "\n",
    "Again, let's start by calculating the frequency (or \"base rate\") of the outcome variable for different conditions of interest. You can do this most easily by using the `.crosstab()` function from the `pandas` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pandas crosstab() function to count outcomes by condition\n",
    "\n",
    "pd.crosstab(data['Pclass'], data['Survived'], margins=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now show the totals (so you can calculate a conditional marginal base rate).\n",
    "\n",
    "pd.crosstab(data['Pclass'], data['Survived'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the style() function (chain it onto the end of prior code) to also overlay a heatmap \n",
    "# i.e., you can still do this in one line of code with .style.background_gradient()\n",
    "\n",
    "pd.crosstab(data['Pclass'], data['Survived'], margins=True).style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you can't read the results, select your own color scheme\n",
    "\n",
    "pd.crosstab(data['Pclass'], data['Survived'], margins=True).style.background_gradient(cmap='autumn_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do a \"three-way\" crosstab for Class, Sex, Survival\n",
    "\n",
    "pd.crosstab([data['Sex'], data['Survived']], data['Pclass'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pandas dataframe to plot a histogram of Age \n",
    "\n",
    "data['Age'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the number of histogram bins\n",
    "\n",
    "data['Age'].hist(bins = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seaborn to plot the kernel density (a kdeplot) for Age\n",
    "\n",
    "facet = sns.FacetGrid(data, aspect = 4)\n",
    "facet.map(sns.kdeplot,'Age', shade = True)\n",
    "facet.set(xlim = (0, data['Age'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Seaborn to plot the kernel density of Fare\n",
    "\n",
    "facet = sns.FacetGrid(data, aspect=4)\n",
    "facet.map(sns.kdeplot,'Fare', shade=True)\n",
    "facet.set(xlim = (0, data['Fare'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo the above in just 1 line of code, but show both a frequency histogram of counts, \n",
    "# and a kernel density of the probability density function\n",
    "\n",
    "# There usually is a simple way to do it with seaborn...\n",
    "\n",
    "facet = sns.distplot(data['Fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to plot a histogram of Survived, separated by Class\n",
    "# Hint: figsize is defined in inches\n",
    "\n",
    "lived = data[data['Survived'] == 1]['Pclass'].value_counts()\n",
    "died  = data[data['Survived'] == 0]['Pclass'].value_counts()\n",
    "df = pd.DataFrame([lived, died])\n",
    "df.index = ['Lived', 'Died']\n",
    "df.plot(kind = 'bar', stacked=True, figsize=(12, 5), title='Survival by Social Economic Class (1st, 2nd, 3rd)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seaborn barplots to plot Survival as a Function of Class\n",
    "\n",
    "sns.barplot(x='Pclass', y='Survived', data=data)\n",
    "plt.ylabel(\"Survival Rate\")\n",
    "plt.title(\"Survival as function of Pclass\")\n",
    "plt.show() # this removes the annoying line that references the final object, e.g. \"<matplotlib.axes._subplots.AxesSubplot at 0x1a1d5f4e48>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to plot a histogram of Survived, separated by Sex\n",
    "\n",
    "lived = data[data['Survived'] == 1]['Sex'].value_counts()\n",
    "died  = data[data['Survived'] == 0]['Sex'].value_counts()\n",
    "df = pd.DataFrame([lived, died])\n",
    "df.index = ['Lived', 'Died']\n",
    "df.plot(kind = 'bar', stacked = True, figsize = (12, 5), title = 'Survival by Gender')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas (with matplotlib customization to make it look good) to draw a pie chart of Survival by Sex\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(16,7))\n",
    "data['Survived'][data['Sex'] == 'male'].value_counts().plot.pie(ax = ax1)\n",
    "data['Survived'][data['Sex'] == 'female'].value_counts().plot.pie(ax = ax2, colors = ['C1', 'C0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use some matplotlib customization to make your previous plot look cool \n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize = (16, 7))\n",
    "data['Survived'][data['Sex'] == 'male'].value_counts().plot.pie(explode=[0,0.2], autopct='%1.1f%%', ax = ax[0], shadow = True)\n",
    "data['Survived'][data['Sex'] == 'female'].value_counts().plot.pie(explode=[0,0.2], autopct='%1.1f%%', ax = ax[1], shadow = True, colors = ['C1', 'C0'])\n",
    "ax[0].set_title('Survived (male)')\n",
    "ax[1].set_title('Survived (female)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a seaborn facet grid to jointly examine Sex, Class, and Survival\n",
    "\n",
    "g = sns.FacetGrid(data, row = 'Sex', col = 'Pclass', hue = 'Survived', margin_titles = True, height = 3, aspect = 1.1)\n",
    "g.map(sns.distplot, 'Age', kde = False, bins = np.arange(0, 80, 5), hist_kws = dict(alpha=0.6))\n",
    "g.add_legend()  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the disribution of Fare as a function of Pclass, Sex and Survived\n",
    "\n",
    "g = sns.FacetGrid(data, row = 'Sex', col = 'Pclass', hue = 'Survived', margin_titles = True, height = 3, aspect = 1.1)\n",
    "g.map(sns.distplot, 'Fare', kde = False, bins = np.arange(0, 550, 50), hist_kws = dict(alpha = 0.6))\n",
    "g.add_legend()  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sometimes you have a particular look you need to reproduce in an exhibit. \n",
    "In that case, you will grab the figure and axis objects from matplotlib \n",
    "after they are created by seaborn (or pandas) so that you can reference \n",
    "them to customize properties of each plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the plt.subplots() function from pyplot to capture the figure and subplot objects\n",
    "# so you can work with both seaborn and matplot lib to make a fully customized distribution plot\n",
    "\n",
    "LABEL_SURVIVED = 'Survived'\n",
    "LABEL_DIED = 'Did Not Survive'\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (12, 6))\n",
    "women = data[data['Sex'] == 'female']\n",
    "men = data[data['Sex'] == 'male']\n",
    "\n",
    "ax = sns.distplot(women[women['Survived'] == 1]['Age'].dropna(), bins = 18, label = LABEL_SURVIVED, ax = axes[0], kde = False)\n",
    "ax = sns.distplot(women[women['Survived'] == 0]['Age'].dropna(), bins = 40, label = LABEL_DIED, ax = axes[0], kde = False)\n",
    "ax.legend()\n",
    "ax.set_title('Female')\n",
    "\n",
    "ax = sns.distplot(men[men['Survived'] == 1]['Age'].dropna(), bins = 18, label = LABEL_SURVIVED, ax = axes[1], kde = False)\n",
    "ax = sns.distplot(men[men['Survived'] == 0]['Age'].dropna(), bins = 40, label = LABEL_DIED, ax = axes[1], kde = False)\n",
    "ax.legend()\n",
    "_ = ax.set_title('Male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED: combine histograms for many variables related to survival into one composite figure\n",
    "\n",
    "R = 2\n",
    "C = 3\n",
    "fields = ['Survived', 'Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked']\n",
    "\n",
    "fig, axs = plt.subplots(R, C, figsize = (12, 8))\n",
    "\n",
    "for row in range(0, R):\n",
    "    for col in range(0, C):  \n",
    "        i = row * C + col       \n",
    "        ax = axs[row][col]\n",
    "        sns.countplot(data[fields[i]], hue = data[\"Survived\"], ax = ax)\n",
    "        ax.set_title(fields[i], fontsize = 14)\n",
    "        ax.legend(title = \"survived\", loc = 'upper center') \n",
    "        \n",
    "plt.tight_layout()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2**: Feature Engineering\n",
    "\n",
    "Using domain knowledge, we can create new features that might improve performance of our model at a later stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the leading \"title\" from the passenger name, and summarize (count) the different titles \n",
    "\n",
    "data['Title'] = data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "data['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3**: Explore Swarm and Violin Plots\n",
    "\n",
    "Swarm and violin plots show the same data as plots of counts and/or density distributions (graphs you did earlier), but they also happen to look very cool and can also draw attention to details that you do not see in other plots. If you have extra time, try to make a few of these below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants (such as PALLET and FIGSIZE) so that all of your figures look consistent\n",
    "\n",
    "PALETTE = [\"lightgreen\" , \"lightblue\"]  # you can set a custom palette with a simple list of named color values\n",
    "FIGSIZE = (13, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a seaborn \"swarmplot\" to examine survival by age and class.\n",
    "\n",
    "fig, ax = plt.subplots(figsize = FIGSIZE)\n",
    "sns.swarmplot(x = 'Pclass', y = 'Age', hue = 'Survived', dodge = True, data = data, palette = PALETTE, size = 7, ax = ax)\n",
    "plt.title('Survival Events by Age and Class ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a seaborn \"violinplot\" to examine survival by age and class. \n",
    "\n",
    "fig, ax = plt.subplots(figsize = FIGSIZE)\n",
    "sns.violinplot(x = \"Pclass\", y = \"Age\", hue = 'Survived', data=data, split=True, bw = 0.05 , palette = PALETTE, ax = ax)\n",
    "plt.title('Survival Distributions by Age and Class ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the catplot function (in just one line of code!) to show the comparable distributions for Class, Age, Sex and Survived\n",
    "\n",
    "g = sns.catplot(x = \"Pclass\", y = \"Age\", hue = \"Survived\", col = \"Sex\", data = data, kind = \"violin\", split = True, bw = 0.05, palette = PALETTE, height = 7, aspect = 0.9, s = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recap of basic Python visualization methods with matplotlib: https://machinelearningmastery.com/data-visualization-methods-in-python/ \n",
    "- Guide to Bokeh, an increasingly popular Python interactive visualization package: https://realpython.com/python-data-visualization-bokeh/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
