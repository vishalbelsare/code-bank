{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Customer Segmentation - K-means & TMAP Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:  [https://github.com/d-insight/code-bank.git](https://github.com/d-insight/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://conversionxl.com/wp-content/uploads/2016/09/segmentation-illustration.png\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "Image source: https://conversionxl.com/wp-content/uploads/2016/09/segmentation-illustration.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset source: *Abreu, N. (2011). Analise do perfil do cliente Recheio e desenvolvimento de um sistema promocional. Mestrado em Marketing, ISCTE-IUL, Lisbon* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this demo is to cluster customers of a wholesale Portugese company into different segments. Note that clustering is an unsupervised learning task where no labels are given. Description of the fields are as follows (m.u. stands for monetary unit): \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature name     | Variable Type | Description \n",
    "|------------------|---------------|--------------------------------------------------------\n",
    "| FRESH            | Continuous    | annual spending (m.u.) on fresh products   \n",
    "| MILK             | Continuous    | annual spending (m.u.) on milk products  \n",
    "| GROCERY          | Continuous    | annual spending (m.u.) on grocery products  \n",
    "| FROZEN           | Continuous    | annual spending (m.u.) on frozen products   \n",
    "| DETERGENTS_PAPER | Continuous    | annual spending (m.u.) on detergents and paper products  \n",
    "| DELICATESSEN     | Continuous    | annual spending (m.u.) on delicatessen products    \n",
    "| CHANNEL          | Categorical   | customers channel where 1 = HoReCa (Hotel/Restaurant/Cafe); 2 = Retail channel\n",
    "| REGION           | Categorical   | customers region where 1 = Lisbon; 2 = Porto; 3 = Other  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 0**: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all import statements at the top of your notebook\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from time import time\n",
    "\n",
    "# Clustering imports\n",
    "from sklearn               import preprocessing\n",
    "from sklearn               import datasets\n",
    "from sklearn.cluster       import KMeans\n",
    "from sklearn.metrics       import silhouette_samples, silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold      import TSNE\n",
    "import tmap                as tm\n",
    "\n",
    "# Visualization packages\n",
    "from bokeh.models         import HoverTool\n",
    "from bokeh.plotting       import output_notebook, figure, show, ColumnDataSource\n",
    "import bokeh.plotting     as bp\n",
    "import matplotlib.pyplot  as plt\n",
    "import matplotlib.cm      as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker    import NullFormatter\n",
    "from faerun               import Faerun\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for replication\n",
    "SEED = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1**: Data Preprocessing and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('customer_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature types\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key statistics\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2**: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several preprocessing steps before doing the clustering:\n",
    "\n",
    "* null values\n",
    "* categorical variables\n",
    "* standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets control redundant and missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of observations and features\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop possible duplicate rows\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop possible missing values\n",
    "data.dropna(inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also convert categorical features into one hot encoded: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_transform = [ 'Channel', 'Region']\n",
    "data_with_dummies = pd.get_dummies(data = data, columns = cols_to_transform )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we should standardize data so that each feature have zero mean and unit standard deviation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = preprocessing.scale(data_with_dummies.values)\n",
    "type(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert standardized data from ndarray to data frame\n",
    "data_with_dummies_ready = pd.DataFrame(X_scaled, columns = data_with_dummies.columns)\n",
    "type(data_with_dummies_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_dummies_ready.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_dummies_ready.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3**: Kmeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering data into 3 clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=SEED).fit(data_with_dummies_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However we should pay attention to the followings:\n",
    "\n",
    "  * __initial centroids__: kmean clustering in sklearn is initializing centoids with different initial random seeds and will report the best result in terms of optimization score.  \n",
    "  \n",
    "  * __number of clusters__: to select the best number of clusters, one strategy is to maximize the \"Silhouette\" coefficient.  \n",
    "  \n",
    "    The Silhouette coefficient is calculated using the mean distance of a data point to all other points in the same cluster (a) and the mean distance to all data points in the nearest-cluster (b). The Silhouette Coefficient for a sample is (b - a) / max(a, b). The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar. We can consider the mean Silhuette coefficient of all samples as a rough metric for clustering quality.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we change number of clusters from 2 to 20 and chose the one with best Silouette score. For each number of clusters, we visualize the silouette score of each data point and project our data using PCA into 2 dimensions to visualize detected clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_with_dummies_ready\n",
    "range_n_clusters = range(2,21)\n",
    "\n",
    "# For different number of clusters do the followings\n",
    "for n_clusters in range_n_clusters:\n",
    "    \n",
    "    # Define a subplot with one row and two columns\n",
    "    fig, (ax1,ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "    \n",
    "    # Obtain cluster labels for n_clusters\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=SEED)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "    \n",
    "    # Calculate average silhouette score \n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "\n",
    "    # Calculate silhouette score for each data point\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "    \n",
    "    # Visualize silhouette score of each data point as well as average silhouette score\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "       \n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10  \n",
    "\n",
    "    ax1.set_title('The silhouette plot for the various clusters.')\n",
    "    ax1.set_xlabel('The silhouette coefficient values')\n",
    "    ax1.set_ylabel('Cluster label')\n",
    "    ax1.axvline(x=silhouette_avg, color='red', linestyle='--')\n",
    "    ax1.set_yticks([])  \n",
    "    ax1.set_xticks([-1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    \n",
    "    # Project data into two dimensions using PCA, with differnt colors for each cluster\n",
    "    pca = PCA(n_components=2, svd_solver='full')\n",
    "    XX = pca.fit(X).transform(X)\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(XX[:, 0], XX[:, 1], marker='.', s=30, lw=0, alpha=0.7, c=colors, edgecolor='k')\n",
    "    centers = pca.transform(clusterer.cluster_centers_)\n",
    "   \n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title('The visualization of the clustered data.')\n",
    "    ax2.set_xlabel('Feature space for the 1st feature')\n",
    "    ax2.set_ylabel('Feature space for the 2nd feature')\n",
    "    \n",
    "    plt.suptitle(('\\n Silhouette analysis for KMeans clustering on sample data '\n",
    "                  'with n_clusters = %d (Avg score: %f)' % (n_clusters, silhouette_avg)), fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the silhouette score with different number of clusters and different random seeds\n",
    "range_seeds = range(0,10)\n",
    "range_n_clusters = range(2,21)\n",
    "list_all = []\n",
    "list_cluster = []\n",
    "for seed in range_seeds:\n",
    "    for n_clusters in range_n_clusters:\n",
    "        \n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=seed)\n",
    "        cluster_labels = clusterer.fit_predict(data_with_dummies_ready)\n",
    "        silhouette_avg = silhouette_score(data_with_dummies_ready, cluster_labels)\n",
    "        \n",
    "        list_cluster.append(silhouette_avg)\n",
    "        list_cluster_tmp = list(list_cluster)\n",
    "        \n",
    "    list_all.append(list_cluster_tmp)\n",
    "    list_cluster.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize silhouette score for different number of clusters and different random seeds\n",
    "plt.xlabel('number of clusters')\n",
    "plt.ylabel('silhouette score')\n",
    "plt.grid(True)\n",
    "plt.xticks(np.arange(min(range_n_clusters),max(range_n_clusters)+1,1.0))\n",
    "gather_list_all = []\n",
    "\n",
    "for i in range_seeds:\n",
    "    gather_list_all.append(list_all[i])\n",
    "\n",
    "results_avg = [float(sum(col))/len(col) for col in zip(*gather_list_all)]\n",
    "\n",
    "plt.plot(range_n_clusters, results_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therefore we choose number of clusters equal to 9\n",
    "\n",
    "clusterer = KMeans(n_clusters=9, random_state=SEED)\n",
    "cluster_labels = clusterer.fit_predict(data_with_dummies_ready)\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SUMMARY OF SILHOUETTE SCORES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width    = 35\n",
    "clusters = [str(i) for i in range_n_clusters]\n",
    "results  = results_avg\n",
    "print('', '=' * width, '\\n', 'Summary of Silhouette Scores'.center(width), '\\n', '=' * width)  \n",
    "for i in range(len(clusters)):\n",
    "    print('K = {}'.format(clusters[i]).center(width-12), '{0:.4f}'.format(results[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 4**: TMAP \n",
    "\n",
    "TMAP is a very fast visualization library for large, high-dimensional data sets. Using TMAP, we generate an interactive visualization to explore the 9 cluster we found earlier. For details on TMAP: http://tmap.gdb.tools/index.html\n",
    "\n",
    "Here is an illustration of the 4 conceptual steps that make up TMAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://tmap.readthedocs.io/en/latest/_images/basic_pipelines.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "Image source: https://tmap.readthedocs.io/en/latest/_images/basic_pipelines.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MinHash encoding structure\n",
    "# NB: Minhash might not run on all machines - instaead, inspect the exported .html file in this directory\n",
    "dims = data_with_dummies_ready.shape[0] * data_with_dummies_ready.shape[1]\n",
    "enc = tm.Minhash(dims)\n",
    "\n",
    "# Locally sensitive hasing (LSH) speeds up k nearest neighbor search\n",
    "lf = tm.LSHForest(dims, 128)\n",
    "lf.batch_add(enc.batch_from_weight_array(data_with_dummies_ready.values))\n",
    "lf.index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coniguration for the tmap layout\n",
    "CFG = tm.LayoutConfiguration()\n",
    "\n",
    "# Create labels that appear on hover\n",
    "labels = ['customer{}_channel{}_region{}'.format(i, \n",
    "                                                 data.iloc[i,0], \n",
    "                                                 data.iloc[i,1]\n",
    "                                                ) for i in list(data_with_dummies_ready.index)]\n",
    "\n",
    "x, y, s, t, _ = tm.layout_from_lsh_forest(lf, CFG)\n",
    "\n",
    "faerun = Faerun(clear_color=\"#111111\", view=\"front\", coords=False)\n",
    "faerun.add_scatter(\n",
    "    \"Customers\",\n",
    "    {\"x\": x, \"y\": y, \"c\": cluster_labels, \"labels\": labels},\n",
    "    colormap=\"tab10\",\n",
    "    shader=\"smoothCircle\",\n",
    "    point_scale=5,\n",
    "    max_point_size=30,\n",
    "    has_legend=True,\n",
    "    categorical=True,\n",
    ")\n",
    "faerun.add_tree(\n",
    "    \"MNIST_tree\", {\"from\": s, \"to\": t}, point_helper=\"Customers\", color=\"#666666\"\n",
    ")\n",
    "faerun.plot(\"tmap_graph\", template=\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 5**: Interactive visualization of clusters using non-linear dimensionality reduction: T-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is able to grasp only **linear** projection of high-dimensional space into lower dimensions. In many cases, what is more precise in to learn the non-linear manifolds in which the data shows the highest variance along with. There are different techniques to achieve this goal, all with their own pros and cons. t-Distributed Stochastic Neighbor Embedding is an state of the art technique to achieve this goal. Following example shows the main intuition behind this technique. For more information, you can have a look [here](https://lvdmaaten.github.io/tsne/). To understand the difference better, look at the following toy example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired from work by Jake Vanderplas -- <vanderplas@astro.washington.edu>\n",
    "\n",
    "n_points = 1000\n",
    "X_sample, color = datasets.make_s_curve(n_points, random_state=0)\n",
    "n_components = 2\n",
    "\n",
    "# Original space\n",
    "fig = plt.figure(figsize=(32, 12))\n",
    "ax = fig.add_subplot(251, projection='3d')\n",
    "ax.scatter(X_sample[:, 0], X_sample[:, 1], X_sample[:, 2], c=color, cmap=plt.cm.Spectral)\n",
    "ax.view_init(4, -72)\n",
    "\n",
    "# PCA space\n",
    "t0 = time()\n",
    "pca = PCA(n_components=n_components, random_state=0)\n",
    "Y_sample = pca.fit_transform(X_sample)\n",
    "t1 = time()\n",
    "print(\"PCA: %.2g sec\" % (t1 - t0))\n",
    "ax = fig.add_subplot(252)\n",
    "plt.scatter(Y_sample[:, 0], Y_sample[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"PCA (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "\n",
    "# TSNE space\n",
    "t0 = time()\n",
    "tsne = TSNE(n_components=n_components, init='pca', random_state=0)\n",
    "Y_sample = tsne.fit_transform(X_sample)\n",
    "t1 = time()\n",
    "print(\"t-SNE: %.2g sec\" % (t1 - t0))\n",
    "ax = fig.add_subplot(253)\n",
    "plt.scatter(Y_sample[:, 0], Y_sample[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"t-SNE (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In brief, T-SNE works by minimizing the divergence between two probability distribution between pairs, proportional to their distances, one in the original space and the other in the reduced dimension space. Unlike PCA, T-SNE is sensitive to its configuration parameters. Among them, following parameters worth mentioning here:\n",
    "\n",
    "* **perplexity**: a parameter representing the number of nearest neighbors considered as 'close' in the original space. \n",
    "* **early_exaggeration**: controls how tight natural clusters in the original space are in the embedded space and how much space will be between them. \n",
    "* **learning_rate**: the step size in solving optimization problem using gradient descent. it can make your optimization to converge fast or to escape from local minima. Note that different runs of this algorithm with different initial points can lead to different results due to non-convexity of the optimization surface. One might run the algorithm several times with different initial values and choose the one with minimum divergence score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming back to the customer segmentation problem, we try to visualize our data with optimum number of clusters (9 clusters) using T-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we transform data into lower dimensions using distance of each data points to the 9 centroids\n",
    "X_kmeans_distances = clusterer.transform(data_with_dummies_ready)\n",
    "X_kmeans_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we transfrom data using 9 dimensional space to 2 dimensional space using t-sne\n",
    "tsne2 = TSNE(n_components=2, verbose=1, random_state=1, method='exact')\n",
    "X_kmeans_distances_tsne2 = tsne2.fit_transform(X_kmeans_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kmeans_distances_tsne2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a colormap of random colors\n",
    "colormap = np.array([\"#6d8dca\", \"#69de53\", \"#723bca\", \"#c3e14c\", \"#c84dc9\", \"#68af4e\", \"#6e6cd5\",\n",
    "\"#e3be38\", \"#4e2d7c\", \"#5fdfa8\", \"#d34690\", \"#3f6d31\", \"#d44427\", \"#7fcdd8\", \"#cb4053\", \"#5e9981\",\n",
    "\"#803a62\", \"#9b9e39\", \"#c88cca\", \"#e1c37b\", \"#34223b\", \"#bdd8a3\", \"#6e3326\", \"#cfbdce\", \"#d07d3c\",\n",
    "\"#52697d\", \"#7d6d33\", \"#d27c88\", \"#36422b\", \"#b68f79\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We transfor the 2 dimensioanl tsne data into a data frame with associated cluster number and color\n",
    "\n",
    "def calculate_color(cluster):\n",
    "    color = colormap[cluster]\n",
    "    return color\n",
    "\n",
    "dataset_kmeans_vis = pd.DataFrame(X_kmeans_distances_tsne2, columns=['x', 'y'])\n",
    "dataset_kmeans_vis['cluster'] = cluster_labels\n",
    "dataset_kmeans_vis['color'] = dataset_kmeans_vis.cluster.apply(calculate_color)\n",
    "dataset_kmeans_vis['channel'] = data['Channel']\n",
    "dataset_kmeans_vis['region'] = data['Region']\n",
    "\n",
    "dataset_kmeans_vis.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize using bokeh library which is used for interactive visualization\n",
    "# BokehJS mit return an error the first time running this \n",
    "\n",
    "source = ColumnDataSource(data=dataset_kmeans_vis)\n",
    "\n",
    "plot_kmeans = bp.figure(plot_width=800, plot_height=600, title='KMeans clustering of wholesale customers',\n",
    "    tools='pan,wheel_zoom,box_zoom,reset,hover',\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "output_notebook()\n",
    "plot_kmeans.scatter(x='x', y='y', color='color', size=5, source=source)\n",
    "hover = plot_kmeans.select(dict(type=HoverTool))\n",
    "hover.tooltips=[(\"index\", \"$index\"),('cluster','@cluster'), ('channel', '@channel'), ('region', '@region')]\n",
    "show(plot_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 6**: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What could be the semantics behind each cluster? \n",
    "* How the set of chosen features can affect cluster semantics ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
