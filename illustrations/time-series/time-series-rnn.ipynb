{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Time Series**: Forecasting with a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:  [https://github.com/d-insight/code-bank.git](https://github.com/d-insight/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this illustration, we will develop a univerate and multivariate time series model using a recurrent neural network (RNN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data we use a weather time series recorded by the [Max Planck Institute for Biogeochemistry](https://www.bgc-jena.mpg.de/wetter/). The dataset contains 14 different features such as air temperature, atmospheric pressure, and humidity. Samples are collected every 10 minutes, beginning in 2003. For this illustration, we will use data collected between 2009 and 2016 for efficiency. This part of the dataset was prepared by Fran√ßois Chollet for his book [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Source__: Portions of this page are reproduced from work created and shared by Google and used according to terms described in the [Creative Commons 4.0 Attribution License](https://creativecommons.org/licenses/by/4.0/). For the original tutorial visit: https://www.tensorflow.org/tutorials/text/image_captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 0**: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Import tensorflow \n",
    "import tensorflow as tf\n",
    "\n",
    "# Plotting and visualization \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 8)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED        = 42       # random seed for replication\n",
    "TRAIN_SPLIT = 300000   # restrict training data to ~2100 days \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_data(dataset, start_index, end_index, history_size, target_size):\n",
    "    \"\"\"\n",
    "    Returns windows of time for the model to train on for univariate data. \n",
    "    \n",
    "    Args:\n",
    "        history_size (int): size of the past window of information\n",
    "        target_size (int): how far into the future the model has to learn to predict\n",
    "        \n",
    "    Returns: \n",
    "        np.array(data): feature data in window\n",
    "        np.array(labels): labels in window \n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, 1)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "        labels.append(dataset[i+target_size])\n",
    "        \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def multivariate_data(dataset, target, start_index, end_index, history_size, target_size, step, single_step=False):\n",
    "    \"\"\"\n",
    "    Returns windows of time for the model to train on for multivariate data. \n",
    "    \n",
    "    Args:\n",
    "        history_size (int): size of the past window of information\n",
    "        target_size (int): how far into the future the model has to learn to predict\n",
    "        \n",
    "    Returns: \n",
    "        np.array(data): feature data in window\n",
    "        np.array(labels): labels in window \n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def create_time_steps(length):\n",
    "    return list(range(-length, 0))\n",
    "\n",
    "def baseline(history):\n",
    "    return np.mean(history)\n",
    "\n",
    "def show_plot(plot_data, delta, title):\n",
    "    \"\"\"\n",
    "    Shows a plot with history data, the true future and the model prediction\n",
    "    \n",
    "    Args:\n",
    "        plot_data: (x, y) data to plot\n",
    "        delta (int): difference between historical data and future (usually 0)\n",
    "        title (str): plot title\n",
    "        \n",
    "    Returns: \n",
    "        np.array(data): feature data in window\n",
    "        np.array(labels): labels in window \n",
    "    \"\"\"\n",
    "    labels = ['History', 'True Future', 'Model Prediction']\n",
    "    marker = ['.-', 'rx', 'go']\n",
    "    time_steps = create_time_steps(plot_data[0].shape[0])\n",
    "    \n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, x in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10,\n",
    "                     label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlim([time_steps[0], (future+5)*2])\n",
    "    plt.xlabel('Time-Step')\n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "def plot_train_history(history, title):\n",
    "    \"\"\"\n",
    "    Plot the training history (i.e. training loss and validation loss)\n",
    "    \n",
    "    Args:\n",
    "        history: training history\n",
    "        title (str): plot title\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(loss))\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def multi_step_plot(history, true_future, prediction):\n",
    "    \"\"\"\n",
    "    Shows a plot with history data, the true future and the model prediction\n",
    "    \n",
    "    Args:\n",
    "        history: training history\n",
    "        true_future: true future temperatures\n",
    "        prediction: predicted temperatures\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    num_in = create_time_steps(len(history))\n",
    "    num_out = len(true_future)\n",
    "\n",
    "    plt.plot(num_in, np.array(history[:, 1]), '.-', label='History')\n",
    "    plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
    "           label='True Future')\n",
    "    if prediction.any():\n",
    "        plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro', label='Predicted Future')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1**: Download data and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download weather data and extract .zip file\n",
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    cache_dir='data/weather/',\n",
    "    cache_subdir='',\n",
    "    extract=True)\n",
    "csv_path, _ = os.path.splitext(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data with pandas \n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, samples are recorded every 10 mintues. This means that, for a single hour, you will have 6 observations. Similarly, a single day will contain 144 (6 x 24) observations.\n",
    "\n",
    "Given a specific time, let's say you want to predict the temperature 6 hours into the future. In order to make this prediction, you choose to use 5 days of samples. Thus, you would create a window containing the last 720 (5 x 144) observations to train the model. Many such configurations are possible, so it's a good idea to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed \n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract temperature data (univerate time series)\n",
    "uni_data_df = df['T (degC)']\n",
    "uni_data_df.index = df['Date Time']\n",
    "\n",
    "uni_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot temperature data\n",
    "uni_data_df.plot(grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2**: Univerate time series\n",
    "\n",
    "First, we will train a model using only a single input feature (temperature), and use that to make predictions for that same value in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract temperature values\n",
    "uni_data = uni_data_df.values\n",
    "\n",
    "# Standardize data\n",
    "uni_train_mean = uni_data[:TRAIN_SPLIT].mean()\n",
    "uni_train_std = uni_data[:TRAIN_SPLIT].std()\n",
    "uni_data = (uni_data-uni_train_mean)/uni_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split: past 30 temperature samples; predict temperature at the next time step\n",
    "\n",
    "univariate_past_history = 30\n",
    "univariate_future_target = 0\n",
    "\n",
    "x_train_uni, y_train_uni = univariate_data(uni_data, 0, TRAIN_SPLIT,\n",
    "                                           univariate_past_history,\n",
    "                                           univariate_future_target)\n",
    "x_val_uni, y_val_uni = univariate_data(uni_data, TRAIN_SPLIT, None,\n",
    "                                       univariate_past_history,\n",
    "                                       univariate_future_target)\n",
    "\n",
    "print('x_train shape:'.ljust(40) + str(x_train_uni.shape))\n",
    "print('Single window of past history shape:'.ljust(40) + str(x_train_uni[0].shape))\n",
    "print('Target temperature:'.ljust(40) + str(y_train_uni[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction\n",
    "show_plot([x_train_uni[0], y_train_uni[0]], 0, 'Sample Example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Prediction: the MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot baseline prediction\n",
    "show_plot([x_train_uni[0], y_train_uni[0], baseline(x_train_uni[0])], 0, 'Baseline Prediction: the MEAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN Prediction\n",
    "\n",
    "A recurrent neural network (RNN) is well-suited to sequential data such as this time series. We will use a specialized RNN layer called Long Short Term Memory (LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle, batch, and cache the dataset\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_univariate = tf.data.Dataset.from_tensor_slices((x_train_uni, y_train_uni))\n",
    "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_univariate = tf.data.Dataset.from_tensor_slices((x_val_uni, y_val_uni))\n",
    "val_univariate = val_univariate.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a simple LSTM model\n",
    "simple_lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(8, input_shape=x_train_uni.shape[-2:]),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "simple_lstm_model.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "EVALUATION_INTERVAL = 200\n",
    "EPOCHS = 10\n",
    "\n",
    "simple_lstm_model.fit(train_univariate, epochs=EPOCHS,\n",
    "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      validation_data=val_univariate, validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 3 windows and plot predictions\n",
    "i = 0\n",
    "for x, y in val_univariate.take(3):\n",
    "    show_plot([x[0].numpy(), y[0].numpy(), simple_lstm_model.predict(x)[0]], 0, 'Simple LSTM model with data {}'.format(i+1))\n",
    "    i += 1\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better than the baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3**: Multiverate Time Series (RNN)\n",
    "\n",
    "Next, we will train a model on three of the original fourteen features. The features used are: air temperature, atmospheric pressure, and air density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features \n",
    "features_considered = ['p (mbar)', 'T (degC)', 'rho (g/m**3)']\n",
    "features = df[features_considered]\n",
    "features.index = df['Date Time']\n",
    "\n",
    "print(features.shape)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot features\n",
    "features.plot(subplots=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data \n",
    "dataset = features.values\n",
    "data_mean = dataset[:TRAIN_SPLIT].mean(axis=0)\n",
    "data_std = dataset[:TRAIN_SPLIT].std(axis=0)\n",
    "dataset = (dataset-data_mean)/data_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Step Model: Predict the Next Timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model trains on hourly samples, so we need 720 original samples (= 5 days)\n",
    "# Forecast temperature 12 hours into the future (72 = 12 x 6)\n",
    "past_history = 720\n",
    "future_target = 72\n",
    "STEP = 6\n",
    "\n",
    "x_train_single, y_train_single = multivariate_data(dataset, dataset[:, 1], 0,\n",
    "                                                   TRAIN_SPLIT, past_history,\n",
    "                                                   future_target, STEP,\n",
    "                                                   single_step=True)\n",
    "x_val_single, y_val_single = multivariate_data(dataset, dataset[:, 1],\n",
    "                                               TRAIN_SPLIT, None, past_history,\n",
    "                                               future_target, STEP,\n",
    "                                               single_step=True)\n",
    "\n",
    "print('x_train_single shape:'.ljust(40) + str(x_train_single.shape))\n",
    "print('Single window of past history shape:'.ljust(40) + str(x_train_single[0].shape))\n",
    "print('Target temperature:'.ljust(40) + str(y_val_single[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle, batch, and cache the dataset\n",
    "train_data_single = tf.data.Dataset.from_tensor_slices((x_train_single, y_train_single))\n",
    "train_data_single = train_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))\n",
    "val_data_single = val_data_single.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a simple LSTM model\n",
    "single_step_model = tf.keras.models.Sequential()\n",
    "single_step_model.add(tf.keras.layers.LSTM(32,\n",
    "                                           input_shape=x_train_single.shape[-2:]))\n",
    "single_step_model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "single_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "single_step_history = single_step_model.fit(train_data_single, epochs=EPOCHS,\n",
    "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                            validation_data=val_data_single,\n",
    "                                            validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history (i.e. training loss and validation loss)\n",
    "plot_train_history(single_step_history,\n",
    "                   'Single Step Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3 windows and predictions\n",
    "DELTA = 12 \n",
    "i = 0\n",
    "for x, y in val_data_single.take(3):\n",
    "    show_plot([x[0][:, 1].numpy(), y[0].numpy(), single_step_model.predict(x)[0]], DELTA, 'Single Step Prediction with data {}'.format(i))\n",
    "    i += 1\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Step Model: Predict a Range of Future Values\n",
    "\n",
    "In a multi-step prediction model, given a past history, the model needs to learn to predict a range of future values.\n",
    "\n",
    "The training data again consists of recordings over the past five days sampled every hour. However, here, the model needs to learn to predict the temperature for the next 12 hours. Since an obversation is taken every 10 minutes, the output is 72 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training/validation data\n",
    "future_target = 72\n",
    "x_train_multi, y_train_multi = multivariate_data(dataset, dataset[:, 1], 0,\n",
    "                                                 TRAIN_SPLIT, past_history,\n",
    "                                                 future_target, STEP)\n",
    "x_val_multi, y_val_multi = multivariate_data(dataset, dataset[:, 1],\n",
    "                                             TRAIN_SPLIT, None, past_history,\n",
    "                                             future_target, STEP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle, batch, and cache the dataset\n",
    "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2 windows and the future values to predict\n",
    "for x, y in train_data_multi.take(2):\n",
    "    multi_step_plot(x[0], y[0], np.array([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LSTM model\n",
    "multi_step_model = tf.keras.models.Sequential()\n",
    "multi_step_model.add(tf.keras.layers.LSTM(32,\n",
    "                                          return_sequences=True,\n",
    "                                          input_shape=x_train_multi.shape[-2:]))\n",
    "multi_step_model.add(tf.keras.layers.LSTM(16, activation='relu'))\n",
    "multi_step_model.add(tf.keras.layers.Dense(72))\n",
    "\n",
    "multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model \n",
    "multi_step_history = multi_step_model.fit(train_data_multi, epochs=EPOCHS,\n",
    "                                          steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                          validation_data=val_data_multi,\n",
    "                                          validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plot_train_history(multi_step_history, 'Multi-Step Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4 windows and the predicted values \n",
    "for x, y in val_data_multi.take(4):\n",
    "    multi_step_plot(x[0], y[0], multi_step_model.predict(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE \n",
    "for x, y in val_univariate.take(1):\n",
    "    show_plot([x[0].numpy(), y[0].numpy(), baseline(x[0])], 0, 'Baseline Prediction: the MEAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIVARIATE\n",
    "for x, y in val_univariate.take(1):\n",
    "    show_plot([x[0].numpy(), y[0].numpy(), simple_lstm_model.predict(x)[0]], 0, 'Simple LSTM model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTIVARIATE (single step)\n",
    "for x, y in val_data_single.take(1):\n",
    "    show_plot([x[0][:, 1].numpy(), y[0].numpy(), single_step_model.predict(x)[0]], DELTA, 'Single Step Prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTIVARIATE (multi-step)\n",
    "for x, y in val_data_multi.take(1):\n",
    "    multi_step_plot(x[0], y[0], multi_step_model.predict(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
