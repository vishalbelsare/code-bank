{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Importance**: Comparing Random Forests vs. Permutation Importance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:  [https://github.com/d-insight/code-bank.git](https://github.com/d-insight/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will compare the impurity-based feature importance of a random forest classifier with the permutation importance on the titanic dataset using. The permutation importance approach is \"model agnostic\" and defines feature importance as the decrease in performance when a single feature value is randomly shuffled. We will show that the impurity-based feature importance can inflate the importance of numerical features.\n",
    "\n",
    "Furthermore, the impurity-based feature importance of random forests suffers from being computed on statistics derived from the training dataset: the importances can be high even for features that are not predictive of the target variable, as long as the model has the capacity to use them to overfit.\n",
    "\n",
    "Source: https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will examine the Titanic data (the sinking of the RMS Titanic) from the Kaggle competition](https://www.kaggle.com/c/titanic). There are many other examples of the Titanic dataset in introductory statistics and Data Science courses, so we also encourage you to look around and see how others have approached the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/95/Titanic_sinking%2C_painting_by_Willy_St%C3%B6wer.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "Image source: https://upload.wikimedia.org/wikipedia/commons/9/95/Titanic_sinking%2C_painting_by_Willy_St%C3%B6wer.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 0**: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all packages\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [16, 8]\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets        import fetch_openml\n",
    "from sklearn.ensemble        import RandomForestClassifier\n",
    "from sklearn.impute          import SimpleImputer\n",
    "from sklearn.inspection      import permutation_importance\n",
    "from sklearn.compose         import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.preprocessing   import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1**: Load & preprocess data\n",
    "\n",
    "We add two random variables that are not correlated in any way with the target variable ``survived``:\n",
    "\n",
    "- ``random_num`` is a high cardinality numerical variable (as many unique values as records)\n",
    "- ``random_cat`` is a low cardinality categorical variable (3 possible values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch titantic data \n",
    "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "\n",
    "# Set numpy random state  to generate random variables\n",
    "rng = np.random.RandomState(seed=SEED)\n",
    "\n",
    "# Add random variables \n",
    "X['random_cat'] = rng.randint(3, size=X.shape[0])\n",
    "X['random_num'] = rng.randn(X.shape[0])\n",
    "\n",
    "# Select columns\n",
    "categorical_columns = ['pclass', 'sex', 'embarked', 'random_cat']\n",
    "numerical_columns = ['age', 'sibsp', 'parch', 'fare', 'random_num']\n",
    "\n",
    "X = X[categorical_columns + numerical_columns]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=SEED)\n",
    "\n",
    "# Impute and one-hot-encode data\n",
    "categorical_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "numerical_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    [('cat', categorical_pipe, categorical_columns),\n",
    "     ('num', numerical_pipe, numerical_columns)])\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2**: Fit random forest model and evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a pipeline to preprocess data, then fit a random forest classifier\n",
    "rf = Pipeline([\n",
    "    ('preprocess', preprocessing),\n",
    "    ('classifier', RandomForestClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "# Fit random forest\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Note the perfect predictions on the train set - the model memorizes the training data ... and overfits\n",
    "print(\"RF train accuracy: %0.3f\" % rf.score(X_train, y_train))\n",
    "print(\"RF test accuracy: %0.3f\" % rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3**: Feature importance - Random Forest (mean decrease in impurity)\n",
    "\n",
    "Random forests compute feature importance based on a decrease in impurity. Impurity-based feature importance ranks the numerical features to be the most important features. As a result, the non-predictive random_num variable is ranked the most important!\n",
    "\n",
    "This problem stems from two limitations of impurity-based feature importances:\n",
    "\n",
    "- impurity-based importances are biased towards high cardinality features (i.e. ``random_num`` random variable)\n",
    "- impurity-based importances are computed on training set statistics and therefore do not reflect the ability of feature to be useful to make predictions that generalize to the test set (when the model has enough capacity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "ohe = (rf.named_steps['preprocess']\n",
    "         .named_transformers_['cat']\n",
    "         .named_steps['onehot'])\n",
    "feature_names = ohe.get_feature_names(input_features=categorical_columns)\n",
    "feature_names = np.r_[feature_names, numerical_columns]\n",
    "\n",
    "# Get feature importances from fitted model\n",
    "tree_feature_importances = (\n",
    "    rf.named_steps['classifier'].feature_importances_)\n",
    "sorted_idx = tree_feature_importances.argsort()\n",
    "\n",
    "# Plot feature importances \n",
    "y_ticks = np.arange(0, len(feature_names))\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(y_ticks, tree_feature_importances[sorted_idx])\n",
    "ax.set_yticklabels(feature_names[sorted_idx])\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_title(\"Random Forest Feature Importances (MDI)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 4**: Feature importance - Permutation Importance\n",
    "\n",
    "As an alternative, the permutation importances of rf are computed on a held out test set. This shows that the low cardinality categorical feature, sex is the most important feature.\n",
    "\n",
    "Also note that both random features have very low importances (close to 0) as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute permutation importance \n",
    "result = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=SEED, n_jobs=-1)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "# Plot importance \n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(result.importances[sorted_idx].T,\n",
    "           vert=False, labels=X_test.columns[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances (TEST set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
