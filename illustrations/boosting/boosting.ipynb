{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DSFM Illustration**: Sequential boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creator: [Data Science for Managers - EPFL Program](https://www.dsfm.ch)  \n",
    "Source:  [https://github.com/dsfm-org/code-bank.git](https://github.com/dsfm-org/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate a sequential process of boosting by chaining together decisions trees to predict, generate residuals, re-predict, generate new residuals, and so on... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 0**: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages \n",
    "import numpy as np\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all constants\n",
    "\n",
    "# decision tree depth\n",
    "MAX_DEPTH = 3\n",
    "\n",
    "# plotting constants\n",
    "FIGSIZE   = (7, 5)\n",
    "LINEWIDTH = 3\n",
    "ROWS      = 2\n",
    "COLS      = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for plotting \n",
    "\n",
    "def myPlot(x, y, labelData, y_pred=None, labelPred=None):\n",
    "    \n",
    "    plt.figure(figsize=FIGSIZE)\n",
    "    plt.plot(x, y, linewidth = LINEWIDTH, label = labelData)\n",
    "    if type(y_pred) == np.ndarray and labelPred: \n",
    "        plt.plot(x, y_pred, linewidth = LINEWIDTH, label = labelPred)\n",
    "    plt.ylim(-1.1, 1.1)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    \n",
    "    return plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1**: Generate toy data from sine function\n",
    "\n",
    "We generate 100 samples from the trigonometric sine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create toy data\n",
    "SAMPLES = 100\n",
    "\n",
    "# Feature value\n",
    "x = np.linspace(-2*np.pi, 2*np.pi, SAMPLES)\n",
    "\n",
    "# Actual values come from the sin function\n",
    "y = np.sin(x)\n",
    "\n",
    "# Plot data\n",
    "myPlot(x, y, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2**: Fit a decision tree regressor to original data\n",
    "\n",
    "We fit a single (!) decision tree to the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict \n",
    "y_hat = DecisionTreeRegressor(max_depth=MAX_DEPTH).fit(x.reshape(-1, 1), y).predict(x.reshape(-1, 1))\n",
    "\n",
    "# Compare predictions with ground truth \n",
    "myPlot(x, y, 'data', y_hat, 'prediction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3a**: Fit a new decision tree regressor to the residuals\n",
    "\n",
    "We now fit a new, single decision tree to the residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals \n",
    "y_res1 = y - y_hat\n",
    "myPlot(x, y_res1, 'residuals')\n",
    "print('Sum of residuals: {}'.format(round(sum(abs(y_res1)), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit new tree to residuals\n",
    "y_res1_hat = DecisionTreeRegressor(max_depth=MAX_DEPTH).fit(x.reshape(-1, 1), y_res1).predict(x.reshape(-1, 1))\n",
    "\n",
    "# Compare predictions with residuals\n",
    "myPlot(x, y_res1, 'residuals', y_res1_hat, 'prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3b**: Fit another new decision tree regressor to the remaining residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit new tree to residuals\n",
    "y_res2 = y_res1 - y_res1_hat\n",
    "y_res2_hat = DecisionTreeRegressor(max_depth=MAX_DEPTH).fit(x.reshape(-1, 1), y_res2).predict(x.reshape(-1, 1))\n",
    "\n",
    "# Compare predictions with residuals\n",
    "myPlot(x, y_res2, 'residuals', y_res2_hat, 'prediction')\n",
    "print('Sum of residuals: {}'.format(round(sum(abs(y_res2)), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3c**: And again ...\n",
    "\n",
    "Note how the new decision trees in each iteration focus on the larger residual errors. The remaining residuals tend to be smaller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit new tree to residuals\n",
    "y_res3 = y_res2 - y_res2_hat\n",
    "y_res3_hat = DecisionTreeRegressor(max_depth=MAX_DEPTH).fit(x.reshape(-1, 1), y_res3).predict(x.reshape(-1, 1))\n",
    "\n",
    "# Compare predictions with residuals\n",
    "myPlot(x, y_res3, 'residuals', y_res3_hat, 'prediction')\n",
    "print('Sum of residuals: {}'.format(round(sum(abs(y_res3)), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3d**: And again ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit new tree to residuals\n",
    "y_res4 = y_res3 - y_res3_hat\n",
    "y_res4_hat = DecisionTreeRegressor(max_depth=MAX_DEPTH).fit(x.reshape(-1, 1), y_res4).predict(x.reshape(-1, 1))\n",
    "\n",
    "# Compare predictions with residuals\n",
    "myPlot(x, y_res4, 'residuals', y_res4_hat, 'prediction')\n",
    "print('Sum of residuals: {}'.format(round(sum(abs(y_res4)), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 4**: All together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust/overwrite maximum decision tree depth \n",
    "MAX_DEPTH = 3\n",
    "\n",
    "# Set up sub-plot \n",
    "fig, axs = plt.subplots(ROWS, COLS, figsize=(20, 10))\n",
    "axs      = axs.flatten()\n",
    "y_data   = y\n",
    "\n",
    "for i, ax in enumerate(axs): \n",
    "    \n",
    "    # Ground truth = residuals \n",
    "    if i > 0: y_data = y_data - y_hat\n",
    "    # Plot ground truth data\n",
    "    if i == 0: ax.set_title('Original data')\n",
    "    if i > 0: ax.set_title('Tree {} on residuals\\nSum of residuals: {}'.format(i, round(sum(abs(y_data)), 2)))\n",
    "    ax.plot(x, y_data, linewidth = LINEWIDTH, label='data')\n",
    "    ax.set_ylim(-1.1, 1.1)\n",
    "        \n",
    "    # Predict \n",
    "    y_hat = DecisionTreeRegressor(max_depth=MAX_DEPTH).fit(x.reshape(-1, 1), y_data).predict(x.reshape(-1, 1))\n",
    "    \n",
    "    # Plot prediction \n",
    "    ax.plot(x, y_hat, linewidth = LINEWIDTH, label='prediction')\n",
    "    ax.grid()\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bonus**: Further Reading\n",
    "\n",
    "- What is the difference between Bagging and Boosting? https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/\n",
    "- The intuition behind boosting: https://medium.com/greyatom/a-quick-guide-to-boosting-in-ml-acf7c1585cb5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
