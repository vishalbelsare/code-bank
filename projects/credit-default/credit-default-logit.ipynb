{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Credit Default Project**: Solved with a Logit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:  [https://github.com/d-insight/code-bank.git](https://github.com/d-insight/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will predict the probability of default on a credit card account. From a risk management perspective, the predicted probability of default is a valuable indicator of whether a customer should be granted credit or not. Learning from a data set where some customers have defaulted on their debt, we implement a simple statistical model.\n",
    "\n",
    "You are already familiar with the customer default payments dataset from the Demo, so feel free to refer back to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://greendayonline.com/wp-content/uploads/2017/03/Recovering-From-Student-Loan-Default.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "\n",
    "Image source: https://greendayonline.com/wp-content/uploads/2017/03/Recovering-From-Student-Loan-Default.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Credit Card Default Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to predict the probability of defaulting on a credit card account at a Taiwanese bank. A credit card default happens when a customer fails to pay the minimum due on a credit card bill for more than 6 months. \n",
    "\n",
    "We will use a dataset from a Taiwanese bank with 30,000 observations (Source: *Yeh, I. C., & Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2), 2473-2480.*). Each observation represents an account at the bank at the end of October 2005.  We renamed the variable default_payment_next_month to customer_default. The target variable to predict is `customer_default` -- i.e., whether the customer will default in the following month (1 = Yes or 0 = No). The dataset also includes 23 other explanatory features. \n",
    "\n",
    "Variables are defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature name     | Variable Type | Description \n",
    "|------------------|---------------|--------------------------------------------------------\n",
    "| customer_default | Binary        | 1 = default in following month; 0 = no default \n",
    "| LIMIT_BAL        | Continuous    | Credit limit   \n",
    "| SEX              | Categorical   | 1 = male; 2 = female\n",
    "| EDUCATION        | Categorical   | 1 = graduate school; 2 = university; 3 = high school; 4 = others\n",
    "| MARRIAGE         | Categorical   | 0 = unknown; 1 = married; 2 = single; 3 = others\n",
    "| AGE              | Continuous    | Age in years  \n",
    "| PAY1             | Categorical   | Repayment status in September, 2005 \n",
    "| PAY2             | Categorical   | Repayment status in August, 2005 \n",
    "| PAY3             | Categorical   | Repayment status in July, 2005 \n",
    "| PAY4             | Categorical   | Repayment status in June, 2005 \n",
    "| PAY5             | Categorical   | Repayment status in May, 2005 \n",
    "| PAY6             | Categorical   | Repayment status in April, 2005 \n",
    "| BILL_AMT1        | Continuous    | Balance in September, 2005  \n",
    "| BILL_AMT2        | Continuous    | Balance in August, 2005  \n",
    "| BILL_AMT3        | Continuous    | Balance in July, 2005  \n",
    "| BILL_AMT4        | Continuous    | Balance in June, 2005 \n",
    "| BILL_AMT5        | Continuous    | Balance in May, 2005  \n",
    "| BILL_AMT6        | Continuous    | Balance in April, 2005  \n",
    "| PAY_AMT1         | Continuous    | Amount paid in September, 2005\n",
    "| PAY_AMT2         | Continuous    | Amount paid in August, 2005\n",
    "| PAY_AMT3         | Continuous    | Amount paid in July, 2005\n",
    "| PAY_AMT4         | Continuous    | Amount paid in June, 2005\n",
    "| PAY_AMT5         | Continuous    | Amount paid in May, 2005\n",
    "| PAY_AMT6         | Continuous    | Amount paid in April, 2005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measurement scale for repayment status is:   \n",
    "\n",
    "    -2 = payment two months in advance   \n",
    "    -1 = payment one month in advance   \n",
    "    0 = pay duly   \n",
    "    1 = payment delay for one month   \n",
    "    2 = payment delay for two months   \n",
    "    3 = payment delay for three months   \n",
    "    4 = payment delay for four months   \n",
    "    5 = payment delay for five months   \n",
    "    6 = payment delay for six months   \n",
    "    7 = payment delay for seven months   \n",
    "    8 = payment delay for eight months   \n",
    "    9 = payment delay for nine months or more  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might remember, we tried to predict customer defaults using logistic regression. The validation of our results was based on a simple cross validation schema by splitting data into a train and test set. We also used class labels instead of class probabilities. Here, we are also implementing a logistic regression. We are adding a more statistically robust validation schema and more appropriate performance metric. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is divided into several parts. For each part, you will have time to work on the question yourself. Feel free to go back to the Demo, use Google/Stackoverflow and work with your neighbour. Together, we will review and discuss the solution to each part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 0**: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all packages \n",
    "\n",
    "# Use short-hand for standard packages\n",
    "import pandas            as pd\n",
    "import numpy             as np\n",
    "import seaborn           as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import individual functions from sklearn \n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.dummy           import DummyClassifier\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.metrics         import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.pipeline        import Pipeline\n",
    "\n",
    "# Special code to ignore un-important warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure that output of plotting commands is displayed inline\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all constants\n",
    "\n",
    "SEED = 1  # base to generate a random number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1**: Data Preprocessing and EDA\n",
    "\n",
    "First, we would like to understand the main characteristics of the dataset. We might need to transform and clean some features before we can specify a statistical model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Load the credit data. Which features need to be one-hot encoded and why? Shall you do that for all categorical features ? Perform the required one-hot encoding and save your preprocessed data in a new data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use the *get_dummies()* function in Pandas."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Only SEX and MARRIAGE need to be one-hot encoded. Other features might look categorical \n",
    "(e.g. EDUCATION), but they don't need to be one-hot encoded as they are ordinal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('credit_data.csv')\n",
    "\n",
    "# One hot encoding of SEX and MARRIAGE status\n",
    "cols_to_transform = ['SEX', 'MARRIAGE']\n",
    "\n",
    "# CODE HERE\n",
    "# One hot encode the two columns SEX and MARRIAGE with the get_dummies() function in Pandas\n",
    "data_with_dummies = None\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert data_with_dummies is not None, 'HINT: you need to complete the code to proceed.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Compute the correlation of different features with respect to the target feature, using Pearson correlation. What are the most correlated features? Why do we not just use the most correlated features in the prediction task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Store the feature name and correlation in a dictionary structure. To compute the Pearson correlation of two dataframe columns A and B, run the following:\n",
    "\n",
    "*df['A'].corr(df['B'])*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation of different features with respect to target feature\n",
    "# Compute the absolute correlation\n",
    "\n",
    "corr_dic = {}\n",
    "\n",
    "for column in data_with_dummies.columns:\n",
    "    \n",
    "    correlation = data_with_dummies[column].corr(data_with_dummies['customer_default'])\n",
    "    corr_dic[column] = abs(correlation)\n",
    "    \n",
    "# Sort by descending correlation\n",
    "# Change dictionary to a data frame\n",
    "corr_df = pd.DataFrame.from_dict(corr_dic, orient = 'index', columns=['correlation'])\n",
    "corr_df.sort_values(by='correlation', ascending = False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The most correlated features are the payment status' features. \n",
    "However, other features are also correlated and removing them might destroy some signal. \n",
    "In machine learning, all features that might be correlated in some way are fed into the learner. \n",
    "We then let the learner decide which ones to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2**: Choosing Performance Metric and Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** What could be an appropriate performance metric for this problem? Discuss different options, starting with simple accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Consider the case where the distribution of different classes is not balanced. Also consider the cut-off threshold to separate positive and negative classes. Find a popular metric which is not sensitive to class distributions as well as cut-off threshold and use it for the rest of your analysis."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Plain accuracy has two main drawbacks for this application:\n",
    "(1) Since the distribution of the two target classes is not balanced, \n",
    "one can label all customers as non-default (i.e. 0) and still get a high accuracy.\n",
    "(2) Accuracy can lead to incoonsistent performance results if we change \n",
    "the probability threshold, which decides whether to predict default or non-default. \n",
    "It would be more sensible to use the probability of default instead of binary predictions.\n",
    "\n",
    "One useful metric for classification of an unbalanced target feature is known as \n",
    "area under the receiver operating characteristic (ROC) curve. \n",
    "This metric will tell you the probability that a randomly selected customer that \n",
    "defaulted is assigned a higher probability of default than a randomly selected customer \n",
    "that did not default. This metric will not suffer from class imbalance and is able to \n",
    "work with probabilistic instead of binary predictions.\n",
    "\n",
    "Here is a toy example of the ROC metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a toy data set\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.3, 0.8])\n",
    "\n",
    "# Compute the ROC score \n",
    "roc_auc_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** What could be an appropriate baseline model? What's the baseline performance? What performance do you expect? We will then compare this baseline to more complex models and see how they perform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Divide the data into train and test sets using the *train_test_split()* function in *sklearn.model_selection* package. Then fit an object from *DummyClassifier* class in *sklearn.dummy* package to the train set and evaluate the performance on the test set. What's the default value of the *strategy* parameter in the *DummyClassifier* function?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There are several option for a baseline model. One option is to label \n",
    "all test data points with the majority target value in training set, i.e. 1. \n",
    "The other is to assign each data point in the test set a postive label at random \n",
    "with a probability equal to the proportion of positive cases in the training set. \n",
    "We apply the second approach since it also predicts values for the less frequent target value. \n",
    "The corresponding value of the strategy parameter is 'stratified'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separates target and features\n",
    "\n",
    "features = data_with_dummies.drop(columns=['customer_default','ID'])\n",
    "target = data_with_dummies['customer_default']\n",
    "\n",
    "# CODE HERE\n",
    "# Create train and test sets (don't forget the seed!)\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert X_train is not None, 'HINT: you need to complete the code to proceed.'\n",
    "\n",
    "# Define the baseline classifier\n",
    "baseline_clf = DummyClassifier(strategy = 'stratified', random_state = SEED)\n",
    "\n",
    "# Fit the dummy classifier \n",
    "baseline_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict target probabilities of belonging to positive class\n",
    "y_pred = baseline_clf.predict_proba(X_test)\n",
    "\n",
    "# Compute area under the ROC curve\n",
    "score = roc_auc_score(y_test, y_pred[:,1])\n",
    "\n",
    "round(score, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3**: Prediction using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Predict the probability of default using logistic regression. Is standardization of data needed? What is the performance of your model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use *LogisticRegression* class from *sklearn.linear_model* package."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Standardization of data for a logistic regression is not needed. \n",
    "The coefficients will correspond to the magnitude of the data. \n",
    "However, standardization might improve convergence speed due to a smoother optimization surface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "# Define a logistic regression without regularization (don't forget the seed!)\n",
    "lr_clf = None\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert lr_clf is not None, 'HINT: you need to complete the code to proceed.'\n",
    "\n",
    "# Fit the dummy classifier \n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict target probabilities of belonging to the positive class\n",
    "y_pred = lr_clf.predict_proba(X_test)\n",
    "\n",
    "# Compute area under the ROC curve\n",
    "score = roc_auc_score(y_test, y_pred[:,1])\n",
    "\n",
    "round(score, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Try to think conceptually about this: what is the correct way to estimate performance of a classifier if we need to tune hyper-parameters? What issues should we avoid? Keep it simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Think about the problem that might arise if we tune hyper-parameters using the train and test sets."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We need to separate the training set into training and validation sets. \n",
    "We can then tune hyperparameters by measuring their performance on validation set.\n",
    "Finally, we can obtain the test score by training our model on the training set and \n",
    "measuring its out-of-sample performance on the test set. \n",
    "We typically divide all data into 60% training, 20% validation and 20% test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target and features into test and training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = SEED)\n",
    "X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = SEED)\n",
    "\n",
    "print('Dimensions of training set: {}'.format(X_train_train.shape))\n",
    "print('Dimensions of validation set: {}'.format(X_train_val.shape))\n",
    "print('Dimensions of testing set: {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3:** Tune the parameter C in the *LogisticRegression()* function and use *l2* penalty. Note that data standardization is required for regularized linear models. What is the correct method to implement the standardization? Obtain the performance of the model using best value of *C*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Think about the possible problem that might arise if we standardize all data (training and test) and then fit a classifier to it. Read about *Pipeline* class from *sklearn.pipeline* package and see how taking advantage of *Pipeline* can avoid the mentioned problem. Also, keep in mind that *C* is the inverse of regularization strength."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We should pay attention to the following aspects: \n",
    "(1) Regularized linear models assume standardized data.\n",
    "(2) The standardization and modeling step should be used in a single pipeline. \n",
    "The entire pipeline fits to the training set and transforms the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features and estimate the logistic regression in a single pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('lr_clf', LogisticRegression(random_state=SEED)))\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.set_params(lr_clf__penalty = 'l2')\n",
    "\n",
    "# Finding the best value of C using the validation set\n",
    "scores = []\n",
    "Cs = []\n",
    "\n",
    "for C in np.logspace(-4, 5, 10):\n",
    "    \n",
    "    pipeline.set_params(lr_clf__C = C) \n",
    "    pipeline.fit(X_train_train, y_train_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_train_val)\n",
    "    score = roc_auc_score(y_train_val, y_train_pred[:,1])\n",
    "    \n",
    "    scores.append(score)\n",
    "    Cs.append(C)\n",
    "\n",
    "best_score = scores.index(max(scores))\n",
    "best_C = Cs[best_score]\n",
    "\n",
    "print('Best C = {} with ROC AUC score = {}'.format(best_C, round(max(scores), 4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of the tuned model on test set \n",
    "pipeline.set_params(lr_clf__C = best_C)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_lr = pipeline.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, y_pred_lr[:,1])\n",
    "\n",
    "print('LR classifer ROC AUC with l2 regularization = {}'.format(round(score, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 4:** Check the coefficients and intercept of your tuned model under *l2* regularization. How do the coefficients compare to the correlations computed earlier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Extract the logistic regression from your pipeline and use a dictionary structure to store the feature names and coefficient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients\n",
    "lr_clf = pipeline.named_steps['lr_clf']\n",
    "coefficients = lr_clf.coef_[0]\n",
    "\n",
    "coef_dic = {}\n",
    "\n",
    "for i, col_name in enumerate(X_train_train.columns):\n",
    "    \n",
    "    coef = round(coefficients[i], 4)\n",
    "    coef_dic[col_name] = coef\n",
    "    \n",
    "# Sort by descending coefficient value\n",
    "# Change dictionary to a data frame\n",
    "coef_df = pd.DataFrame.from_dict(coef_dic, orient = 'index', columns=['coefficient'])\n",
    "coef_df['abs_coefficient'] = abs(coef_df['coefficient'])\n",
    "coef_df.sort_values(by='abs_coefficient', ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intercept: {}'.format(lr_clf.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 5:** Draw the ROC curve of your tuned model under *l2* regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use *roc_curve* function from *sklearn.metrics* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw ROC curve\n",
    "\n",
    "y_pred_lr = pipeline.predict_proba(X_test)\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_lr[:, 1])\n",
    "\n",
    "plt.figure(figsize = (12, 12))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_lr, tpr_lr, label='Logistic Regression')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 6:** Fit a logistic regression (with no l1 and l2 regularization) to the top 6 important features as defined by the logistic regression coefficients. How do you compare the performance of this model to the logistic regression model trained on all features?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Top 6 important features are the following: \n",
    "\n",
    "PAY_1     :  0.6450\n",
    "BILL_AMT1 : -0.3748\n",
    "PAY_AMT1  : -0.2132\n",
    "PAY_AMT2  : -0.1700\n",
    "BILL_AMT2 :  0.1396\n",
    "PAY_2     :  0.1303\n",
    "\n",
    "Note that these might vary with different seed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top6_features = ['PAY_1','BILL_AMT1','PAY_AMT1','PAY_AMT2','BILL_AMT2','PAY_2']\n",
    "\n",
    "X_train_top = X_train[top6_features] \n",
    "X_test_top = X_test[top6_features]\n",
    "\n",
    "top6_features_clf = LogisticRegression(C=10e9, random_state=SEED) \n",
    "top6_features_clf.fit(X_train_top, y_train)\n",
    "y_pred = top6_features_clf.predict_proba(X_test_top)\n",
    "score = roc_auc_score(y_test, y_pred[:,1])\n",
    "\n",
    "round(score, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 4**: Improving the Prediction with a Better Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** If model A performs better than all other models for our problem, does it mean that it is in general a better model? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Google *no-free-lunch* theorem."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "No, a model that performs well for a certain problem won't perform as well for all other problems. \n",
    "Actually, the no-free-lunch theorem states that if we consider all possible prediction problems, \n",
    "no single model will beat all others when evaluated using average performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** How can you make your performance results statistically more reliable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Consider selection bias in both hyperparameter tuning and testing steps."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We could avoid possible selection bias during hyperparameter tuning and performance evaluation:\n",
    "(1) During hyperparameter tuning, we can assess performance of our model on different \n",
    "validation sets and report the average performance.\n",
    "(2) During testing, we can assess performance of our model on different test sets and report the average.\n",
    "\n",
    "One solution to increase statistical model reliability is to apply k-fold cross validation. \n",
    "This method considers k different test sets such that each data point is assigned to only one test set. \n",
    "We will apply this method in the coming Demos and Projects.\n",
    "\n",
    "Here is an example of creating a k-fold schema for cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define toy data \n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([4, 4, 4, 2, 2, 4])\n",
    "\n",
    "# CODE HERE\n",
    "# Divide data into different folds without preserving target distribution (k = 2) (se the KFold function)\n",
    "kf = None\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert kf is not None, 'HINT: you need to complete the code to proceed.'\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    train = [y[i] for i in train_index] \n",
    "    test = [y[i] for i in test_index] \n",
    "    \n",
    "    print('Without stratification:', 'TRAIN:', train , 'TEST:', test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into diferent folds with preserving target distribution (k = 2)\n",
    "skf = StratifiedKFold(n_splits = 2)\n",
    "\n",
    "for train_index, test_index in skf.split(X = X, y = y):\n",
    "    \n",
    "    train = [y[i] for i in train_index] \n",
    "    test = [y[i] for i in test_index] \n",
    "    \n",
    "    print('With stratification:', 'TRAIN:', train, 'TEST:', test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bonus**: Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extensive notebook using the same dataset: https://www.kaggle.com/lucabasa/credit-card-default-a-very-pedagogical-notebook\n",
    "- Linking data science back to business objectives with a \"Profit Curve\": http://inseaddataanalytics.github.io/INSEADAnalytics/CourseSessions/ClassificationProcessCreditCardDefault.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
