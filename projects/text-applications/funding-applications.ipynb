{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DSFM Project**: Pre-Screen Funding Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creator: [Data Science for Managers - EPFL Program](https://www.dsfm.ch)  \n",
    "Source:  [https://github.com/dsfm-org/code-bank.git](https://github.com/dsfm-org/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Founded in 2000 by a high school teacher in the Bronx, DonorsChoose.org empowers public school teachers from across the country to request much-needed materials and experiences for their students. At any given time, there are thousands of classroom requests that can be brought to life with a gift of any amount.\n",
    "\n",
    "DonorsChoose.org receives hundreds of thousands of project proposals each year for classroom projects in need of funding. Right now, a large number of volunteers is needed to manually screen each submission before it's approved to be posted on the DonorsChoose.org website.\n",
    "\n",
    "In this project, we will predict the probability that a DonorsChoose.org project proposal submitted by a teacher will be approved, using the text of project descriptions as well as additional metadata about the project, teacher, and school. DonorsChoose.org can then use this information to identify projects most likely to need further review before approval. Hence, the organisation can save the time of volunteer reviewers so that they can focus on more promising projects.\n",
    "\n",
    "Your machine learning algorithm can help more teachers get funded more quickly, and with less cost to DonorsChoose.org, allowing them to channel even more funding directly to classrooms across the country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cached.imagescaler.hbpl.co.uk/resize/scaleWidth/580/cached.offlinehbpl.hbpl.co.uk/news/NST/C8B9CC1D-03B0-9B80-4CFE78B5B539240F.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "Image source: https://cached.imagescaler.hbpl.co.uk/resize/scaleWidth/580/cached.offlinehbpl.hbpl.co.uk/news/NST/C8B9CC1D-03B0-9B80-4CFE78B5B539240F.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will see, this dataset includes many different kinds of features with structured and unstructured data. You need to predict whether an application needs further study. To assess the quality of your predictions, consider the area under the curve (AUC) performance metric.\n",
    "\n",
    "The dataset consists of application materials (see *application_data.csv*) and resources requested (see *resource_data.csv*).\n",
    "\n",
    "The application materials data (see *application_data.csv*) contains the following features.\n",
    "\n",
    "| Feature name  | Description  |\n",
    "|----------------|--------------|\n",
    "| id  | Unique id of the project application    |\n",
    "| teacher_id    | id of the teacher submitting the application  |\n",
    "| teacher_prefix    | title of the teacher's name (Ms., Mr., etc.)    |\n",
    "| school_state    | US state of the teacher's school    |\n",
    "| project_submitted_datetime    | application submission timestamp    |\n",
    "| project_grade_category    | school grade levels (PreK-2, 3-5, 6-8, and 9-12)   |\n",
    "| project_subject_categories   | category of the project (e.g., \"Music & The Arts\")    |\n",
    "| project_subject_subcategories    | sub-category of the project (e.g., \"Visual Arts\")    |\n",
    "| project_title    | title of the project    |\n",
    "| project_essay_1    | first essay*   |\n",
    "| project_essay_2    | second essay*    |\n",
    "| project_essay_3    | third essay*   |\n",
    "| project_essay_4    | fourth essay*  |\n",
    "| project_resource_summary    | summary of the resources needed for the project    |\n",
    "| teacher_number_of_previously_posted_projects   | number of previously posted applications by the submitting teacher    |\n",
    "| project_is_approved    | whether DonorsChoose proposal was accepted (0=\"rejected\", 1=\"accepted\"); train.csv only    |\n",
    "\n",
    "\n",
    "\\*Note: Prior to May 17, 2016, the prompts for the essays were as follows:\n",
    "\n",
    "  * project_essay_1: \"Introduce us to your classroom\"  \n",
    "\n",
    "  * project_essay_2: \"Tell us more about your students\"  \n",
    "\n",
    "  * project_essay_3: \"Describe how your students will use the materials you're requesting\"  \n",
    "\n",
    "  * project_essay_4: \"Close by sharing why your project will make a difference\"  \n",
    "\n",
    "Starting on May 17, 2016, the number of essays was reduced from 4 to 2, and the prompts for the first 2 essays were changed to the following:\n",
    "\n",
    "  * project_essay_1: \"Describe your students: What makes your students special? Specific details about their background, your neighborhood, and your school are all helpful.\"  \n",
    "\n",
    "  * project_essay_2: \"About your project: How will these materials make a difference in your students' learning and improve their school lives?\"  \n",
    "\n",
    "For all projects with project_submitted_datetime of 2016-05-17 and later, the values of project_essay_3 and project_essay_4 will be missing (i.e. NaN).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposals also include resources requested. Each project may include multiple requested resources (see *resource_data.csv*). Each row in *resource_data.csv* corresponds to a resource, so multiple rows may tie to the same project by id.\n",
    "\n",
    "The resource data contains the following features.\n",
    "\n",
    "| Feature name  | Description  |\n",
    "|----------------|--------------|\n",
    "| id  | unique id of the project application; joins with test.csv. and train.csv on id   |\n",
    "| description    |  description of the resource requested     |\n",
    "| quantity   | quantity of resource requested      |\n",
    "| price    | price of resource requested      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset source: [Kaggle DonorsChoose.org Application Screening challenge](https://www.kaggle.com/c/donorschoose-application-screening/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is divided into several parts. For each part, you will have time to work on the question yourself. Feel free to go back to the Demo, use Google/Stackoverflow and work with your neighbour. Together, we will review and discuss the solution to each part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 0**: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all packages \n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Use short-hand for standard packages\n",
    "import pandas            as pd\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "import texthero          as hero\n",
    "\n",
    "# Import individual functions\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition           import PCA\n",
    "from sklearn.model_selection         import StratifiedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline                import Pipeline\n",
    "from scipy.sparse                    import hstack\n",
    "from bs4                             import BeautifulSoup\n",
    "from nltk.corpus                     import stopwords \n",
    "from nltk.stem                       import SnowballStemmer\n",
    "from xgboost                         import XGBClassifier\n",
    "from collections                     import Counter\n",
    "\n",
    "# Special code to ignore un-important warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure that output of plotting commands is displayed inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Set the maximum number of rows displayed \n",
    "pd.options.display.max_rows = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all constants\n",
    "\n",
    "SEED    = 41  # base to generate a random number\n",
    "SCORE   = 'roc_auc'\n",
    "FIGSIZE = (16, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1**: Data Preprocessing and EDA\n",
    "\n",
    "First, we have to download the data. Next, we would like to understand the main characteristics of the dataset. We might need to transform and clean some features before we can specify a statistical model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Unlike other projects, this project includes a training set too big for GitHub. Through the terminal lab of Jupyter lab, download the data using the *wget* command, unzip it using the *zip* command and check that it's in the root directory of the project. \n",
    "\n",
    "Locations : \n",
    "\n",
    "    Applications dataset: https://storage.googleapis.com/dsfm-datasets/text-applications/application_data.csv.zip\n",
    "    Resources dataset: https://storage.googleapis.com/dsfm-datasets/text-applications/resource_data.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use *wget* and *unzip* commands. Use *!* followed by a bash command in a cell to run a bash command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N 'https://storage.googleapis.com/dsfm-datasets/text-applications/application_data.csv.zip'\n",
    "!wget -N 'https://storage.googleapis.com/dsfm-datasets/text-applications/resource_data.csv.zip'\n",
    "!unzip -o application_data.csv.zip\n",
    "!unzip -o resource_data.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Load the two datasets and investigate their features. What could be a unifying strategy to create the same \"project essay\" columns? Do the essays share common topics? Implement your strategy. We will deal with missing values afterwards."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Consider the description of essays 1 to 4.\n",
    "One could unify them by merging essays 1 and 2 as 'envirenment essays' as well as \n",
    "merging essays 3 with 4 as 'project essyas'. These two types of essays correspond respectively \n",
    "to the project_essay_1 and project_essay_2 columns of projects, submitted on or after 2016-05-17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load applications\n",
    "applications = pd.read_csv('application_data.csv')\n",
    "\n",
    "print('Application dataset')\n",
    "print(applications.shape)\n",
    "applications.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load resources\n",
    "resources = pd.read_csv('resource_data.csv')\n",
    "\n",
    "print('Resources dataset')\n",
    "print(resources.shape)\n",
    "resources.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two new features to represent essays: one about the class environment, the other about the project\n",
    "\n",
    "print('Shape before: {}'.format(applications.shape))\n",
    "\n",
    "# Fill new environment column\n",
    "project_essay_env = []\n",
    "\n",
    "for i, row in applications.iterrows():\n",
    "    \n",
    "    if pd.isna(row[\"project_essay_3\"]) and pd.isna(row[\"project_essay_4\"]):\n",
    "        project_essay_env.append(row['project_essay_1'])\n",
    "        \n",
    "    else:\n",
    "        project_essay_env.append(str(row[\"project_essay_1\"]) + \" \" + str(row[\"project_essay_2\"]))\n",
    "\n",
    "assert len(project_essay_env) == applications.shape[0], 'New column length does not match'\n",
    "applications['project_essay_env'] = project_essay_env\n",
    "\n",
    "# Fill new project column\n",
    "project_essay_proj = []\n",
    "\n",
    "for i, row in applications.iterrows():\n",
    "    \n",
    "    if pd.isna(row[\"project_essay_3\"]) and pd.isna(row[\"project_essay_4\"]):\n",
    "        project_essay_proj.append(row['project_essay_2'])\n",
    "        \n",
    "    else:\n",
    "        project_essay_proj.append(str(row[\"project_essay_3\"]) + \" \" + str(row[\"project_essay_4\"]))\n",
    "\n",
    "assert len(project_essay_proj) == applications.shape[0], 'New column length does not match'\n",
    "applications['project_essay_proj'] = project_essay_proj\n",
    "\n",
    "print('Shape after: {}'.format(applications.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant essay features\n",
    "applications.drop(columns=['project_essay_1', 'project_essay_2', 'project_essay_3', 'project_essay_4'], inplace=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "applications.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Check if there is still any missing value\n",
    "print('Shape: {}\\n'.format(applications.shape))\n",
    "print(applications.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3:** Merge the resources and application datasets on the *id* feature. You can aggregate the total cost of requested resources for every project and merge it into applications dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use *groupby* and *agg* functions of dataframe to create aggregates. Use *merge* function of *Pandas* to merge two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create useful feature out of resources: total requested budget\n",
    "resources['total_budget'] = resources['quantity'] * resources['price']\n",
    "\n",
    "# Drop non-informative features\n",
    "resources.drop(columns=['quantity','price', 'description'], inplace=True)\n",
    "\n",
    "# Aggregate total budget by id\n",
    "resources_agg = resources.groupby('id', as_index=False).agg({\"total_budget\": \"sum\"})\n",
    "print('Resources shape: {}'.format(resources_agg.shape))\n",
    "\n",
    "# CODE HERE \n",
    "# Merge two datasets\n",
    "applications_full = None\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert applications_full is not None, 'HINT: you need to complete the code to proceed.'\n",
    "\n",
    "# Check data type of new datasets\n",
    "print('Merged shape: {}\\n'.format(applications_full.shape))\n",
    "applications_full.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 4:**  Separate your merged dataset into features and target variables and visualize the distribution of the target. Why is area under the ROC curve (AUC) a suitable classification metric? Why can we not use accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split target and feature\n",
    "target   = applications_full['project_is_approved']\n",
    "features = applications_full.drop(columns=['project_is_approved','id', 'project_submitted_datetime'])\n",
    "\n",
    "print('Target: {}'.format(target.shape))\n",
    "print('Features: {}'.format(features.shape))\n",
    "\n",
    "# Plot histogram of target\n",
    "target.hist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Area under ROC curve could be a good metric due to being insensitive to skewness of target values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2**: Encode Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** What would be a possible issue with one-hot encoding of teacher_id column? Should we keep this feature? There might be another, more useful teacher-specific feature we can use. Which one?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "One-hot encoding teacher ids would create a lot of useless binary features.\n",
    "However, one useful teacher-specific information is contained in the 'teacher_number_of_previously_posted_projects' feature.\n",
    "Hence, one might drop the 'teacher_id' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.drop(columns = ['teacher_id'], inplace = True)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Encode these categorical features using one-hot-encoding: ['teacher_prefix', 'school_state', 'project_grade_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_transform = ['teacher_prefix', 'school_state', 'project_grade_category']\n",
    "\n",
    "features = pd.get_dummies(data = features, columns = cols_to_transform, drop_first=True)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3:** What could be the issue with simply one-hot-encoding of *project_subject_category* and *project_subject_subcategory* features? Come up with a sensible approach and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with the subject categories\n",
    "\n",
    "# Extract only the categories column\n",
    "features_categories = features[['project_subject_categories']].copy()\n",
    "features_categories['project_subject_categories_list'] = features_categories['project_subject_categories'].str.split(', ')\n",
    "features_categories.drop(columns=['project_subject_categories'], inplace=True)\n",
    "\n",
    "# Explode the column\n",
    "features_categories = features_categories.explode('project_subject_categories_list')\n",
    "print('Number of categories: {}'.format(len(set(features_categories['project_subject_categories_list'].values))))\n",
    "Counter(features_categories['project_subject_categories_list'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the applications now and group them by index values\n",
    "features_categories = pd.get_dummies(features_categories, columns=['project_subject_categories_list'])\n",
    "features_categories = features_categories.groupby(features_categories.index).sum()\n",
    "features_categories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now repeat the process for subject subcategories\n",
    "\n",
    "# Extract only the categories column\n",
    "features_subcategories = features[['project_subject_subcategories']].copy()\n",
    "features_subcategories['project_subject_subcategories_list'] = features_subcategories['project_subject_subcategories'].str.split(', ')\n",
    "features_subcategories.drop(columns=['project_subject_subcategories'], inplace=True)\n",
    "\n",
    "# Explode the column\n",
    "features_subcategories = features_subcategories.explode('project_subject_subcategories_list')\n",
    "print('Number of sub-categories: {}'.format(len(set(features_subcategories['project_subject_subcategories_list'].values))))\n",
    "Counter(features_subcategories['project_subject_subcategories_list'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the applications now and group them by index values\n",
    "features_subcategories = pd.get_dummies(features_subcategories, columns=['project_subject_subcategories_list'])\n",
    "features_subcategories = features_subcategories.groupby(features_subcategories.index).sum()\n",
    "features_subcategories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge one-hot encoded categories and sub-categories with the dataframe\n",
    "\n",
    "# Categories\n",
    "features = features.merge(features_categories, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# CODE HERE\n",
    "# Concatenate sub-category dummies into existing features data frame\n",
    "features = None\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert features is not None, 'HINT: you need to complete the code to proceed.'\n",
    "\n",
    "print(features.shape)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original columns\n",
    "features.drop(columns=['project_subject_categories','project_subject_subcategories'], inplace=True)\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3**: Encode Text\n",
    "\n",
    "In this part, we use a new data science package called [TextHero](https://github.com/jbesomi/texthero) for easy text pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all textual features\n",
    "features['app_text'] = features['project_title'] + ' ' + features['project_essay_env'] + ' ' + features['project_essay_proj'] + ' ' + features['project_resource_summary']\n",
    "features.drop(columns = ['project_title',\n",
    "                         'project_resource_summary',\n",
    "                         'project_essay_env',\n",
    "                         'project_essay_proj'], inplace = True)\n",
    "\n",
    "\n",
    "features['app_text_clean'] = features['app_text'].pipe(hero.clean)\n",
    "features_pca = features['app_text_clean'].pipe(hero.tfidf, max_features=500).pipe(hero.pca, n_components=50)\n",
    "textual_features_pca = pd.DataFrame(dict(zip(features_pca.index, features_pca.values))).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 4:** You can think of generating some new features out of text which are beneficial for the target application in mind. In particular, add the number of words in application text as a new feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "# Generate 'number of words' feature\n",
    "num_words = None\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert num_words is not None, 'HINT: you need to complete the code to proceed.'\n",
    "\n",
    "features['num_words'] = num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 4**: Prepare Features and Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Organize your features into three dataframes of *textual_features* (projected to 50 PCA dimensions), *non_textual_features* and *all_features*. Check the data types of all features: are they all numeric? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textual features\n",
    "textual_features_pca.columns = ['dim_' + str(col) for col in textual_features_pca.columns]\n",
    "textual_features = textual_features_pca.copy()\n",
    "\n",
    "# CODE HERE\n",
    "# Only keep non-textual featues, i.e. drop 2 application text columns\n",
    "non_textual_features = None\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert non_textual_features is not None, 'HINT: you need to complete the code to proceed.'\n",
    "\n",
    "# All features: are all types numeric? \n",
    "all_features = pd.concat([non_textual_features, textual_features], axis = 1)\n",
    "all_features.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Come up with a conceptual baseline to compare with your models. No coding required."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "An AUC score of 0.5 could be conisidered as a baseline, which corresponds to random guessing.\n",
    "It means that the probability that a randomly selected application from accepted set\n",
    "has a higher score than a randomly selected application from rejected set is 50 %."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 5**: Gradient Boosting Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Fit a gradient boosting tree model from the the *XGBoost* library to your data and tune the value of the *n_estimators* parameter, the number of trees to fit. Use nested cross-validation function with 3 splits each in the inner and outer folds. Assess the performance of your model when tuning the *n_estimators* parameter and using the following sets of features:\n",
    "\n",
    "1. Non-textual features\n",
    "2. Textual features\n",
    "3. All features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use the following function for nested cross-validation (Try to understand it before using it):\n",
    "\n",
    "    from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score\n",
    "\n",
    "    def nested_cv(X, y, est_pipe, p_grid, p_score, n_splits_inner = 3, n_splits_outer = 3, n_cores = 1, seed = 0):\n",
    "\n",
    "        # cross-validation schema for inner and outer loops\n",
    "        inner_cv = StratifiedKFold(n_splits = n_splits_inner, shuffle = True, random_state = seed)\n",
    "        outer_cv = StratifiedKFold(n_splits = n_splits_outer, shuffle = True, random_state = seed)\n",
    "\n",
    "        # grid search to tune hyper parameters\n",
    "        est = GridSearchCV(estimator = est_pipe, param_grid = p_grid, cv = inner_cv, scoring = p_score, n_jobs = n_cores)\n",
    "\n",
    "        # nested CV with parameter optimization\n",
    "        nested_scores = cross_val_score(estimator = est, X = X, y = y, cv = outer_cv, scoring = p_score, n_jobs = n_cores)\n",
    "\n",
    "        print('Average score: %0.4f (+/- %0.4f)' % (nested_scores.mean(), nested_scores.std() * 1.96))\n",
    "        \n",
    "Moreover use the *XGBClassifier* class from *xgboost* package, which has a similar interface to other sklearn classifiers. *XGboost* library includes high perfromance implementations of gradient boosting trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested cross validation function\n",
    "\n",
    "def nested_cv(X, y, est_pipe, p_grid, p_score, n_splits_inner = 3, n_splits_outer = 3, n_cores = 1, seed = SEED):\n",
    "\n",
    "    # Cross-validation schema for inner and outer loops\n",
    "    inner_cv = StratifiedKFold(n_splits = n_splits_inner, shuffle = True, random_state = seed)\n",
    "    outer_cv = StratifiedKFold(n_splits = n_splits_outer, shuffle = True, random_state = seed)\n",
    "    \n",
    "    # Grid search to tune hyper parameters\n",
    "    est = GridSearchCV(estimator = est_pipe, param_grid = p_grid, cv = inner_cv, scoring = p_score, n_jobs = n_cores)\n",
    "\n",
    "    # Nested CV with parameter optimization\n",
    "    nested_scores = cross_val_score(estimator = est, X = X, y = y, cv = outer_cv, scoring = p_score, n_jobs = n_cores)\n",
    "    \n",
    "    print('Average score: %0.4f (+/- %0.4f)' % (nested_scores.mean(), nested_scores.std() * 1.96))\n",
    "\n",
    "# Define pipeline\n",
    "estimators = []\n",
    "estimators.append(('xgb_clf', XGBClassifier()))\n",
    "xgb_pipe = Pipeline(estimators)\n",
    "xgb_pipe.set_params(xgb_clf__n_jobs = -1)\n",
    "xgb_pipe.set_params(xgb_clf__random_state = SEED)\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"xgb_clf__n_estimators\": [int(i) for i in np.linspace(10, 100, 10)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Non textual features\n",
    "nested_cv(X = non_textual_features, y = target, est_pipe = xgb_pipe, p_grid = p_grid, p_score = SCORE, n_cores = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# CODER HERE\n",
    "# Textual features (copy what you ran in the cell just above, but with textual features only)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# All features\n",
    "nested_cv(X = all_features, y = target, est_pipe = xgb_pipe, p_grid = p_grid, p_score = SCORE, n_cores = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Print a list of the 10 most important features considering all features as pedictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "\n",
    "xgb_clf = xgb_pipe.named_steps['xgb_clf']\n",
    "xgb_clf.fit(all_features, target)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for index in reversed(np.argsort(xgb_clf.feature_importances_)):\n",
    "    \n",
    "    print(all_features.columns[index].ljust(50) , ':', xgb_clf.feature_importances_[index])\n",
    "    i = i + 1\n",
    "\n",
    "    if (i == 10):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bonus**: Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Further ideas for feature engineering: https://www.kaggle.com/shivamb/extensive-text-data-feature-engineering\n",
    "- Winning submission on Kaggle (don't worry if you don't understand everything): https://www.kaggle.com/shadowwarrior/1st-place-solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EPFL",
   "language": "python",
   "name": "epfl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
