{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DSFM Project**: Predict House Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creator: [Data Science for Managers - EPFL Program](https://www.dsfm.ch)  \n",
    "Source:  [https://github.com/dsfm-org/code-bank.git](https://github.com/dsfm-org/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will predict the sale price of houses located in Ames, Iowa, based on sevaral features. This is a regression problem as we are predicting a continuous outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.needpix.com/rsynced_images/new-home-for-sale-1405784329d8m.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "Image source: https://storage.needpix.com/rsynced_images/new-home-for-sale-1405784329d8m.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Housing Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 1460 houses with 80 features, described in the *data_description.txt* file in this directory. The target feature is the *SalesPrice* in US Dollar. As you will see, this dataset includes numerous features of different kinds: a good playground for feature engineering and a good lesson for carefully understanding the dataset. Furthermore, this project shows the power of tree-based models, such as random forests and gradient boosting trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset source: [Kaggle Ames Housing Dataset challenge](https://www.kaggle.com/prevek18/ames-housing-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the quality of your predictions, consider the root mean square error (RMSE) of the logarithm of the predicted value and the logarithm of the observed *SalesPrice*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is divided into several parts. For each part, you will have time to work on the question yourself. Feel free to go back to the Demo, use Google/Stackoverflow and work with your neighbour. Together, we will review and discuss the solution to each part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 0**: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all packages\n",
    "\n",
    "# Use short-hand for standard packages\n",
    "import pandas            as pd\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sns\n",
    "\n",
    "# Import individual functions\n",
    "from sklearn.impute          import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, KFold, validation_curve, learning_curve\n",
    "from sklearn.ensemble        import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.dummy           import DummyRegressor\n",
    "from sklearn.metrics         import mean_squared_error\n",
    "from sklearn.linear_model    import Ridge, Lasso\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.inspection      import permutation_importance\n",
    "from math                    import sqrt\n",
    "\n",
    "# Special code to ignore un-important warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure that output of plotting commands is displayed inline\n",
    "%matplotlib inline \n",
    "\n",
    "# Set the maximum number of rows displayed \n",
    "pd.options.display.max_rows = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all constants\n",
    "\n",
    "SEED = 0  # base to generate a random number\n",
    "\n",
    "# Performance metric and number of CV splits\n",
    "SCORE    = 'neg_mean_squared_error'\n",
    "N_SPLITS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1**: Data Preprocessing and EDA\n",
    "\n",
    "First, we would like to understand the main characteristics of the dataset. We might need to transform and clean some features before we can specify a statistical model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Investigate observations with missing/null values. For each feature, what patterns can you find? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: For every feature, calculate how many observations have missing values. Also calculate the percentage of observations where each feature is missing. You can also create a heatmap using *seaborn.heatmap()* function and feed it with *dataframe.isnull()*. This approach returns an object of the same size with boolean values to indicate if values are missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "# Load the house_data.csv file as a Pandas data frame \n",
    "data = None\n",
    "print(data.shape)\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert data is not None, 'HINT: you need to complete the code to proceed.'\n",
    "\n",
    "# Check percentage of missing data in each feature\n",
    "# Count missing values and sort (descending)\n",
    "total = data.isnull().sum()\n",
    "total = total.sort_values(ascending=False)\n",
    "\n",
    "# Compute percentage of missing values\n",
    "percent = data.isnull().sum() / data.isnull().count()\n",
    "percent = percent.sort_values(ascending=False)\n",
    "\n",
    "# Concatenate\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** What's an appropriate strategy to deal with missing/null values? Implement your strategy and save a version of the dataset with no missing values in a new dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: For the features with many missing values, drop it using *dataframe.drop()*. You have to define a threshold representing \"many missing values\". Have a look at the number of missing values per feature computed above to find an appropriate threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete features with more than 200 missing values\n",
    "data = data.drop((missing_data[missing_data['Total'] > 200]).index, 1)\n",
    "\n",
    "# How did the shape change? \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with most frequent value for categorical data\n",
    "imp_categorical = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Impute missing values with the mean for continuous data\n",
    "imp_continuous  = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Impute each column with missing values\n",
    "for column in data.columns:\n",
    "    \n",
    "    # Categorical\n",
    "    if data[column].dtype == np.dtype('O'):\n",
    "        data[column] = imp_categorical.fit_transform(data[column].values.reshape(-1, 1))\n",
    "    \n",
    "    # Continous\n",
    "    else:\n",
    "        data[column] = imp_continuous.fit_transform(data[column].values.reshape(-1, 1))\n",
    "        \n",
    "# Check number of null values in the new data frame\n",
    "print(data.isnull().sum().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3:** After imputing missing values, skim through the description of features in the .txt file. Simply mark each feature as numeric (N), categorical (C) or ordinal (O) in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Categorical features have no ordering semantics while ordinal features have ordering semantics.\n",
    "\n",
    "Apply the *value_counts()* function to a data frame column to count distinct categorical/ordinal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['BldgType'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature name   | Type  |\n",
    "|----------------|-------|\n",
    "| Id             | C     |\n",
    "| MSSubClass     | C     |\n",
    "| MSZoning       | C     |\n",
    "| LotFrontage    | N     |\n",
    "| LotArea        | N     |\n",
    "| Street         | O     |\n",
    "| Alley          | O     |\n",
    "| LotShape       | C     |\n",
    "| LandContour    | C     |\n",
    "| Utilities      | O     |\n",
    "| LotConfig      | C     |\n",
    "| LandSlope      | C     |\n",
    "| Neighborhood   | C     |\n",
    "| Condition1     | C     |\n",
    "| Condition2     | C     |\n",
    "| BldgType       | C     |\n",
    "| HouseStyle     | C     |\n",
    "| OverallQual    | N     |\n",
    "| OverallCond    | N     |\n",
    "| YearBuilt      | N     |\n",
    "| YearRemodAdd   | N     |\n",
    "| RoofStyle      | C     |\n",
    "| RoofMatl       | C     |\n",
    "| Exterior1st    | C     |\n",
    "| Exterior2nd    | C     |\n",
    "| MasVnrType     | C     |\n",
    "| MasVnrArea     | N     |\n",
    "| ExterQual      | O     |\n",
    "| ExterCond      | O     |\n",
    "| Foundation     | C     |\n",
    "| BsmtQual       | O     |\n",
    "| BsmtCond       | O     |\n",
    "| BsmtExposure   | C     |\n",
    "| BsmtFinType1   | O     |\n",
    "| BsmtFinSF1     | N     |\n",
    "| BsmtFinType2   | O     |\n",
    "| BsmtFinSF2     | N     |\n",
    "| BsmtUnfSF      | N     |\n",
    "| TotalBsmtSF    | N     |\n",
    "| Heating        | C     |\n",
    "| HeatingQC      | O     |\n",
    "| CentralAir     | C     |\n",
    "| Electrical     | C     |\n",
    "| 1stFlrSF       | N     |\n",
    "| 2ndFlrSF       | N     |\n",
    "| LowQualFinSF   | N     |\n",
    "| GrLivArea      | N     |\n",
    "| BsmtFullBath   | N     |\n",
    "| BsmtHalfBath   | N     |\n",
    "| FullBath       | N     |\n",
    "| HalfBath       | N     |\n",
    "| BedroomAbvGr   | N     |\n",
    "| KitchenAbvGr   | N     |\n",
    "| KitchenQual    | O     |\n",
    "| TotRmsAbvGrd   | N     |\n",
    "| Functional     | O     |\n",
    "| Fireplaces     | N     |\n",
    "| FireplaceQu    | O     |\n",
    "| GarageType     | C     |\n",
    "| GarageYrBlt    | N     |\n",
    "| GarageFinish   | O     |\n",
    "| GarageCars     | N     |\n",
    "| GarageArea     | N     |\n",
    "| GarageQual     | O     |\n",
    "| GarageCond     | O     |\n",
    "| PavedDrive     | O     |\n",
    "| WoodDeckSF     | N     |\n",
    "| OpenPorchSF    | N     |\n",
    "| EnclosedPorch  | N     |\n",
    "| 3SsnPorch      | N     |\n",
    "| ScreenPorch    | N     |\n",
    "| PoolArea       | N     |\n",
    "| PoolQC         | O     |\n",
    "| Fence          | O     |\n",
    "| MiscFeature    | C     |\n",
    "| MiscVal        | N     |\n",
    "| MoSold         | C     |\n",
    "| YrSold         | N     |\n",
    "| SaleType       | C     |\n",
    "| SaleCondition  | C     |\n",
    "| SalePrice      | N     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 4:** How can we numerically represent categorical and ordinal features? Perform the required feature encodings and save your pre-processed data into a new data frame. Check the data type of all features at the end and make sure they are all numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use the *get_dummies()* function in Pandas to transform categorical features into one-hot encodings. For ordinal features, try to transform them using a dictionary which maps old, non-numeric labels into new numeric labels, preserving ordering semantics. Use the *replace()* function in Pandas to transform ordinal features. Use the descriptions of features .txt file to build the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of categorical features\n",
    "\n",
    "cols_to_transform = ['MSSubClass', 'MSZoning', 'LotShape', 'LandContour', 'LotConfig', \n",
    "                    'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
    "                    'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n",
    "                    'Foundation', 'Heating', 'CentralAir', \n",
    "                    'Electrical', 'MoSold', 'SaleType', 'SaleCondition', 'GarageType',\n",
    "                    'BsmtExposure', 'MasVnrType']\n",
    "\n",
    "data = pd.get_dummies(data = data, columns = cols_to_transform )\n",
    "\n",
    "# Label encoding of ordinal features\n",
    "\n",
    "dic_street = {'Grvl': 1, 'Pave': 2}\n",
    "data.replace({'Street':dic_street},inplace=True)\n",
    "\n",
    "dic_utilities = {'AllPub': 4, 'NoSeWa': 2}\n",
    "data.replace({'Utilities':dic_utilities},inplace=True)\n",
    "\n",
    "dic_exterqual = {'Gd':4, 'TA':3, 'Ex':5, 'Fa':2}\n",
    "data.replace({'ExterQual':dic_exterqual},inplace=True)\n",
    "\n",
    "dic_extercond = {'Gd':4, 'TA':3, 'Ex':5, 'Fa':2, 'Po':1}\n",
    "data.replace({'ExterCond':dic_extercond}, inplace=True)\n",
    "\n",
    "dic_heatingqc = {'Gd':4, 'TA':3, 'Ex':5, 'Fa':2, 'Po':1}\n",
    "data.replace({'HeatingQC':dic_heatingqc}, inplace=True)\n",
    "\n",
    "dic_kitchenqual = {'Gd':4, 'TA':3, 'Ex':5, 'Fa':2, 'Po':1}\n",
    "data.replace({'KitchenQual':dic_kitchenqual}, inplace=True)\n",
    "\n",
    "dic_functional = {'Typ':8, 'Min1':7, 'Maj1':4, 'Min2':6, 'Mod':5, 'Maj2':3, 'Sev':2}\n",
    "data.replace({'Functional':dic_functional}, inplace=True)\n",
    "\n",
    "dic_paveddrive = {'Y':3, 'N':1, 'P':2}\n",
    "data.replace({'PavedDrive':dic_paveddrive}, inplace=True)\n",
    "\n",
    "dic_garagecond = {'Gd':4, 'TA':3, 'Ex':5, 'Fa':2, 'Po':1}\n",
    "data.replace({'GarageCond':dic_garagecond}, inplace=True)\n",
    "\n",
    "dic_garagefinish = {'RFn':3, 'Unf':2, 'Fin':4, 'NA':1}\n",
    "data.replace({'GarageFinish':dic_garagefinish}, inplace=True)\n",
    "\n",
    "dic_garagequal = {'Gd':4, 'TA':3, 'Ex':5, 'Fa':2, 'Po':1}\n",
    "data.replace({'GarageQual':dic_garagequal}, inplace=True)\n",
    "\n",
    "dic_bsmtcond = {'Gd':4, 'TA':3, 'Ex':5, 'Fa':2, 'Po':1}\n",
    "data.replace({'BsmtCond':dic_bsmtcond}, inplace=True)\n",
    "\n",
    "dic_bsmtqual = {'Gd':4, 'TA':3, 'Ex':5, 'Fa':2, 'Po':1}\n",
    "data.replace({'BsmtQual':dic_bsmtqual}, inplace=True)\n",
    "\n",
    "dic_bsmtfintype1 = {'GLQ':7, 'ALQ':6, 'Unf':2, 'Rec':4, 'BLQ':5, 'LwQ':3, 'NA':1}\n",
    "data.replace({'BsmtFinType1':dic_bsmtfintype1}, inplace=True)\n",
    "\n",
    "dic_bsmtfintype2 = {'GLQ':7, 'ALQ':6, 'Unf':2, 'Rec':4, 'BLQ':5, 'LwQ':3, 'NA':1}\n",
    "data.replace({'BsmtFinType2':dic_bsmtfintype2}, inplace=True)\n",
    "\n",
    "# adjust data types to float \n",
    "data['BsmtQual'] = data['BsmtQual'].astype(float)\n",
    "data['BsmtFinType1'] = data['BsmtFinType1'].astype(float)\n",
    "data['BsmtFinType2'] = data['BsmtFinType2'].astype(float)\n",
    "data['KitchenQual'] = data['KitchenQual'].astype(float)\n",
    "data['GarageFinish'] = data['GarageFinish'].astype(float)\n",
    "\n",
    "# Check data types of preprocessed dataframe: Are they all numeric?\n",
    "# data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 5:** Why do you think the root mean square error (RMSE) between the log of predicted price and log of observed price is a better metric than a RMSE applied directly on observed and predicted prices? Visualize the distributions of the target feature and the log-transformed target feature. Also, separate your data into features and the target, where your target is the log of *SalePrice*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use *numpy.log1p()* to log-transform the target feature. Use *seaborn.distplot()* to visualize distributions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Log transforming a positively skewed numeric target will diminish \n",
    "its skewness (loosely speaking, making it look more like a Normal distribution). \n",
    "As a consequence, errors in predicting expensive and cheap houses \n",
    "will have more similar effects on the result of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sales price\n",
    "fig, ax = plt.subplots(figsize = (14, 8))\n",
    "sns.distplot(data['SalePrice'])\n",
    "plt.show()\n",
    "\n",
    "# Plot log-transformed sales price\n",
    "fig, ax = plt.subplots(figsize = (14, 8))\n",
    "sns.distplot(np.log1p(data['SalePrice']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target \n",
    "features = data.drop(columns = ['Id','SalePrice'])\n",
    "target   = np.log1p(data['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 6:** Visualize the correlation of different features with the target and sort them based on the absolute value of correlation. What are the top 10 correlated values? Do they match your expectations? Why? Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Compute pearson correlation of two dataframe columns A and B as follows:\n",
    "\n",
    "*df['A'].corr(df['B'])*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of different features with respect to target\n",
    "\n",
    "corr_dic = dict()\n",
    "\n",
    "# Go through each column\n",
    "for column in data.columns:\n",
    "    \n",
    "    correlation = data[column].corr(data['SalePrice'])\n",
    "    corr_dic[column] = abs(correlation)\n",
    "\n",
    "# Print top 10 correlated features\n",
    "i = 0\n",
    "for w in sorted(corr_dic, key=corr_dic.get, reverse=True):\n",
    "    \n",
    "    print('{}'.format(w).ljust(15), corr_dic[w])\n",
    "    \n",
    "    i = i + 1\n",
    "    if (i == 10):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2**: Preparing Baseline and Cross-Validation Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Divide your cleaned dataset into a training set and test set with ratio of 1 to 4. Dont forget to randomize your data beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Divide data into training and test sets using *train_test_split()* function in *sklearn.model_selection* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target and features into test and training sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = SEED)\n",
    "\n",
    "print('Training set: \\t{}'.format(X_train.shape))\n",
    "print('Test set: \\t{}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** First, compute a baseline model to compare more advanced models to later on. Consider root mean square error (RMSE) as your regression performance metric. This ensures that the error has the same unit as the target. What's the baseline performance? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Fit an object from *DummyRegressor* class in *sklearn.dummy* package to training set and evaluate the performance on the test set. Keep the default strategy of *DummyRegressor*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline regressor\n",
    "baseline_reg = DummyRegressor()\n",
    "\n",
    "# Fit the dummy regressor\n",
    "baseline_reg.fit(X_train, y_train)\n",
    "\n",
    "# CODE HERE\n",
    "# Predict log of house prices\n",
    "y_pred = None\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert y_pred is not None, 'HINT: you need to complete the code to proceed.'\n",
    "\n",
    "# Compute root mean square error (log-transformed)\n",
    "rmse_baseline = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('Log-transformed RMSE: ', rmse_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3**: Prediction using Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Fit a ridge regression model to your data and tune the value of *alpha* parameter, the regularization strength. Note that your data should be standardized. Use a validation curve to visualize your tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use the *Pipeline* class from *sklearn.pipeline* to make a pipeline of *StandardScaler* and *Ridge* transformers. Use the *validation_curve()* function from *sklearn.model_selection* on the training set and tune the value of *alpha*.\n",
    "\n",
    "To visualize your tuning process, use the following function:\n",
    "\n",
    "    def plot_validation_curve(train_scores,cv_scores,x_data,scale='lin',title='',y_label='',x_label=''):\n",
    "\n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        train_scores_std = np.std(train_scores, axis=1)\n",
    "        cv_scores_mean = np.mean(cv_scores, axis=1)\n",
    "        cv_scores_std = np.std(cv_scores, axis=1)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize = (14, 8))\n",
    "        plt.title(title)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        lw = 2\n",
    "\n",
    "        plt.fill_between(x_data, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.2, color=\"r\", lw=lw)\n",
    "        plt.fill_between(x_data, cv_scores_mean - cv_scores_std, cv_scores_mean + cv_scores_std, alpha=0.2, color=\"g\", lw=lw)\n",
    "\n",
    "        if (scale == 'lin'):\n",
    "            plt.plot(x_data, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "            plt.plot(x_data, cv_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n",
    "        elif (scale == 'log'):\n",
    "            plt.semilogx(x_data, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "            plt.semilogx(x_data, cv_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n",
    "        plt.grid()\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('ridge_reg', Ridge()))\n",
    "ridge_pipe = Pipeline(estimators)\n",
    "ridge_pipe.set_params(ridge_reg__random_state = SEED)\n",
    "\n",
    "# CV schema\n",
    "cv_schema = KFold(n_splits = N_SPLITS, random_state = SEED)\n",
    "\n",
    "# Tune model against a single hyper-parameter, alpha\n",
    "tuning_param = 'ridge_reg__alpha'\n",
    "tuning_param_range = np.logspace(-5, 3, 10)\n",
    "\n",
    "# Tune hyper-parameter using validation curve\n",
    "train_scores_val, cv_scores_val = validation_curve(\n",
    "    ridge_pipe, X_train, y_train, param_name = tuning_param, param_range = tuning_param_range,\n",
    "    cv = cv_schema, scoring = SCORE, n_jobs = 1)\n",
    "\n",
    "# Define a function to facilitate drawing validation curves\n",
    "def plot_validation_curve(train_scores, cv_scores, x_data, scale='lin', title='', y_label='', x_label=''):\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    cv_scores_mean = np.mean(cv_scores, axis=1)\n",
    "    cv_scores_std = np.std(cv_scores, axis=1)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (14, 8))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    lw = 2\n",
    "    \n",
    "    plt.fill_between(x_data, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.2, color=\"r\", lw=lw)\n",
    "    plt.fill_between(x_data, cv_scores_mean - cv_scores_std, cv_scores_mean + cv_scores_std, alpha=0.2, color=\"g\", lw=lw)\n",
    "    \n",
    "    if (scale == 'lin'):\n",
    "        plt.plot(x_data, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "        plt.plot(x_data, cv_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n",
    "        \n",
    "    elif (scale == 'log'):\n",
    "        plt.semilogx(x_data, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "        plt.semilogx(x_data, cv_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n",
    "        \n",
    "    plt.grid()\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "# Draw validation curve\n",
    "plot_validation_curve(-train_scores_val, -cv_scores_val, tuning_param_range, scale='log', \n",
    "                      title='validation curve', y_label='MSE', x_label='alpha')\n",
    "\n",
    "# Obtain the best value of the hyper parameter\n",
    "best_param_val = tuning_param_range[np.argmin(np.mean(-cv_scores_val, axis=1))]\n",
    "print('Best alpha: {}'.format(best_param_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Report the performance on test data. Is this estimation fully reliable? Why? Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Set the value of *alpha* to the optimum value. Fit your pipeline to training data and predict the house prices on test data. Use *mean_squared_error()* function from *sklearn.metrics* package to measure your regression performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimum value for alpha\n",
    "ridge_pipe.set_params(ridge_reg__alpha = best_param_val)\n",
    "\n",
    "# Fit the ridge pipe\n",
    "ridge_pipe.fit(X_train,y_train)\n",
    "\n",
    "# Predict log of house prices\n",
    "y_pred_ridge = ridge_pipe.predict(X_test)\n",
    "\n",
    "# CODE HERE\n",
    "# Compute root mean square error (log-transformed)\n",
    "rmse_ridge = None\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert rmse_ridge is not None, 'HINT: you need to complete the code to proceed.'\n",
    "\n",
    "print('Log-transformed RMSE: ', rmse_ridge)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The estimation is not fully reliable due to a single test set, i.e. no cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3:** Print the list of ridge coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Extract the ridge regression from your pipeline and use its methods directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients\n",
    "\n",
    "ridge_reg = ridge_pipe.named_steps['ridge_reg']\n",
    "ridge_coefs = ridge_reg.coef_\n",
    "\n",
    "print('Shape: {}'.format(ridge_coefs.shape))\n",
    "print(ridge_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 4**: Prediction using Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Fit a lasso regression model to your data and tune the value of *alpha* parameter, the regularization strength. Note that your data should be standardized. Use a validation curve to visualize your tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('lasso_reg', Lasso()))\n",
    "lasso_pipe = Pipeline(estimators)\n",
    "lasso_pipe.set_params(lasso_reg__random_state = SEED)\n",
    "\n",
    "# CV schema\n",
    "cv_schema = KFold(n_splits = N_SPLITS, random_state = SEED)\n",
    "\n",
    "# Tune model against a single hyper parameter\n",
    "tuning_param = 'lasso_reg__alpha'\n",
    "tuning_param_range = np.logspace(-5, 3, 10)\n",
    "\n",
    "# Tune hyper-parameter using validation curve\n",
    "train_scores_val, cv_scores_val = validation_curve(\n",
    "    lasso_pipe, X_train, y_train, param_name = tuning_param, param_range = tuning_param_range,\n",
    "    cv = cv_schema, scoring = SCORE, n_jobs = 1)\n",
    "\n",
    "# Draw validation curve   \n",
    "plot_validation_curve(-train_scores_val, -cv_scores_val, tuning_param_range, scale = 'log', \n",
    "                      title = 'validation curve', y_label = 'MSE', x_label = 'alpha')\n",
    "\n",
    "# Obtain the best value of the hyper-parameter\n",
    "best_param_val = tuning_param_range[np.argmin(np.mean(-cv_scores_val, axis=1))]\n",
    "print('Best alpha: {}'.format(best_param_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Report the performance on test data. Is this estimation fully reliable? Why? Why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimum value for alpha\n",
    "lasso_pipe.set_params(lasso_reg__alpha = best_param_val)\n",
    "\n",
    "# Fit the lasso pipe\n",
    "lasso_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predict log of house prices\n",
    "y_pred_lasso = lasso_pipe.predict(X_test)\n",
    "\n",
    "# Compute root mean square error (log-transformed)\n",
    "rmse_lasso = sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "print('Log-transformed RMSE: ', rmse_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3:** Print the list of lasso coefficients. What's different compared to the ridge coefficients? What are the benefits and potential issues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "# Extract coefficients from the pipeline\n",
    "lasso_reg = None\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert lasso_reg is not None, 'HINT: you need to complete the code to proceed.'\n",
    "\n",
    "lasso_coefs = lasso_reg.coef_\n",
    "\n",
    "print('Shape: {}'.format(lasso_coefs.shape))\n",
    "print(lasso_coefs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Despite similar perfromance, many of lasso coefficients are equal to 0.\n",
    "This can simplify the model and make it more interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 5**: Prediction using Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Fit a random forest model to your data and tune the value of the *n_estimators* parameter, the number of trees in the forest. Use the validation curve to visualize your tuning process. Note that tree-based models do not require standardized features to converge well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "estimators = []\n",
    "estimators.append(('rf_reg', RandomForestRegressor()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_reg__random_state = SEED)\n",
    "\n",
    "# CODE HERE\n",
    "# Define the CV schema using the KFold() function in sklearn (don't forget the seed!)\n",
    "cv_schema = None\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert cv_schema is not None, 'HINT: you need to complete the code to proceed.'\n",
    "\n",
    "# Tune model against a single hyper parameter\n",
    "tuning_param = 'rf_reg__n_estimators'\n",
    "tuning_param_range = [int(i) for i in np.linspace(10.0, 270.0, 20)]\n",
    "\n",
    "# Tune hyper parameter using validation curve\n",
    "train_scores_val, cv_scores_val = validation_curve(\n",
    "    rf_pipe, X_train, y_train, param_name = tuning_param, param_range = tuning_param_range,\n",
    "    cv = cv_schema, scoring = SCORE, n_jobs = -1)\n",
    "\n",
    "# Draw validation curve   \n",
    "plot_validation_curve(-train_scores_val, -cv_scores_val, tuning_param_range, scale = 'lin', \n",
    "                      title = 'validation curve', y_label = 'MSE', x_label = 'n_estimators')\n",
    "\n",
    "# Obtain the best value of the hyper parameter\n",
    "best_param_val = tuning_param_range[np.argmin(np.mean(-cv_scores_val, axis=1))]\n",
    "print('Best n_estimators: {}'.format(best_param_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Report the performance on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimum value for n_estimators\n",
    "rf_pipe.set_params(rf_reg__n_estimators = best_param_val)\n",
    "\n",
    "# Fit the lasso pipe\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predict log of house prices\n",
    "y_pred_rf = rf_pipe.predict(X_test)\n",
    "\n",
    "# Compute root mean square error (log-transformed)\n",
    "rmse_rf = sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "print('Log-transformed RMSE: ', rmse_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3:** Print a list of the 10 most important features as defined by the decrease in mean impurity, in descending order. \n",
    "\n",
    "Hint: use the `feature_importances_` property of the fitted random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "rf_reg = rf_pipe.named_steps['rf_reg']\n",
    "\n",
    "i = 0\n",
    "for index in reversed(np.argsort(rf_reg.feature_importances_)):\n",
    "    \n",
    "    print('{}'.format(features.columns[index]).ljust(20) , rf_reg.feature_importances_[index])\n",
    "    \n",
    "    i = i + 1\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 4:** Compare the impurity-based feature importance to permutation importance on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute permutation importance \n",
    "result = permutation_importance(rf_reg, X_test, y_test, n_repeats=10, random_state=SEED, n_jobs=-1)\n",
    "sorted_idx = result.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top 10 most important features \n",
    "for i, col in enumerate(sorted_idx[::-1]):\n",
    "    print('{}'.format(X_test.columns[col]).ljust(20), result.importances_mean[col])\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot permutation importance of 10 most important features \n",
    "fig, ax = plt.subplots(figsize = (14, 8))\n",
    "ax.boxplot(result.importances[sorted_idx][-10:].T,\n",
    "           vert=False, labels=X_test.columns[sorted_idx][-10:])\n",
    "ax.set_title(\"Permutation Importances (TEST set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 6**: Prediction using Gradient Boosting Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Fit a gradient boosting regressor tree model to your data and tune the value of the *n_estimators* parameter, the number of boosting stages to perform. Use a validation curve to visualize your tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "estimators = []\n",
    "estimators.append(('gb_reg', GradientBoostingRegressor()))\n",
    "gb_pipe = Pipeline(estimators)\n",
    "gb_pipe.set_params(gb_reg__random_state = SEED)\n",
    "\n",
    "# CV schema\n",
    "cv_schema = KFold(n_splits = N_SPLITS, random_state = SEED)\n",
    "\n",
    "# Tune model against a single hyper parameter\n",
    "tuning_param = 'gb_reg__n_estimators'\n",
    "tuning_param_range = [int(i) for i in np.linspace(10.0, 310.0, 25)]\n",
    "\n",
    "# Tune hyper parameter using validation curve\n",
    "train_scores_val, cv_scores_val = validation_curve(\n",
    "    gb_pipe, X_train, y_train, param_name = tuning_param, param_range = tuning_param_range,\n",
    "    cv = cv_schema, scoring = SCORE, n_jobs = -1)\n",
    "\n",
    "# Draw validation curve   \n",
    "plot_validation_curve(-train_scores_val, -cv_scores_val, tuning_param_range, scale = 'lin', \n",
    "                      title = 'validation curve', y_label = 'MSE', x_label = 'n_estimators')\n",
    "\n",
    "# Obtain the best value of the hyper parameter\n",
    "best_param_val = tuning_param_range[np.argmin(np.mean(-cv_scores_val, axis=1))]\n",
    "print('Best n_estimators: {}'.format(best_param_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** Report the performance on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimum value for n_estimators\n",
    "gb_pipe.set_params(gb_reg__n_estimators = best_param_val)\n",
    "\n",
    "# Fit the lasso pipe\n",
    "gb_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predict log of house prices\n",
    "y_pred_gb = gb_pipe.predict(X_test)\n",
    "\n",
    "# Compute root mean square error (log-transformed)\n",
    "rmse_gb = sqrt(mean_squared_error(y_test, y_pred_gb))\n",
    "print('Log-transformed RMSE: ', rmse_gb)\n",
    "\n",
    "# What's the original-scale RMSE? Why do we only consider it towards the end? \n",
    "rmse = sqrt(mean_squared_error(np.exp(y_test), np.exp(y_pred_gb)))\n",
    "print('Original scale RMSE:  ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3:** Print a list of the 10 most important features as defined by the decrease in mean impurity, in descending order. What changed?\n",
    "\n",
    "Hint: use the `feature_importances_` property of the fitted gradient boosting tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "gb_reg = gb_pipe.named_steps['gb_reg']\n",
    "\n",
    "i = 0\n",
    "for index in reversed(np.argsort(gb_reg.feature_importances_)):\n",
    "    print('{}'.format(features.columns[index]).ljust(20) , gb_reg.feature_importances_[index])\n",
    "    \n",
    "    i = i + 1\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 4:** Compare the impurity-based feature importance to permutation importance on the test set. What changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "# Compute permutation importance of the pipeline on test data, with 10 repeats (don't forget the seed!)\n",
    "result = None\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert result is not None, 'HINT: you need to complete the code to proceed.'\n",
    "\n",
    "sorted_idx = result.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top 10 most important features \n",
    "for i, col in enumerate(sorted_idx[::-1]):\n",
    "    print('{}'.format(X_test.columns[col]).ljust(20), result.importances_mean[col])\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot permutation importance of 10 most important features \n",
    "fig, ax = plt.subplots(figsize = (14, 8))\n",
    "ax.boxplot(result.importances[sorted_idx][-10:].T,\n",
    "           vert=False, labels=X_test.columns[sorted_idx][-10:])\n",
    "ax.set_title(\"Permutation Importances (TEST set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 7**: Potential Prediction Improvement with More Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** A data scientist suggests that the model needs more data to improve performance. How do you know whether investing in more data indeed improves model performance? Pick the best predictive model you obtained above and visualize a learning curve, which plots the performance against training set size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use *learning_curve()* function from *sklearn.model_selection* to assess performance of your model using training data with different sizes. Consider 0.1, 0.2, ..., 1.0 times of the training size. Use the following function to visualize your learning curve:\n",
    "\n",
    "    def plot_learning_curve(train_scores,cv_scores,x_data,scale='lin',title='',y_label='',x_label=''):\n",
    "\n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        train_scores_std = np.std(train_scores, axis=1)\n",
    "        cv_scores_mean = np.mean(cv_scores, axis=1)\n",
    "        cv_scores_std = np.std(cv_scores, axis=1)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize = (14, 8))\n",
    "        plt.title(title)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        lw = 2\n",
    "\n",
    "        plt.fill_between(x_data, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.2, color=\"r\", lw=lw)\n",
    "        plt.fill_between(x_data, cv_scores_mean - cv_scores_std, cv_scores_mean + cv_scores_std, alpha=0.2, color=\"g\", lw=lw)\n",
    "\n",
    "        if (scale == 'lin'):\n",
    "            plt.plot(x_data, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "            plt.plot(x_data, cv_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n",
    "        elif (scale == 'log'):\n",
    "            plt.semilogx(x_data, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "            plt.semilogx(x_data, cv_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n",
    "        plt.grid()\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the best predictive model\n",
    "rf_reg = GradientBoostingRegressor(n_estimators = 250)\n",
    "cv_schema = KFold(n_splits = N_SPLITS, random_state = SEED)\n",
    "\n",
    "train_sizes = np.linspace(.1, 1.0, 10)\n",
    "\n",
    "# CODE HERE\n",
    "# Compute the learning curve with the learning_curve() function from sklearn\n",
    "train_sizes, train_scores_learn, cv_scores_learn = None\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert train_sizes is not None, 'HINT: you need to complete the code to proceed.'\n",
    "\n",
    "# Define a function to facilitate drawing learning curves\n",
    "\n",
    "def plot_learning_curve(train_scores,cv_scores,x_data,scale='lin',title='',y_label='',x_label=''):\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    cv_scores_mean = np.mean(cv_scores, axis=1)\n",
    "    cv_scores_std = np.std(cv_scores, axis=1)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (14, 8))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    lw = 2\n",
    "    \n",
    "    plt.fill_between(x_data, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.2, color=\"r\", lw=lw)\n",
    "    plt.fill_between(x_data, cv_scores_mean - cv_scores_std, cv_scores_mean + cv_scores_std, alpha=0.2, color=\"g\", lw=lw)\n",
    "    \n",
    "    if (scale == 'lin'):\n",
    "        plt.plot(x_data, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "        plt.plot(x_data, cv_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n",
    "    elif (scale == 'log'):\n",
    "        plt.semilogx(x_data, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "        plt.semilogx(x_data, cv_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n",
    "    plt.grid()\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "# Plot learning curve\n",
    "\n",
    "plot_learning_curve(np.sqrt(-train_scores_learn), np.sqrt(-cv_scores_learn), train_sizes, scale='lin', \n",
    "              title='Learning curve of gradient boosting regressor', y_label='MSE', x_label='train set size')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Consider the green line. It shows that the cross-validated score keeps decreasing as \n",
    "the training set increases and shows no signs of levelling off. \n",
    "The data scientist's suggestion has merit and we should invest in obtaining more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** In general, what data-related strategies can be proposed to come up with a better prediction ?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. More high-quality data\n",
    "2. More features with high predictive power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 8**: Potential Prediction Improvement with Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1:** Create a simple ensemble regressor using the average of your two best models, random forest and gradient boosting regression. Compare its performance with each of the two individual models. Does performance actually improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "# Calculate average of prediction vectors (hint: use the np.mean() function)\n",
    "y_pred_ensemble = None\n",
    "\n",
    "# Assert OK to proceed \n",
    "assert y_pred_ensemble is not None, 'HINT: you need to complete the code to proceed.'\n",
    "\n",
    "# Compute root mean square error (log-transformed)\n",
    "rmse_ensemble = sqrt(mean_squared_error(y_test, y_pred_ensemble))\n",
    "print('Log-transformed RMSE: ', rmse_ensemble)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(np.exp(y_test), np.exp(y_pred_ensemble)))\n",
    "print('Original scale RMSE:  ', rmse)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Performance does not improve much, if at all. This part is to show you how Kaggle challenges\n",
    "are often won: by combining many models and aggregating their predictions. \n",
    "Making such an ensemble model work is more about trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2:** How can you make your performance results statistically more reliable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Consider sources of selection bias in the test set."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Our test data can suffer from selection bias, even though we randomly select it. \n",
    "We just consider a single set. A statistically more reliable approach is to \n",
    "consider another k-fold cross validation for test set while considering a second k-fold cross validation for the hyper-parameter tuning.\n",
    "Such a schema is known as 'nested cross-validation'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3:** What strategies regarding modeling technique can be proposed to come up with a better prediction ?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Other regression models (e.g. xgboost, another tree-based model)\n",
    "2. Very careful tuning of all model hyper-parameters\n",
    "3. More complex ensemble strategies, e.g. combining 10 separately trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SUMMARY OF RMSE VALUES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width   = 35\n",
    "models  = ['Baseline', 'Ridge', 'Lasso', 'Random Forest', 'Gradient Boosting', 'Averaged Ensemble']\n",
    "results = [rmse_baseline, rmse_ridge, rmse_lasso, rmse_rf, rmse_gb, rmse_ensemble]\n",
    "print('', '=' * width, '\\n', 'Summary of RMSE Scores'.center(width), '\\n', '=' * width)  \n",
    "for i in range(len(models)):\n",
    "    print(models[i].center(width-8), '{0:.4f}'.format(results[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bonus**: Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Intuitive guide to tree-based models: https://towardsdatascience.com/a-guide-to-decision-trees-for-machine-learning-and-data-science-fe2607241956\n",
    "- Technical basics of gradient boosting: http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/\n",
    "- XGBoost and why it wins Kaggle challenges: https://medium.com/syncedreview/tree-boosting-with-xgboost-why-does-xgboost-win-every-machine-learning-competition-ca8034c0b283\n",
    "- More about ensemble models: https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/\n",
    "- More about permutation importance vs random forest feature importance: https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
