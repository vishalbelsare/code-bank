{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "removed-truth",
   "metadata": {},
   "source": [
    "# **Open-source**: Examples for style transfer & time series (SOLUTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21f392-1ade-4f16-9c75-bc2a7cd9b73e",
   "metadata": {},
   "source": [
    "Source:  [https://github.com/d-insight/code-bank.git](https://github.com/d-insight/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-karaoke",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-norfolk",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-taylor",
   "metadata": {},
   "source": [
    "In this exercise, we leverage open-source tools to show the power of re-using existing work from the data science community. We will (1) convert the style of an image, based on a pre-trained open-source model, and (2) use cutting-edge models for time series predictions\n",
    "\n",
    "__Main exercise 1__\n",
    "\n",
    "This *neural style transfer* takes a *content image* and a *style reference image* (e.g. by Picasso, Kandinsky, Van Gogh). The goal is to \"paint\" the content image in the style of the reference image, using neural networks.\n",
    "\n",
    "Original paper: *A Neural Algorithm of Artistic Style* by [Gatys et al. (2015)](https://arxiv.org/abs/1508.06576)\n",
    "\n",
    "__Main exercise 2__\n",
    "\n",
    "In the second part, we will explore how to build and manipulate a time series, train a forecasting model, and evaluate the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-timothy",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-radar",
   "metadata": {},
   "source": [
    "## Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "\n",
    "# Style transfer\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import time\n",
    "import functools\n",
    "\n",
    "# Load compressed models from tensorflow_hub\n",
    "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n",
    "\n",
    "# Time series\n",
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "from darts.models import (\n",
    "    NaiveDrift,\n",
    "    Prophet,\n",
    "    ExponentialSmoothing,\n",
    "    AutoARIMA,\n",
    "    Theta\n",
    ")\n",
    "from darts.metrics import mape, mase\n",
    "\n",
    "# Define plotting format\n",
    "mpl.rcParams['figure.figsize'] = (12, 12)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor*255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor)>3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor)\n",
    "\n",
    "def load_img(path_to_img):\n",
    "    max_dim = 512\n",
    "    img = tf.io.read_file(path_to_img)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "    long_dim = max(shape)\n",
    "    scale = max_dim / long_dim\n",
    "\n",
    "    new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "    img = tf.image.resize(img, new_shape)\n",
    "    img = img[tf.newaxis, :]\n",
    "    return img\n",
    "\n",
    "def imshow(image, title=None):\n",
    "    if len(image.shape) > 3:\n",
    "        image = tf.squeeze(image, axis=0)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-poetry",
   "metadata": {},
   "source": [
    "# **MAIN EXERCISE 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-henry",
   "metadata": {},
   "source": [
    "## Part 1: Upload image (optional)\n",
    "\n",
    "Upload an image to the directory of this notebook. You can (1) upload an image from your computer or (2) copy an image from the web. We encourage the former â€“ it's more fun."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-mileage",
   "metadata": {},
   "source": [
    "**Q 1:** Upload image titled `myImage.jpg` and replace the existing `myImage.jpg`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-retro",
   "metadata": {},
   "source": [
    "## Part 2: Choose style image\n",
    "\n",
    "Now comes the creative part. Choose one of the available style reference images below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-reduction",
   "metadata": {},
   "source": [
    "**Q 1:** Define the path to the style image in the `/styles` directory. \n",
    "\n",
    "Hint: Create a variable called `content_path` to reference the style image you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_path = 'styles/kandinsky.jpg'\n",
    "# style_path = 'styles/vanGogh.jpg'\n",
    "# style_path = 'styles/monet.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-warrant",
   "metadata": {},
   "source": [
    "**Q 2:** Show the style image.\n",
    "\n",
    "Hint: Use the `load_img()` and `imshow()` helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image = load_img(style_path)\n",
    "imshow(style_image, 'Style Image')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-vertex",
   "metadata": {},
   "source": [
    "## Part 3: Apply open-source model\n",
    "\n",
    "We now download an open-source, pre-trained neural network to \"paint\" our image in the style above. The model is available on the TensorFlow Hub [here](https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model\n",
    "hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-multiple",
   "metadata": {},
   "source": [
    "**Q 1:** Apply the style to your image.\n",
    "\n",
    "Hint: Use the `hub_model()` with the `content_image` and `style_image` as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = load_img('myImage.jpg')\n",
    "stylized_image = hub_model(tf.constant(content_image), tf.constant(style_image))[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-orientation",
   "metadata": {},
   "source": [
    "**Q 2:** Plot the stylized image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_to_image(stylized_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c55d74d-b045-46ca-af31-368d64ed625b",
   "metadata": {},
   "source": [
    "# **MAIN EXERCISE 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19446788-9dee-44ec-aeff-de9a41011cab",
   "metadata": {},
   "source": [
    "## Part 4: Load and prepare data\n",
    "\n",
    "We will use the well known [monthly airline passengers dataset](https://github.com/jbrownlee/Datasets/blob/master/monthly-airline-passengers.csv).\n",
    "\n",
    "A `TimeSeries` simply represents a univariate or multivariate time series, with a proper time index. It is a wrapper around a `pandas.DataFrame`, and it can be built in a few different ways:\n",
    "* From an entire Pandas `DataFrame` directly\n",
    "* From a time index and an array of corresponding values\n",
    "* From a subset of Pandas `DataFrame` columns, indicating which are the time column and the values columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f87d46-e093-4127-b47d-a69810898458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/AirPassengers.csv', delimiter=\",\")\n",
    "series = TimeSeries.from_dataframe(df, 'Month', ['#Passengers'])\n",
    "mpl.rcParams['figure.figsize'] = (8, 8)\n",
    "series.plot(grid=True, lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f04752-644b-407b-8a04-2b65fd17cfa7",
   "metadata": {},
   "source": [
    "**Q 1:** Create a training and validation series and plot.\n",
    "\n",
    "Let's split our `TimeSeries` into a training and a validation series. Note: in general, it is also a good practice to keep a test series aside and never touch it until the end of the process. Here, we just build a training and a test series for simplicity.\n",
    "\n",
    "The training series will be a `TimeSeries` containing values until January 1958 (excluded), and the validation series a `TimeSeries` containing the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fe94fa-5417-4b55-8d7f-f2c747e4eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = series.split_before(pd.Timestamp('19580101'))\n",
    "train.plot(grid=True, lw=2, label='training')\n",
    "val.plot(grid=True, lw=2, label='validation')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa624b4d-3cd8-44a5-b5ef-e4d907c70067",
   "metadata": {},
   "source": [
    "## Part 5: Fit different time series models\n",
    "\n",
    "`darts` is built to make it easy to train and validate several models in a unified way. Let's train a few more and compute their respective mean absolute percentage error (MAPE) on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ddde53-6d89-4e57-9ec6-15c0e0f031dd",
   "metadata": {},
   "source": [
    "**Q 1:** Evaluate the following time series models: `NaiveDrift() ExponentialSmoothing() Prophet() AutoARIMA() Theta()`.\n",
    "\n",
    "Hint 1: The above models are all functions readily built into DARTS.\n",
    "\n",
    "Hint 2: Write a model evaluation helper function that takes one of the above models as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c6d02-1356-4867-a71c-70f569b3b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    model.fit(train)\n",
    "    forecast = model.predict(len(val))\n",
    "    print('model {} obtains MAPE: {:.2f}%'.format(model, mape(val, forecast)))\n",
    "\n",
    "eval_model(NaiveDrift())\n",
    "eval_model(ExponentialSmoothing())\n",
    "eval_model(Prophet())\n",
    "eval_model(AutoARIMA())\n",
    "eval_model(Theta())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5a5bd2-8694-4157-b920-d7b012a52c42",
   "metadata": {},
   "source": [
    "Here, we did only built these models with their default parameters. We can probably do better if we fine-tune model-specific parameters to our problem. We skip this step here, but encourage you to try it out yourself and see how by how much you can improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f20a7ad-3514-4ce4-b95b-abf6922ec398",
   "metadata": {},
   "source": [
    "## Part 6: Plot the best model\n",
    "\n",
    "Finally, we plot how well the predictions fit the actual value in the validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5278472-011e-4cfa-9c6d-34d76497595f",
   "metadata": {},
   "source": [
    "**Q 1:** Re-fit the best performing model from the preceding part and save the predictions in a variable, so we can plot the predictions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac57be0-6182-452e-8dd3-a2d43690ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = ExponentialSmoothing()\n",
    "best_model.fit(train)\n",
    "pred_best = best_model.predict(len(val))\n",
    "\n",
    "print('The MAPE is: {:.2f}%'.format(mape(val, pred_best)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab5f9a8-06b8-4f83-9562-0e9c83b2c921",
   "metadata": {},
   "source": [
    "**Q 2:** Plot predicted vs. actual values in the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.plot(grid=True, lw=2, label='train')\n",
    "val.plot(grid=True, lw=2, label='true')\n",
    "pred_best.plot(grid=True, lw=2, label='prediction')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e42c51-3e2a-49e6-a2c3-2aee3c8e2212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EPFL",
   "language": "python",
   "name": "epfl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
