{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Evaluation**: Cost-Sensitive Learning from Imbalanced Data (SOLUTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:  [https://github.com/d-insight/code-bank.git](https://github.com/d-insight/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase. In case of a fraudulent transaction, the credit card company can incur substantial costs. In this exercise, we model the costs associated with prediction errors using a regularized logit model. We will learn that (1) weighting classes and (2) data resampling can help lower real-world costs for prediction errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://images.unsplash.com/photo-1563013544-824ae1b704d3?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=2250&q=80\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "\n",
    "Image source: https://images.unsplash.com/photo-1563013544-824ae1b704d3?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=2250&q=80\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "The data contain only numerical input variables which are the result of a data transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. \n",
    "\n",
    "Variables are defined as follows: \n",
    "\n",
    "| Feature name     | Variable Type | Description \n",
    "|------------------|---------------|--------------------------------------------------------\n",
    "| Time             | Continuous    | Seconds elapsed between each transaction and the first transaction in the dataset\n",
    "| V1               | Continuous    | Transformed feature 1 (due to confidentiality)\n",
    "| ...              | ...           | ...\n",
    "| V28              | Continuous    | Transformed feature 28\n",
    "| Amount           | Continuous    | Transaction amount\n",
    "| Class            | Binary        | Target variable (1 = fraud; 0 = no fraud)\n",
    "\n",
    "Data source: https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "References:\n",
    "\n",
    "- [Jason Brownlee on MachineLearningMastery.com](https://machinelearningmastery.com/cost-sensitive-learning-for-imbalanced-classification/)\n",
    "- [Elkan (2001) - The Foundations of Cost-Sensitive Learning](http://web.cs.iastate.edu/~honavar/elkan.pdf)\n",
    "- [Thai-Nghe et al. (2010) - Cost-sensitive learning methods for imbalanced data](https://ieeexplore.ieee.org/document/5596486)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "\n",
    "# Statistical modeling functions from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.metrics         import confusion_matrix, plot_confusion_matrix, roc_auc_score, accuracy_score, f1_score\n",
    "from sklearn.utils           import resample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Think of CHF costs for prediction errors (change these costs -> click \"Run All\" -> see how model performance changes)\n",
    "# Default = 1000\n",
    "COST_FN = 1000\n",
    "\n",
    "# Default = 1\n",
    "COST_FP = 1\n",
    "\n",
    "# Column names\n",
    "COLUMN_NAMES = [\"Time\", \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\", \"V10\", \"V11\", \"V12\", \"V13\", \"V14\", \"V15\",\n",
    "                \"V16\", \"V17\", \"V18\", \"V19\", \"V20\", \"V21\", \"V22\", \"V23\", \"V24\" ,\"V25\", \"V26\", \"V27\", \"V28\", \"Amount\", \"Class\"]\n",
    "\n",
    "# Seed for replication\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "Run the cell below to download the .csv data from our Google Cloud storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N 'https://storage.googleapis.com/dsfm-datasets/model-evaluation/creditcard.csv.zip'\n",
    "!unzip -o -j creditcard.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MAIN EXERCISE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load data\n",
    "\n",
    "First, we load in the credit card data and summarize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1**: Load the `creditcard.csv` file and set the column names, using the column names defined above. Display the first 5 rows and the shape of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2**: Are any missing values in the data? What are the summary statistics for each column? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values by variable\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: EDA and train/test split\n",
    "\n",
    "Next, we investigate the class distribution of the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1**: Plot a histogram of the target variable `Class`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2**: What percentage of transactions are fraudulent? \n",
    "\n",
    "Hint: Use the `value_counts` function in Pandas on the `Class` variable to count the number of transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df['Class'].value_counts()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of fraudulent transactions\n",
    "percentage_fraudulent = count[1] / (count[1] + count[0]) * 100\n",
    "print('{}% of transactions are fraudulent'.format(round(percentage_fraudulent, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3**: Split the data into training (80%) and testing data (20%). What's the shape of all four data sets?\n",
    "\n",
    "Hint: Don't forget to stratify by the target variable. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into training and testing sets\n",
    "X = df.drop(columns=['Class'], inplace=False)\n",
    "y = df['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = SEED, stratify = y)\n",
    "\n",
    "print(X_train.shape , X_test.shape , y_train.shape , y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Part 3: Baseline - all predictions errors have the same cost\n",
    "\n",
    "In this part, we establish the baseline that (naively) assigns uniform costs to each prediction error. We use a regularized l2 logistic regression model throughout this exercise. For the baseline, the model does not know that costs for FP and FN are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1**: Fit the logit model and predict fraud on the test data as a probability between 0 and 1. What does the confusion matrix look like? \n",
    "\n",
    "Hint: Use the `plot_confusion_matrix` in the `sklearn` package to plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification using a logit model\n",
    "lr_clf = LogisticRegression(max_iter = 1000)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict fraud - as a binary outcome and a probability between 0 and 1\n",
    "y_pred_lr       = lr_clf.predict(X_test)\n",
    "y_pred_lr_proba = lr_clf.predict_proba(X_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "print('{} observations'.format(len(X_test)))\n",
    "cm_baseline = plot_confusion_matrix(lr_clf, X_test, y_test, values_format='.5g')\n",
    "cm_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2**: Count the total prediction errors, compute the accuracy score, AUC score, and the total prediction costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many errors did the the model make?\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "errors_baseline = cm_lr[0][1] + cm_lr[1][0]\n",
    "cost_baseline = cm_lr[0][1] * COST_FP + cm_lr[1][0] * COST_FN\n",
    "amount_baseline = sum(X_test[y_test != y_pred_lr]['Amount'])\n",
    "\n",
    "acc_baseline  = accuracy_score(y_test, y_pred_lr)\n",
    "auc_baseline  = roc_auc_score(y_test, y_pred_lr_proba[:, 1])\n",
    "f1_baseline   = f1_score(y_test, y_pred_lr, average='weighted')\n",
    "\n",
    "print('Num errors =', errors_baseline, '\\n')\n",
    "print('Accuracy   =', \"{0:.4f}\".format(acc_baseline))\n",
    "print('AUC score  =', \"{0:.4f}\".format(auc_baseline))\n",
    "print('F1 score   =', \"{0:.4f}\".format(f1_baseline))\n",
    "print('Total amt. =', \"CHF {0:.2f}\".format(amount_baseline))\n",
    "print('Total cost =', \"CHF {0:.2f}\".format(cost_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3**: Fit the logit model and predict fraud on the test data as a probability between 0 and 1. What does the confusion matrix look like? \n",
    "\n",
    "Hint: Use the `plot_confusion_matrix` in the `sklearn` package to plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification using a logit model\n",
    "lr_clf = LogisticRegression(max_iter = 1000)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict fraud - as a binary outcome and a probability between 0 and 1\n",
    "y_pred_lr       = lr_clf.predict(X_test)\n",
    "y_pred_lr_proba = lr_clf.predict_proba(X_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "print('{} observations'.format(len(X_test)))\n",
    "cm_baseline = plot_confusion_matrix(lr_clf, X_test, y_test, values_format='.5g')\n",
    "cm_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 4**: Count the total prediction errors, compute the accuracy score, AUC score, and the total prediction costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many errors did the the model make?\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "errors_baseline = cm_lr[0][1] + cm_lr[1][0]\n",
    "cost_baseline = cm_lr[0][1] * COST_FP + cm_lr[1][0] * COST_FN\n",
    "\n",
    "acc_baseline  = accuracy_score(y_test, y_pred_lr)\n",
    "auc_baseline  = roc_auc_score(y_test, y_pred_lr_proba[:, 1])\n",
    "f1_baseline   = f1_score(y_test, y_pred_lr, average='weighted')\n",
    "\n",
    "print('Num errors =', errors_baseline, '\\n')\n",
    "print('Accuracy   =', \"{0:.4f}\".format(acc_baseline))\n",
    "print('AUC score  =', \"{0:.4f}\".format(auc_baseline))\n",
    "print('F1 score   =', \"{0:.4f}\".format(f1_baseline))\n",
    "print('Total cost =', \"CHF {0:.2f}\".format(cost_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We see that the model performs reasonably well judging from AUC and F1 scores. It still makes 28 FN prediction errors, which are particularly expensive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# **ADVANCED EXERCISE**\n",
    "\n",
    "*Optional.* If time permits and you feel comfortable with Python, continue with the advanced parts of this exercise below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Class weights - modifying algorithm parameters\n",
    "\n",
    "One approach to cost-sensitive learning for imbalanced classification is to up-weigh the minority class, which we will implement in this part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1**: Fit the same logit model as in Part 3, but set the `class_weight` parameter in the `LogisticRegression` function to represent different prediction error costs.\n",
    "\n",
    "Hint: You can re-use the code from Part 3, Question 1. All you have to add is a value for the `class_weight` parameter in the `LogisticRegression` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification using a logit model\n",
    "lr_clf = LogisticRegression(max_iter = 1000, class_weight={0:COST_FP, 1: COST_FN})\n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict fraud - as a binary outcome and a probability between 0 and 1\n",
    "y_pred_lr       = lr_clf.predict(X_test)\n",
    "y_pred_lr_proba = lr_clf.predict_proba(X_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "print('{} observations'.format(len(X_test)))\n",
    "cm_weighted = plot_confusion_matrix(lr_clf, X_test, y_test, values_format='.5g')\n",
    "cm_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2**: Count the total prediction errors, compute the accuracy score, AUC score, and the total prediction costs.\n",
    "\n",
    "Hint: You can re-use the code from Part 3, Question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many errors did the model make?\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "errors_weighted = cm_lr[0][1] + cm_lr[1][0]\n",
    "cost_weighted = cm_lr[0][1] * COST_FP + cm_lr[1][0] * COST_FN\n",
    "amount_weighted = sum(X_test[y_test != y_pred_lr]['Amount'])\n",
    "\n",
    "acc_weighted  = accuracy_score(y_test, y_pred_lr)\n",
    "auc_weighted  = roc_auc_score(y_test, y_pred_lr_proba[:, 1])\n",
    "f1_weighted   = f1_score(y_test, y_pred_lr, average='weighted')\n",
    "\n",
    "print('Num errors =', errors_weighted, '\\n')\n",
    "print('Accuracy   =', \"{0:.4f}\".format(acc_weighted))\n",
    "print('AUC score  =', \"{0:.4f}\".format(auc_weighted))\n",
    "print('F1 score   =', \"{0:.4f}\".format(f1_weighted))\n",
    "print('Total amt. =', \"CHF {0:.2f}\".format(amount_weighted))\n",
    "print('Total cost =', \"CHF {0:.2f}\".format(cost_weighted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We see that the number of FNs was reduced from 28 to 8, substantially reducing the total prediction costs. However, the number of FP increase by two orders of magnitude (!). Given the FN and FP costs, the benefits of reducing FNs outweigh the higher costs of more FPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 5: Sample weights - modifying algorithm parameters\n",
    "\n",
    "Another approach to cost-sensitive learning for imbalanced classification is to up-weigh more important samples. We will use the transaction amount as a proxy for how important the prediction for that sample is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1**: Fit the same logit model as in Part 3, but set the `sample_weight` parameter in the `LogisticRegression.fit()` function to represent the transaction amounts for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification using a logit model\n",
    "lr_clf = LogisticRegression(max_iter = 1000)\n",
    "lr_clf.fit(X_train, y_train, sample_weight = X_train['Amount'])\n",
    "\n",
    "# Predict fraud - as a binary outcome and a probability between 0 and 1\n",
    "y_pred_lr       = lr_clf.predict(X_test)\n",
    "y_pred_lr_proba = lr_clf.predict_proba(X_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "print('{} observations'.format(len(X_test)))\n",
    "cm_weightedSample = plot_confusion_matrix(lr_clf, X_test, y_test, values_format='.5g')\n",
    "cm_weightedSample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2**: Count the total prediction errors, compute the accuracy score, AUC score, and the total prediction costs.\n",
    "\n",
    "Hint: You can re-use the code from Part 3, Question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many errors did the model make?\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "errors_weightedSample = cm_lr[0][1] + cm_lr[1][0]\n",
    "cost_weightedSample = cm_lr[0][1] * COST_FP + cm_lr[1][0] * COST_FN\n",
    "amount_weightedSample = sum(X_test[y_test != y_pred_lr]['Amount'])\n",
    "\n",
    "acc_weightedSample  = accuracy_score(y_test, y_pred_lr)\n",
    "auc_weightedSample  = roc_auc_score(y_test, y_pred_lr_proba[:, 1])\n",
    "f1_weightedSample   = f1_score(y_test, y_pred_lr, average='weighted')\n",
    "\n",
    "print('Num errors =', errors_weightedSample, '\\n')\n",
    "print('Accuracy   =', \"{0:.4f}\".format(acc_weightedSample))\n",
    "print('AUC score  =', \"{0:.4f}\".format(auc_weightedSample))\n",
    "print('F1 score   =', \"{0:.4f}\".format(f1_weightedSample))\n",
    "print('Total amt. =', \"CHF {0:.2f}\".format(amount_weightedSample))\n",
    "print('Total cost =', \"CHF {0:.2f}\".format(cost_weightedSample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We see that the number of FNs increased from 8 to 21 and the number of FPs decreased from 4162 to 21. The total transaction amount of the 42 prediction errors substantially decreased from CHF 1'130'300.33 to CHF 3'186.50. In short, if the prediction costs change based on the transaction amount, we have greatly improved our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Data resampling - modifying training data\n",
    "\n",
    "Yet another approach to tackling the class imbalance problem is to resample data. One can resample by undersampling the majority class (not fraudulent) or oversampling the minority class (fraudulent), as summarized by the image below. Given our pre-defined costs for prediction errors, this is also called \"cost-proportionate resampling\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png\" width=\"700\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "Image source: https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1**: Upsample the minority class using the `resample` function in the `sklearn` package.\n",
    "\n",
    "Hint: Only use the TRAINING data for resampling, not the full dataset. We want to leave the TESTING data untouched for evaluating performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-construct the training data with the target variable\n",
    "Xy_train = X_train.copy()\n",
    "Xy_train['Class'] = y_train\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = Xy_train[Xy_train['Class'] == 0]\n",
    "df_minority = Xy_train[Xy_train['Class'] == 1]\n",
    "\n",
    "print('{} majority samples in the training data.'.format(len(df_majority)))\n",
    " \n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace = True,                # sample with replacement\n",
    "                                 n_samples = len(df_majority),  # to match majority class\n",
    "                                 random_state = SEED)           # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled_train = pd.concat([df_majority, df_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled_train['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 2**: Re-fit the same logit model as in Part 3, but with the upsampled training data and unchanged testing data. Before doing that, make sure to split the features (X) from the target (y) variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into features and target variables\n",
    "X_train_upsampled = df_upsampled_train.drop(columns=['Class'], inplace=False)\n",
    "y_train_upsampled = df_upsampled_train['Class']\n",
    "\n",
    "print(X_train_upsampled.shape , X_test.shape , y_train_upsampled.shape , y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification using a logit model\n",
    "lr_clf = LogisticRegression(max_iter = 1000)\n",
    "lr_clf.fit(X_train_upsampled, y_train_upsampled)\n",
    "\n",
    "# Predict fraud - as a binary outcome and a probability between 0 and 1\n",
    "y_pred_lr       = lr_clf.predict(X_test)\n",
    "y_pred_lr_proba = lr_clf.predict_proba(X_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "print('{} observations'.format(len(X_test)))\n",
    "cm_resampled = plot_confusion_matrix(lr_clf, X_test, y_test, values_format='.5g')\n",
    "cm_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3**: Count the total prediction errors, compute the accuracy score, AUC score, and the total prediction costs. What changed?\n",
    "\n",
    "Hint: You can re-use the code from Part 3, Question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many errors did the model make?\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "errors_resampled = cm_lr[0][1] + cm_lr[1][0]\n",
    "cost_resampled = cm_lr[0][1] * COST_FP + cm_lr[1][0] * COST_FN\n",
    "amount_resampled = sum(X_test[y_test != y_pred_lr]['Amount'])\n",
    "\n",
    "acc_resampled  = accuracy_score(y_test, y_pred_lr)\n",
    "auc_resampled  = roc_auc_score(y_test, y_pred_lr_proba[:, 1])\n",
    "f1_resampled   = f1_score(y_test, y_pred_lr, average='weighted')\n",
    "\n",
    "print('Num errors =', errors_resampled, '\\n')\n",
    "print('Accuracy   =', \"{0:.4f}\".format(acc_resampled))\n",
    "print('AUC score  =', \"{0:.4f}\".format(auc_resampled))\n",
    "print('F1 score   =', \"{0:.4f}\".format(f1_resampled))\n",
    "print('Total amt. =', \"CHF {0:.2f}\".format(amount_resampled))\n",
    "print('Total cost =', \"CHF {0:.2f}\".format(cost_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We see that the number of False Negatives slightly increased from 8 to 9, but the number of False Positives also decreased substantially. In sum, the total costs are slightly lower for the data resampling approach than for the class weights approach. If the prediction costs depend on the type of error (FNs and FPs), we have further improved our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Summary of ROC curves and model performances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "cm_baseline.plot(values_format='.5g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights\n",
    "cm_weighted.plot(values_format='.5g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample weights\n",
    "cm_weightedSample.plot(values_format='.5g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling\n",
    "cm_resampled.plot(values_format='.5g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance, errors, and costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of model performances\n",
    "width     = 25\n",
    "width_box = 100\n",
    "models    = ['Baseline', 'Class weights', 'Sample weights', 'Resampling']\n",
    "metrics   = [' Accuracy', ' AUC', ' F1', ' Num errors', ' CHF amount', ' CHF costs']\n",
    "accs      = [acc_baseline, acc_weighted, acc_weightedSample, acc_resampled]\n",
    "aucs      = [auc_baseline, auc_weighted, auc_weightedSample, auc_resampled]\n",
    "f1s       = [f1_baseline, f1_weighted, f1_weightedSample, f1_resampled]\n",
    "errors    = [errors_baseline, errors_weighted, errors_weightedSample, errors_resampled]\n",
    "costs     = [cost_baseline, cost_weighted, cost_weightedSample, cost_resampled]\n",
    "amounts   = [amount_baseline, amount_weighted, amount_weightedSample, amount_resampled]\n",
    "summary   = [accs, aucs, f1s, errors, amounts, costs]\n",
    "\n",
    "print('Summary table: Predictive performance on TEST data.')\n",
    "print(str('=' * width * (len(models)+1)))\n",
    "print(''.ljust(width) + '{}'.format(models[0]).ljust(width) + '{}'.format(models[1]).ljust(width) + '{}'.format(models[2]).ljust(width) + '{}'.format(models[3]).ljust(width))\n",
    "print(str('=' * width * (len(models)+1)))\n",
    "for i in range(len(metrics)):\n",
    "    line = metrics[i].ljust(width) + '{}'.format(round(summary[i][0], 4)).ljust(width) + '{}'.format(round(summary[i][1], 4)).ljust(width) + '{}'.format(round(summary[i][2], 4)).ljust(width) + '{}'.format(round(summary[i][3], 4)).ljust(width)\n",
    "    print(line.center(width_box))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [SMOTE for Imbalanced Classification with Python](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
